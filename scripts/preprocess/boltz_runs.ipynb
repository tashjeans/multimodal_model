{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e6a569d5",
      "metadata": {},
      "source": [
        "This script:\n",
        "- Takes YAML and manifest files\n",
        "- Runs Boltz on those files\n",
        "\n",
        "\n",
        "Inputs:\n",
        "- Directory containing YAML files needed to run Boltz\n",
        "- Manifest file path for those YAML files\n",
        "\n",
        "Outputs:\n",
        "- Boltz directory containing runs of YAML files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8eeb39b",
      "metadata": {},
      "source": [
        "Run Boltz for Train/Test/Val in Chunk Directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "839cf2f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import subprocess, shlex\n",
        "import pandas as pd\n",
        "\n",
        "# === Configure paths ===\n",
        "BASE_DIR = Path(\"/home/natasha/multimodal_model\")\n",
        "\n",
        "RUN_ROOT = BASE_DIR / \"outputs\"\n",
        "RUN_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "BOLTZ_OUT_TRAIN = RUN_ROOT / \"train\"\n",
        "BOLTZ_OUT_VAL   = RUN_ROOT / \"val\"\n",
        "BOLTZ_OUT_TEST  = RUN_ROOT / \"test\"\n",
        "BOLTZ_OUT_TRAIN.mkdir(parents=True, exist_ok=True)\n",
        "BOLTZ_OUT_VAL.mkdir(parents=True, exist_ok=True)\n",
        "BOLTZ_OUT_TEST.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "YAML_DIR_TRAIN = BASE_DIR / \"data\" / \"train\"\n",
        "YAML_DIR_VAL   = BASE_DIR / \"data\" / \"val\"\n",
        "YAML_DIR_TEST  = BASE_DIR / \"data\" / \"test\"\n",
        "\n",
        "# Where your chunks live (created by your chunking helper)\n",
        "TRAIN_CHUNKS_ROOT = YAML_DIR_TRAIN / \"_chunks\"\n",
        "TEST_CHUNKS_ROOT  = YAML_DIR_TEST / \"_chunks\"\n",
        "\n",
        "# One GPU => run chunks sequentially\n",
        "NPROC = 1\n",
        "\n",
        "BOLTZ_CMD_TEMPLATE = (\n",
        "    \"conda run -n boltz-env --no-capture-output boltz predict {input_path} \"\n",
        "    \"--out_dir {outdir} \"\n",
        "    \"--accelerator gpu \"\n",
        "    \"--devices 1 \"\n",
        "    \"--model boltz2 \"\n",
        "    \"--recycling_steps 1 \"\n",
        "    \"--sampling_steps 10 \"\n",
        "    \"--diffusion_samples 1 \"\n",
        "    \"--max_parallel_samples 1 \"\n",
        "    \"--max_msa_seqs 64 \"\n",
        "    \"--num_subsampled_msa 34 \"\n",
        "    # IMPORTANT: remove --override once you're in production/resume mode\n",
        "    \"--write_embeddings\"\n",
        ")\n",
        "\n",
        "# --override (only use when want to rerun on all values)\n",
        "# also reduced num sampled msa to 34 and also number  of recycling steps to 10 (from 20), to try and speed things up\n",
        "\n",
        "def run_cli(input_path: Path, outdir: Path) -> int:\n",
        "    \"\"\"\n",
        "    input_path can be:\n",
        "      - a single YAML file, or\n",
        "      - a directory containing many YAMLs (chunk)\n",
        "    \"\"\"\n",
        "    input_path = Path(input_path).resolve()\n",
        "    outdir = Path(outdir).resolve()\n",
        "    outdir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    cmd = BOLTZ_CMD_TEMPLATE.format(input_path=str(input_path), outdir=str(outdir))\n",
        "    print(\"CMD:\", cmd)\n",
        "\n",
        "    with open(outdir / \"stdout.log\", \"w\") as so, open(outdir / \"stderr.log\", \"w\") as se:\n",
        "        proc = subprocess.run(\n",
        "            shlex.split(cmd),\n",
        "            stdout=so,\n",
        "            stderr=se,\n",
        "            text=True,\n",
        "            cwd=str(BASE_DIR),\n",
        "        )\n",
        "\n",
        "    print(\"Return code:\", proc.returncode)\n",
        "    return proc.returncode\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17420e36",
      "metadata": {},
      "source": [
        "This notebook takes the manifest files, the YAML files and the csv files and matches the pair_id across all three sources to ensure consolidating tagging across all data sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c34fba12",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "import pandas as pd\n",
        "import yaml\n",
        "\n",
        "BASE_DIR = Path(\"/home/natasha/multimodal_model\")\n",
        "DATA_DIR = BASE_DIR / \"data\"\n",
        "OUT_PATH = BASE_DIR / \"manifests\" / \"pair_table.csv\"   # or wherever you want\n",
        "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5c460bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _collect_sequence_entries(obj):\n",
        "    \"\"\"\n",
        "    Recursively walk a loaded YAML object and collect any dicts that look like\n",
        "    { ..., \"sequence\": <string>, ... }.\n",
        "    Returns list of dicts with keys: name, chain, kind, sequence (as available).\n",
        "    \"\"\"\n",
        "    found = []\n",
        "\n",
        "    def walk(x):\n",
        "        if isinstance(x, dict):\n",
        "            if \"sequence\" in x and isinstance(x[\"sequence\"], (str, type(None))):\n",
        "                entry = {\n",
        "                    \"name\": x.get(\"name\") or x.get(\"id\") or x.get(\"chain\") or x.get(\"label\"),\n",
        "                    \"kind\": x.get(\"type\") or x.get(\"kind\") or x.get(\"molecule\") or x.get(\"entity\"),\n",
        "                    \"chain\": x.get(\"chain\") or x.get(\"chain_id\") or x.get(\"asym_id\"),\n",
        "                    \"sequence\": x.get(\"sequence\"),\n",
        "                }\n",
        "                found.append(entry)\n",
        "            for v in x.values():\n",
        "                walk(v)\n",
        "        elif isinstance(x, list):\n",
        "            for v in x:\n",
        "                walk(v)\n",
        "\n",
        "    walk(obj)\n",
        "    return found\n",
        "\n",
        "\n",
        "def _normalise_seq(s):\n",
        "    if s is None:\n",
        "        return None\n",
        "    s = str(s).strip()\n",
        "    if s == \"\" or s.lower() in {\"none\", \"null\", \"nan\"}:\n",
        "        return None\n",
        "    return s\n",
        "\n",
        "\n",
        "def _assign_roles(entries):\n",
        "    \"\"\"\n",
        "    Determine (peptide, tcra, tcrb, hla) from collected sequence entries.\n",
        "\n",
        "    Priority:\n",
        "      1) explicit 'name' contains hints (tcr a/b, peptide, hla/mhc)\n",
        "      2) fallback to length heuristics:\n",
        "         - peptide: <= 30\n",
        "         - hla: >= 250 (often 300-400)\n",
        "         - tcra/tcrb: 60-200\n",
        "    \"\"\"\n",
        "    # clean and filter\n",
        "    cleaned = []\n",
        "    for e in entries:\n",
        "        seq = _normalise_seq(e.get(\"sequence\"))\n",
        "        if seq is None:\n",
        "            continue\n",
        "        cleaned.append({**e, \"sequence\": seq, \"len\": len(seq), \"name_l\": (e.get(\"name\") or \"\").lower()})\n",
        "\n",
        "    # --- name-based picks ---\n",
        "    pep = None\n",
        "    hla = None\n",
        "    tcra = None\n",
        "    tcrb = None\n",
        "\n",
        "    def pick_first(pred):\n",
        "        for e in cleaned:\n",
        "            if pred(e):\n",
        "                return e[\"sequence\"]\n",
        "        return None\n",
        "\n",
        "    # Peptide\n",
        "    pep = pick_first(lambda e: any(k in e[\"name_l\"] for k in [\"pep\", \"peptide\", \"antigen\"]))\n",
        "\n",
        "    # HLA/MHC\n",
        "    hla = pick_first(lambda e: any(k in e[\"name_l\"] for k in [\"hla\", \"mhc\", \"class i\", \"class_i\", \"class-i\"]))\n",
        "\n",
        "    # TCR alpha/beta\n",
        "    tcra = pick_first(lambda e: any(k in e[\"name_l\"] for k in [\"tcra\", \"tcr_a\", \"tcr alpha\", \"alpha chain\", \"alpha_chain\"]))\n",
        "    tcrb = pick_first(lambda e: any(k in e[\"name_l\"] for k in [\"tcrb\", \"tcr_b\", \"tcr beta\", \"beta chain\", \"beta_chain\"]))\n",
        "\n",
        "    # --- fallback to length heuristics if missing ---\n",
        "    # If peptide not set, choose the shortest <=30\n",
        "    if pep is None:\n",
        "        pep_cands = [e for e in cleaned if e[\"len\"] <= 30]\n",
        "        pep = min(pep_cands, key=lambda e: e[\"len\"])[\"sequence\"] if pep_cands else None\n",
        "\n",
        "    # If HLA not set, choose the longest >=250 (or just absolute longest if none >=250)\n",
        "    if hla is None:\n",
        "        hla_cands = [e for e in cleaned if e[\"len\"] >= 250]\n",
        "        if hla_cands:\n",
        "            hla = max(hla_cands, key=lambda e: e[\"len\"])[\"sequence\"]\n",
        "        elif cleaned:\n",
        "            hla = max(cleaned, key=lambda e: e[\"len\"])[\"sequence\"]\n",
        "\n",
        "    # Remaining candidates for TCRa/TCRb: 60-200, excluding chosen pep/hla by identity\n",
        "    remaining = [e for e in cleaned if e[\"sequence\"] not in {pep, hla} and 60 <= e[\"len\"] <= 220]\n",
        "\n",
        "    # If tcra/tcrb still missing, pick two longest from remaining\n",
        "    if (tcra is None) or (tcrb is None):\n",
        "        remaining_sorted = sorted(remaining, key=lambda e: e[\"len\"], reverse=True)\n",
        "        if tcra is None and len(remaining_sorted) >= 1:\n",
        "            tcra = remaining_sorted[0][\"sequence\"]\n",
        "        if tcrb is None and len(remaining_sorted) >= 2:\n",
        "            tcrb = remaining_sorted[1][\"sequence\"]\n",
        "\n",
        "    return pep, tcra, tcrb, hla\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3a2d42e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found YAMLs: 70539\n",
            "Parsed OK : 70525\n",
            "Bad/skip  : 14\n",
            "\n",
            "First 10 skipped:\n",
            " - /home/natasha/multimodal_model/data/train/_chunks/chunk_000/pair_10061.yaml => YAML parse error: [Errno 2] No such file or directory: '/home/natasha/multimodal_model/data/train/_chunks/chunk_000/pair_10061.yaml'\n",
            " - /home/natasha/multimodal_model/data/train/_chunks/chunk_000/pair_10863.yaml => YAML parse error: [Errno 2] No such file or directory: '/home/natasha/multimodal_model/data/train/_chunks/chunk_000/pair_10863.yaml'\n",
            " - /home/natasha/multimodal_model/data/train/_chunks/chunk_000/pair_11083.yaml => YAML parse error: [Errno 2] No such file or directory: '/home/natasha/multimodal_model/data/train/_chunks/chunk_000/pair_11083.yaml'\n",
            " - /home/natasha/multimodal_model/data/train/_chunks/chunk_000/pair_11244.yaml => YAML parse error: [Errno 2] No such file or directory: '/home/natasha/multimodal_model/data/train/_chunks/chunk_000/pair_11244.yaml'\n",
            " - /home/natasha/multimodal_model/data/train/_chunks/chunk_000/pair_11302.yaml => YAML parse error: [Errno 2] No such file or directory: '/home/natasha/multimodal_model/data/train/_chunks/chunk_000/pair_11302.yaml'\n",
            " - /home/natasha/multimodal_model/data/train/_chunks/chunk_014/pair_8054.yaml => YAML parse error: [Errno 2] No such file or directory: '/home/natasha/multimodal_model/data/train/_chunks/chunk_014/pair_8054.yaml'\n",
            " - /home/natasha/multimodal_model/data/train/_chunks/chunk_014/pair_8117.yaml => YAML parse error: [Errno 2] No such file or directory: '/home/natasha/multimodal_model/data/train/_chunks/chunk_014/pair_8117.yaml'\n",
            " - /home/natasha/multimodal_model/data/train/_chunks/chunk_014/pair_8118.yaml => YAML parse error: [Errno 2] No such file or directory: '/home/natasha/multimodal_model/data/train/_chunks/chunk_014/pair_8118.yaml'\n",
            " - /home/natasha/multimodal_model/data/train/_chunks/chunk_014/pair_8397.yaml => YAML parse error: [Errno 2] No such file or directory: '/home/natasha/multimodal_model/data/train/_chunks/chunk_014/pair_8397.yaml'\n",
            " - /home/natasha/multimodal_model/data/train/_chunks/chunk_015/pair_9004.yaml => YAML parse error: [Errno 2] No such file or directory: '/home/natasha/multimodal_model/data/train/_chunks/chunk_015/pair_9004.yaml'\n",
            "\n",
            "Wrote: /home/natasha/multimodal_model/manifests/pair_table.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pair_id</th>\n",
              "      <th>yaml_path</th>\n",
              "      <th>split</th>\n",
              "      <th>chunk</th>\n",
              "      <th>pep_seq</th>\n",
              "      <th>tcra_seq</th>\n",
              "      <th>tcrb_seq</th>\n",
              "      <th>hla_seq</th>\n",
              "      <th>pep_len</th>\n",
              "      <th>tcra_len</th>\n",
              "      <th>tcrb_len</th>\n",
              "      <th>hla_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pair_000</td>\n",
              "      <td>/home/natasha/multimodal_model/data/test/_chun...</td>\n",
              "      <td>test</td>\n",
              "      <td>chunk_000</td>\n",
              "      <td>TSTLQEQIGW</td>\n",
              "      <td>EAGVTQFPSHSVIEKGQTVTLRCDPISGHDNLYWYRRVMGKEIKFL...</td>\n",
              "      <td>GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQL...</td>\n",
              "      <td>MRVTAPRTVLLLLWGAVALTETWAGSHSMRYFYTAMSRPGRGEPRF...</td>\n",
              "      <td>10</td>\n",
              "      <td>117.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>pair_001</td>\n",
              "      <td>/home/natasha/multimodal_model/data/test/_chun...</td>\n",
              "      <td>test</td>\n",
              "      <td>chunk_000</td>\n",
              "      <td>GSLSPELRPIF</td>\n",
              "      <td>GEDVEQSLFLSVREGDSSVINCTYTDSSSTYLYWYKQEPGAGLQLL...</td>\n",
              "      <td>DGGITQSPKYLFRKEGQNVTLSCEQNLNHDAMYWYRQDPGQGLRLI...</td>\n",
              "      <td>MRVTAPRTVLLLLWGAVALTETWAGSHSMRYFYTAMSRPGRGEPRF...</td>\n",
              "      <td>11</td>\n",
              "      <td>111.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>pair_002</td>\n",
              "      <td>/home/natasha/multimodal_model/data/test/_chun...</td>\n",
              "      <td>test</td>\n",
              "      <td>chunk_000</td>\n",
              "      <td>GTIRPEIPDYF</td>\n",
              "      <td>GEDVEQSLFLSVREGDSSVINCTYTDSSSTYLYWYKQEPGAGLQLL...</td>\n",
              "      <td>DGGITQSPKYLFRKEGQNVTLSCEQNLNHDAMYWYRQDPGQGLRLI...</td>\n",
              "      <td>MRVTAPRTVLLLLWGAVALTETWAGSHSMRYFYTAMSRPGRGEPRF...</td>\n",
              "      <td>11</td>\n",
              "      <td>111.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>pair_003</td>\n",
              "      <td>/home/natasha/multimodal_model/data/test/_chun...</td>\n",
              "      <td>test</td>\n",
              "      <td>chunk_000</td>\n",
              "      <td>KAFSPEVIPMF</td>\n",
              "      <td>GEDVEQSLFLSVREGDSSVINCTYTDSSSTYLYWYKQEPGAGLQLL...</td>\n",
              "      <td>DGGITQSPKYLFRKEGQNVTLSCEQNLNHDAMYWYRQDPGQGLRLI...</td>\n",
              "      <td>MRVTAPRTVLLLLWGAVALTETWAGSHSMRYFYTAMSRPGRGEPRF...</td>\n",
              "      <td>11</td>\n",
              "      <td>111.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>pair_004</td>\n",
              "      <td>/home/natasha/multimodal_model/data/test/_chun...</td>\n",
              "      <td>test</td>\n",
              "      <td>chunk_000</td>\n",
              "      <td>KSLTPEVRGYW</td>\n",
              "      <td>GEDVEQSLFLSVREGDSSVINCTYTDSSSTYLYWYKQEPGAGLQLL...</td>\n",
              "      <td>DGGITQSPKYLFRKEGQNVTLSCEQNLNHDAMYWYRQDPGQGLRLI...</td>\n",
              "      <td>MRVTAPRTVLLLLWGAVALTETWAGSHSMRYFYTAMSRPGRGEPRF...</td>\n",
              "      <td>11</td>\n",
              "      <td>111.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>362</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    pair_id                                          yaml_path split  \\\n",
              "0  pair_000  /home/natasha/multimodal_model/data/test/_chun...  test   \n",
              "1  pair_001  /home/natasha/multimodal_model/data/test/_chun...  test   \n",
              "2  pair_002  /home/natasha/multimodal_model/data/test/_chun...  test   \n",
              "3  pair_003  /home/natasha/multimodal_model/data/test/_chun...  test   \n",
              "4  pair_004  /home/natasha/multimodal_model/data/test/_chun...  test   \n",
              "\n",
              "       chunk      pep_seq                                           tcra_seq  \\\n",
              "0  chunk_000   TSTLQEQIGW  EAGVTQFPSHSVIEKGQTVTLRCDPISGHDNLYWYRRVMGKEIKFL...   \n",
              "1  chunk_000  GSLSPELRPIF  GEDVEQSLFLSVREGDSSVINCTYTDSSSTYLYWYKQEPGAGLQLL...   \n",
              "2  chunk_000  GTIRPEIPDYF  GEDVEQSLFLSVREGDSSVINCTYTDSSSTYLYWYKQEPGAGLQLL...   \n",
              "3  chunk_000  KAFSPEVIPMF  GEDVEQSLFLSVREGDSSVINCTYTDSSSTYLYWYKQEPGAGLQLL...   \n",
              "4  chunk_000  KSLTPEVRGYW  GEDVEQSLFLSVREGDSSVINCTYTDSSSTYLYWYKQEPGAGLQLL...   \n",
              "\n",
              "                                            tcrb_seq  \\\n",
              "0  GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQL...   \n",
              "1  DGGITQSPKYLFRKEGQNVTLSCEQNLNHDAMYWYRQDPGQGLRLI...   \n",
              "2  DGGITQSPKYLFRKEGQNVTLSCEQNLNHDAMYWYRQDPGQGLRLI...   \n",
              "3  DGGITQSPKYLFRKEGQNVTLSCEQNLNHDAMYWYRQDPGQGLRLI...   \n",
              "4  DGGITQSPKYLFRKEGQNVTLSCEQNLNHDAMYWYRQDPGQGLRLI...   \n",
              "\n",
              "                                             hla_seq  pep_len  tcra_len  \\\n",
              "0  MRVTAPRTVLLLLWGAVALTETWAGSHSMRYFYTAMSRPGRGEPRF...       10     117.0   \n",
              "1  MRVTAPRTVLLLLWGAVALTETWAGSHSMRYFYTAMSRPGRGEPRF...       11     111.0   \n",
              "2  MRVTAPRTVLLLLWGAVALTETWAGSHSMRYFYTAMSRPGRGEPRF...       11     111.0   \n",
              "3  MRVTAPRTVLLLLWGAVALTETWAGSHSMRYFYTAMSRPGRGEPRF...       11     111.0   \n",
              "4  MRVTAPRTVLLLLWGAVALTETWAGSHSMRYFYTAMSRPGRGEPRF...       11     111.0   \n",
              "\n",
              "   tcrb_len  hla_len  \n",
              "0     112.0      362  \n",
              "1     111.0      362  \n",
              "2     111.0      362  \n",
              "3     111.0      362  \n",
              "4     111.0      362  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def _infer_split_and_chunk(yaml_path: Path):\n",
        "    p = str(yaml_path)\n",
        "    split = None\n",
        "    chunk = None\n",
        "\n",
        "    # split inference\n",
        "    if \"/data/train/\" in p:\n",
        "        split = \"train\"\n",
        "    elif \"/data/val/\" in p:\n",
        "        split = \"val\"\n",
        "    elif \"/data/test/\" in p:\n",
        "        split = \"test\"\n",
        "    else:\n",
        "        split = \"unknown\"\n",
        "\n",
        "    # chunk inference\n",
        "    m = re.search(r\"/_chunks/(chunk_\\d{3})/\", p)\n",
        "    if m:\n",
        "        chunk = m.group(1)\n",
        "    return split, chunk\n",
        "\n",
        "\n",
        "def build_pair_table(data_dir: Path) -> pd.DataFrame:\n",
        "    yamls = sorted(data_dir.rglob(\"*.yaml\"))\n",
        "    rows = []\n",
        "    bad = []\n",
        "\n",
        "    for y in yamls:\n",
        "        pair_id = y.stem  # expects pair_XXXX\n",
        "        split, chunk = _infer_split_and_chunk(y)\n",
        "\n",
        "        try:\n",
        "            obj = yaml.safe_load(y.read_text())\n",
        "        except Exception as e:\n",
        "            bad.append((y, f\"YAML parse error: {e}\"))\n",
        "            continue\n",
        "\n",
        "        entries = _collect_sequence_entries(obj)\n",
        "        pep, tcra, tcrb, hla = _assign_roles(entries)\n",
        "\n",
        "        # If any are None, track as bad (you can decide whether to keep)\n",
        "        if pep is None or hla is None:\n",
        "            bad.append((y, f\"Missing pep or hla after parsing (pep={pep is not None}, hla={hla is not None})\"))\n",
        "            continue\n",
        "\n",
        "        rows.append({\n",
        "            \"pair_id\": pair_id,\n",
        "            \"yaml_path\": str(y),\n",
        "            \"split\": split,\n",
        "            \"chunk\": chunk,\n",
        "            \"pep_seq\": pep,\n",
        "            \"tcra_seq\": tcra,\n",
        "            \"tcrb_seq\": tcrb,\n",
        "            \"hla_seq\": hla,\n",
        "            \"pep_len\": len(pep) if pep else None,\n",
        "            \"tcra_len\": len(tcra) if tcra else None,\n",
        "            \"tcrb_len\": len(tcrb) if tcrb else None,\n",
        "            \"hla_len\": len(hla) if hla else None,\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows).sort_values([\"split\", \"chunk\", \"pair_id\"], na_position=\"last\").reset_index(drop=True)\n",
        "\n",
        "    print(f\"Found YAMLs: {len(yamls)}\")\n",
        "    print(f\"Parsed OK : {len(df)}\")\n",
        "    print(f\"Bad/skip  : {len(bad)}\")\n",
        "    if bad:\n",
        "        print(\"\\nFirst 10 skipped:\")\n",
        "        for p, reason in bad[:10]:\n",
        "            print(\" -\", p, \"=>\", reason)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "pair_table = build_pair_table(DATA_DIR)\n",
        "pair_table.to_csv(OUT_PATH, index=False)\n",
        "print(f\"\\nWrote: {OUT_PATH}\")\n",
        "pair_table.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fd88d54",
      "metadata": {},
      "source": [
        "Take pair_id table and build pair_id in files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba052bc8",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_303492/2995586741.py:12: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  pair_table = pd.read_csv(PAIR_TABLE_PATH)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pair_table rows: 70525 unique pair_ids: 32679\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "PAIR_TABLE_PATH = Path(\"/home/natasha/multimodal_model/manifests/pair_table.csv\")\n",
        "\n",
        "def norm_seq(s):\n",
        "    if pd.isna(s):\n",
        "        return \"\"\n",
        "    return \"\".join([c for c in str(s).strip().upper() if c.isalpha()])\n",
        "\n",
        "pair_table = pd.read_csv(PAIR_TABLE_PATH)\n",
        "\n",
        "# normalised join keys\n",
        "pair_table[\"Peptide_n\"] = pair_table[\"pep_seq\"].map(norm_seq)\n",
        "pair_table[\"TCRa_n\"] = pair_table[\"tcra_seq\"].fillna(\"\").map(norm_seq)\n",
        "pair_table[\"TCRb_n\"] = pair_table[\"tcrb_seq\"].fillna(\"\").map(norm_seq)\n",
        "pair_table[\"HLAseq_n\"] = pair_table[\"hla_seq\"].map(norm_seq)\n",
        "\n",
        "pair_table_keyed = pair_table[[\"pair_id\",\"split\",\"chunk\",\"yaml_path\",\"Peptide_n\",\"TCRa_n\",\"TCRb_n\",\"HLAseq_n\"]].copy()\n",
        "\n",
        "print(\"pair_table rows:\", len(pair_table_keyed), \"unique pair_ids:\", pair_table_keyed[\"pair_id\"].nunique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44acef76",
      "metadata": {},
      "outputs": [],
      "source": [
        "def tag_df_with_pair_id(df_path: Path, split_name: str) -> None:\n",
        "    df = pd.read_csv(df_path)\n",
        "\n",
        "    # Make join keys from df (adapt column names if yours differ)\n",
        "    df[\"Peptide_n\"] = df[\"Peptide\"].map(norm_seq)\n",
        "    df[\"TCRa_n\"] = df[\"TCRa\"].fillna(\"\").map(norm_seq)\n",
        "    df[\"TCRb_n\"] = df[\"TCRb\"].fillna(\"\").map(norm_seq)\n",
        "    df[\"HLAseq_n\"] = df[\"HLA_sequence\"].map(norm_seq)\n",
        "\n",
        "    # Restrict pair_table to the same split to reduce accidental collisions\n",
        "    pt = pair_table_keyed[pair_table_keyed[\"split\"] == split_name].copy()\n",
        "\n",
        "    merged = df.merge(\n",
        "        pt,\n",
        "        how=\"left\",\n",
        "        on=[\"Peptide_n\",\"TCRa_n\",\"TCRb_n\",\"HLAseq_n\"],\n",
        "        suffixes=(\"\", \"_pt\"),\n",
        "        validate=\"m:1\"  # each df row should map to at most one pair_id\n",
        "    )\n",
        "\n",
        "    # counts\n",
        "    n_total = len(merged)\n",
        "    n_tagged = merged[\"pair_id\"].notna().sum()\n",
        "    n_missing = n_total - n_tagged\n",
        "\n",
        "    print(f\"\\n[{split_name}] {df_path}\")\n",
        "    print(\"  total rows :\", n_total)\n",
        "    print(\"  tagged     :\", n_tagged)\n",
        "    print(\"  missing    :\", n_missing)\n",
        "\n",
        "    # Backup then write updated CSV\n",
        "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    backup_path = df_path.with_suffix(df_path.suffix + f\".bak_{ts}\")\n",
        "    df_path.rename(backup_path)\n",
        "    print(\"  backup ->\", backup_path)\n",
        "\n",
        "    # Keep original columns, but add pair_id (and optionally yaml_path/chunk)\n",
        "    out = merged.copy()\n",
        "    # drop helper cols\n",
        "    out = out.drop(columns=[\"Peptide_n\",\"TCRa_n\",\"TCRb_n\",\"HLAseq_n\"], errors=\"ignore\")\n",
        "\n",
        "    # Optional: keep yaml_path + chunk for easier debugging\n",
        "    # out already has yaml_path/chunk from pair_table\n",
        "\n",
        "    out.to_csv(df_path, index=False)\n",
        "    print(\"  wrote  ->\", df_path)\n",
        "\n",
        "    # If anything is missing, save a diagnostic file\n",
        "    if n_missing > 0:\n",
        "        miss_path = df_path.with_suffix(df_path.suffix + f\".missing_pair_id_{ts}.csv\")\n",
        "        out[out[\"pair_id\"].isna()].to_csv(miss_path, index=False)\n",
        "        print(\"  missing rows saved ->\", miss_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b6ea186",
      "metadata": {},
      "outputs": [
        {
          "ename": "MergeError",
          "evalue": "Merge keys are not unique in right dataset; not a many-to-one merge",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mMergeError\u001b[0m                                Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n",
            "\u001b[0;32m----> 1\u001b[0m \u001b[43mtag_df_with_pair_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/natasha/multimodal_model/data/train/train_df.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m      2\u001b[0m tag_df_with_pair_id(Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/natasha/multimodal_model/data/val/val_df.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;32m      3\u001b[0m tag_df_with_pair_id(Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/natasha/multimodal_model/data/test/test_df.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\n",
            "Cell \u001b[0;32mIn[11], line 13\u001b[0m, in \u001b[0;36mtag_df_with_pair_id\u001b[0;34m(df_path, split_name)\u001b[0m\n",
            "\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Restrict pair_table to the same split to reduce accidental collisions\u001b[39;00m\n",
            "\u001b[1;32m     11\u001b[0m pt \u001b[38;5;241m=\u001b[39m pair_table_keyed[pair_table_keyed[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m split_name]\u001b[38;5;241m.\u001b[39mcopy()\n",
            "\u001b[0;32m---> 13\u001b[0m merged \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n",
            "\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpt\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPeptide_n\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTCRa_n\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTCRb_n\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHLAseq_n\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mm:1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# each df row should map to at most one pair_id\u001b[39;49;00m\n",
            "\u001b[1;32m     19\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# counts\u001b[39;00m\n",
            "\u001b[1;32m     22\u001b[0m n_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(merged)\n",
            "\n",
            "File \u001b[0;32m~/miniconda3/envs/tcr-multimodal/lib/python3.10/site-packages/pandas/core/frame.py:10859\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n",
            "\u001b[1;32m  10840\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;32m  10841\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
            "\u001b[1;32m  10842\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmerge\u001b[39m(\n",
            "\u001b[0;32m   (...)\u001b[0m\n",
            "\u001b[1;32m  10855\u001b[0m     validate: MergeValidate \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
            "\u001b[1;32m  10856\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n",
            "\u001b[1;32m  10857\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m merge\n",
            "\u001b[0;32m> 10859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n",
            "\u001b[1;32m  10860\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m  10861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m  10862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m  10863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m  10864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m  10865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m  10866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m  10867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m  10868\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m  10869\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m  10870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m  10871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m  10872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m  10873\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\n",
            "File \u001b[0;32m~/miniconda3/envs/tcr-multimodal/lib/python3.10/site-packages/pandas/core/reshape/merge.py:170\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n",
            "\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n",
            "\u001b[1;32m    156\u001b[0m         left_df,\n",
            "\u001b[1;32m    157\u001b[0m         right_df,\n",
            "\u001b[0;32m   (...)\u001b[0m\n",
            "\u001b[1;32m    167\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n",
            "\u001b[1;32m    168\u001b[0m     )\n",
            "\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[0;32m--> 170\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n",
            "\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
            "\n",
            "File \u001b[0;32m~/miniconda3/envs/tcr-multimodal/lib/python3.10/site-packages/pandas/core/reshape/merge.py:813\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n",
            "\u001b[1;32m    809\u001b[0m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n",
            "\u001b[1;32m    810\u001b[0m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n",
            "\u001b[1;32m    811\u001b[0m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n",
            "\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;32m--> 813\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_validate_kwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m)\u001b[49m\n",
            "\n",
            "File \u001b[0;32m~/miniconda3/envs/tcr-multimodal/lib/python3.10/site-packages/pandas/core/reshape/merge.py:1670\u001b[0m, in \u001b[0;36m_MergeOperation._validate_validate_kwd\u001b[0;34m(self, validate)\u001b[0m\n",
            "\u001b[1;32m   1668\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m validate \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmany_to_one\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm:1\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
            "\u001b[1;32m   1669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m right_unique:\n",
            "\u001b[0;32m-> 1670\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MergeError(\n",
            "\u001b[1;32m   1671\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMerge keys are not unique in right dataset; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;32m   1672\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot a many-to-one merge\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;32m   1673\u001b[0m         )\n",
            "\u001b[1;32m   1675\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m validate \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmany_to_many\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm:m\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
            "\u001b[1;32m   1676\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "\n",
            "\u001b[0;31mMergeError\u001b[0m: Merge keys are not unique in right dataset; not a many-to-one merge"
          ]
        }
      ],
      "source": [
        "tag_df_with_pair_id(Path(\"/home/natasha/multimodal_model/data/train/train_df.csv\"), \"train\")\n",
        "tag_df_with_pair_id(Path(\"/home/natasha/multimodal_model/data/val/val_df.csv\"), \"val\")\n",
        "tag_df_with_pair_id(Path(\"/home/natasha/multimodal_model/data/test/test_df.csv\"), \"test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c009f29",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "def norm_seq(s):\n",
        "    if pd.isna(s):\n",
        "        return \"\"\n",
        "    return \"\".join([c for c in str(s).strip().upper() if c.isalpha()])\n",
        "\n",
        "def tag_df_with_pair_id_allow_dupes(df_path: Path, split_name: str, pair_table: pd.DataFrame) -> None:\n",
        "    df = pd.read_csv(df_path)\n",
        "\n",
        "    # build df keys\n",
        "    df[\"_pep\"]  = df[\"Peptide\"].map(norm_seq)\n",
        "    df[\"_a\"]    = df[\"TCRa\"].fillna(\"\").map(norm_seq)\n",
        "    df[\"_b\"]    = df[\"TCRb\"].fillna(\"\").map(norm_seq)\n",
        "    df[\"_hla\"]  = df[\"HLA_sequence\"].map(norm_seq)\n",
        "\n",
        "    # subset pair_table to split, and key it\n",
        "    pt = pair_table[pair_table[\"split\"] == split_name].copy()\n",
        "    pt[\"_pep\"] = pt[\"pep_seq\"].map(norm_seq)\n",
        "    pt[\"_a\"]   = pt[\"tcra_seq\"].fillna(\"\").map(norm_seq)\n",
        "    pt[\"_b\"]   = pt[\"tcrb_seq\"].fillna(\"\").map(norm_seq)\n",
        "    pt[\"_hla\"] = pt[\"hla_seq\"].map(norm_seq)\n",
        "\n",
        "    # include lengths as additional key to reduce collisions\n",
        "    pt[\"_pep_len\"]  = pt[\"pep_len\"]\n",
        "    pt[\"_a_len\"]    = pt[\"tcra_len\"].fillna(0).astype(int)\n",
        "    pt[\"_b_len\"]    = pt[\"tcrb_len\"].fillna(0).astype(int)\n",
        "    pt[\"_hla_len\"]  = pt[\"hla_len\"]\n",
        "\n",
        "    df[\"_pep_len\"]  = df[\"_pep\"].str.len()\n",
        "    df[\"_a_len\"]    = df[\"_a\"].str.len()\n",
        "    df[\"_b_len\"]    = df[\"_b\"].str.len()\n",
        "    df[\"_hla_len\"]  = df[\"_hla\"].str.len()\n",
        "\n",
        "    key_cols = [\"_pep\",\"_a\",\"_b\",\"_hla\",\"_pep_len\",\"_a_len\",\"_b_len\",\"_hla_len\"]\n",
        "\n",
        "    # group both sides by key, then assign pair_ids within each key group by order\n",
        "    df[\"pair_id\"] = pd.NA\n",
        "\n",
        "    pt_groups = {k: g[\"pair_id\"].tolist() for k, g in pt.groupby(key_cols, dropna=False)}\n",
        "    used = set()\n",
        "\n",
        "    missing_keys = 0\n",
        "    overfull_keys = 0\n",
        "\n",
        "    for k, idxs in df.groupby(key_cols, dropna=False).groups.items():\n",
        "        candidates = pt_groups.get(k, [])\n",
        "        if not candidates:\n",
        "            missing_keys += len(idxs)\n",
        "            continue\n",
        "\n",
        "        # remove already used\n",
        "        candidates = [c for c in candidates if c not in used]\n",
        "\n",
        "        if len(candidates) < len(idxs):\n",
        "            # not enough candidates to assign uniquely\n",
        "            overfull_keys += (len(idxs) - len(candidates))\n",
        "\n",
        "        # assign as many as we can\n",
        "        for row_i, pid in zip(list(idxs), candidates):\n",
        "            df.at[row_i, \"pair_id\"] = pid\n",
        "            used.add(pid)\n",
        "\n",
        "    n_total = len(df)\n",
        "    n_tagged = df[\"pair_id\"].notna().sum()\n",
        "    n_missing = n_total - n_tagged\n",
        "\n",
        "    print(f\"\\n[{split_name}] {df_path}\")\n",
        "    print(\"  total rows :\", n_total)\n",
        "    print(\"  tagged     :\", n_tagged)\n",
        "    print(\"  missing    :\", n_missing)\n",
        "    print(\"  missing_keys_rows:\", missing_keys)\n",
        "    print(\"  overfull_key_rows:\", overfull_keys)\n",
        "\n",
        "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    backup_path = df_path.with_suffix(df_path.suffix + f\".bak_{ts}\")\n",
        "    df_path.rename(backup_path)\n",
        "    print(\"  backup ->\", backup_path)\n",
        "\n",
        "    # drop helper cols\n",
        "    df_out = df.drop(columns=key_cols, errors=\"ignore\")\n",
        "    df_out.to_csv(df_path, index=False)\n",
        "    print(\"  wrote  ->\", df_path)\n",
        "\n",
        "    if n_missing > 0:\n",
        "        miss_path = df_path.with_suffix(df_path.suffix + f\".missing_pair_id_{ts}.csv\")\n",
        "        df_out[df_out[\"pair_id\"].isna()].to_csv(miss_path, index=False)\n",
        "        print(\"  missing rows saved ->\", miss_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d13ad56",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_303492/4189138952.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  pair_table = pd.read_csv(\"/home/natasha/multimodal_model/manifests/pair_table.csv\")\n"
          ]
        },
        {
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: EOF inside string starting at row 11522",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n",
            "\u001b[1;32m      1\u001b[0m pair_table \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/natasha/multimodal_model/manifests/pair_table.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;32m----> 3\u001b[0m \u001b[43mtag_df_with_pair_id_allow_dupes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/natasha/multimodal_model/data/train/train_df.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair_table\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m      4\u001b[0m tag_df_with_pair_id_allow_dupes(Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/natasha/multimodal_model/data/val/val_df.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m, pair_table)\n",
            "\u001b[1;32m      5\u001b[0m tag_df_with_pair_id_allow_dupes(Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/natasha/multimodal_model/data/test/test_df.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, pair_table)\n",
            "\n",
            "Cell \u001b[0;32mIn[13], line 11\u001b[0m, in \u001b[0;36mtag_df_with_pair_id_allow_dupes\u001b[0;34m(df_path, split_name, pair_table)\u001b[0m\n",
            "\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtag_df_with_pair_id_allow_dupes\u001b[39m(df_path: Path, split_name: \u001b[38;5;28mstr\u001b[39m, pair_table: pd\u001b[38;5;241m.\u001b[39mDataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;32m---> 11\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_path\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# build df keys\u001b[39;00m\n",
            "\u001b[1;32m     14\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_pep\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPeptide\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(norm_seq)\n",
            "\n",
            "File \u001b[0;32m~/miniconda3/envs/tcr-multimodal/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n",
            "\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n",
            "\u001b[1;32m   1014\u001b[0m     dialect,\n",
            "\u001b[1;32m   1015\u001b[0m     delimiter,\n",
            "\u001b[0;32m   (...)\u001b[0m\n",
            "\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n",
            "\u001b[1;32m   1023\u001b[0m )\n",
            "\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n",
            "\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\n",
            "File \u001b[0;32m~/miniconda3/envs/tcr-multimodal/lib/python3.10/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n",
            "\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n",
            "\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
            "\n",
            "File \u001b[0;32m~/miniconda3/envs/tcr-multimodal/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n",
            "\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n",
            "\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n",
            "\u001b[1;32m   1919\u001b[0m     (\n",
            "\u001b[1;32m   1920\u001b[0m         index,\n",
            "\u001b[1;32m   1921\u001b[0m         columns,\n",
            "\u001b[1;32m   1922\u001b[0m         col_dict,\n",
            "\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n",
            "\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n",
            "\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
            "\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
            "\n",
            "File \u001b[0;32m~/miniconda3/envs/tcr-multimodal/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n",
            "\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n",
            "\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n",
            "\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
            "\n",
            "File \u001b[0;32mpandas/_libs/parsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\n",
            "File \u001b[0;32mpandas/_libs/parsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\n",
            "File \u001b[0;32mpandas/_libs/parsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\n",
            "File \u001b[0;32mpandas/_libs/parsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\n",
            "File \u001b[0;32mpandas/_libs/parsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 11522"
          ]
        }
      ],
      "source": [
        "pair_table = pd.read_csv(\"/home/natasha/multimodal_model/manifests/pair_table.csv\")\n",
        "\n",
        "tag_df_with_pair_id_allow_dupes(Path(\"/home/natasha/multimodal_model/data/train/train_df.csv\"), \"train\", pair_table)\n",
        "tag_df_with_pair_id_allow_dupes(Path(\"/home/natasha/multimodal_model/data/val/val_df.csv\"), \"val\", pair_table)\n",
        "tag_df_with_pair_id_allow_dupes(Path(\"/home/natasha/multimodal_model/data/test/test_df.csv\"), \"test\", pair_table)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae8113a5",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_303492/1277184969.py:10: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  pt = pd.read_csv(PAIR_TABLE_PATH)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[train] wrote 63,945 rows -> /home/natasha/multimodal_model/data/train/train_df.fixed.csv\n",
            "[val] wrote 1,968 rows -> /home/natasha/multimodal_model/data/val/val_df.fixed.csv\n",
            "[test] wrote 4,410 rows -> /home/natasha/multimodal_model/data/test/test_df.fixed.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "PAIR_TABLE_PATH = Path(\"/home/natasha/multimodal_model/manifests/pair_table.csv\")\n",
        "\n",
        "OUT_TRAIN = Path(\"/home/natasha/multimodal_model/data/train/train_df.fixed.csv\")\n",
        "OUT_VAL   = Path(\"/home/natasha/multimodal_model/data/val/val_df.fixed.csv\")\n",
        "OUT_TEST  = Path(\"/home/natasha/multimodal_model/data/test/test_df.fixed.csv\")\n",
        "\n",
        "pt = pd.read_csv(PAIR_TABLE_PATH)\n",
        "\n",
        "def write_fixed(split: str, out_path: Path):\n",
        "    sub = pt[pt[\"split\"] == split].copy()\n",
        "\n",
        "    fixed = pd.DataFrame({\n",
        "        \"pair_id\": sub[\"pair_id\"],\n",
        "        \"Peptide\": sub[\"pep_seq\"].fillna(\"\"),\n",
        "        \"TCRa\": sub[\"tcra_seq\"].fillna(\"\"),\n",
        "        \"TCRb\": sub[\"tcrb_seq\"].fillna(\"\"),\n",
        "        \"HLA_sequence\": sub[\"hla_seq\"].fillna(\"\"),\n",
        "        \"yaml_path\": sub[\"yaml_path\"],\n",
        "        \"chunk\": sub[\"chunk\"],\n",
        "        # keep lengths because your model wants them\n",
        "        \"pep_len\": sub[\"pep_len\"],\n",
        "        \"tcra_len\": sub[\"tcra_len\"],\n",
        "        \"tcrb_len\": sub[\"tcrb_len\"],\n",
        "        \"hla_len\": sub[\"hla_len\"],\n",
        "    })\n",
        "\n",
        "    fixed.to_csv(out_path, index=False)\n",
        "    print(f\"[{split}] wrote {len(fixed):,} rows -> {out_path}\")\n",
        "\n",
        "write_fixed(\"train\", OUT_TRAIN)\n",
        "write_fixed(\"val\",   OUT_VAL)\n",
        "write_fixed(\"test\",  OUT_TEST)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "afa1ea10",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== TRAIN chunk_000 ===\n",
            "CMD: conda run -n boltz-env --no-capture-output boltz predict /home/natasha/multimodal_model/data/train/_chunks/chunk_000 --out_dir /home/natasha/multimodal_model/outputs/train/chunk_000 --accelerator gpu --devices 1 --model boltz2 --recycling_steps 1 --sampling_steps 20 --diffusion_samples 1 --max_parallel_samples 1 --max_msa_seqs 64 --num_subsampled_msa 64 --override --write_embeddings\n",
            "Return code: 0\n",
            "\n",
            "=== TRAIN chunk_001 ===\n",
            "CMD: conda run -n boltz-env --no-capture-output boltz predict /home/natasha/multimodal_model/data/train/_chunks/chunk_001 --out_dir /home/natasha/multimodal_model/outputs/train/chunk_001 --accelerator gpu --devices 1 --model boltz2 --recycling_steps 1 --sampling_steps 20 --diffusion_samples 1 --max_parallel_samples 1 --max_msa_seqs 64 --num_subsampled_msa 64 --override --write_embeddings\n",
            "Return code: 0\n",
            "\n",
            "=== TRAIN chunk_002 ===\n",
            "CMD: conda run -n boltz-env --no-capture-output boltz predict /home/natasha/multimodal_model/data/train/_chunks/chunk_002 --out_dir /home/natasha/multimodal_model/outputs/train/chunk_002 --accelerator gpu --devices 1 --model boltz2 --recycling_steps 1 --sampling_steps 20 --diffusion_samples 1 --max_parallel_samples 1 --max_msa_seqs 64 --num_subsampled_msa 64 --override --write_embeddings\n",
            "Return code: 1\n",
            "[STOP] Train chunk failed: chunk_002. See logs in /home/natasha/multimodal_model/outputs/train/chunk_002\n"
          ]
        }
      ],
      "source": [
        "# RUN TRAIN CHUNKS\n",
        "\n",
        "def list_chunk_dirs(chunks_root: Path):\n",
        "    chunks_root = Path(chunks_root).resolve()\n",
        "    if not chunks_root.exists():\n",
        "        raise FileNotFoundError(f\"Chunks root not found: {chunks_root}\")\n",
        "\n",
        "    # chunk_000, chunk_001, ...\n",
        "    chunk_dirs = sorted([p for p in chunks_root.iterdir() if p.is_dir()])\n",
        "    if not chunk_dirs:\n",
        "        raise ValueError(f\"No chunk directories found in: {chunks_root}\")\n",
        "    return chunk_dirs\n",
        "\n",
        "train_chunk_dirs = list_chunk_dirs(TRAIN_CHUNKS_ROOT)\n",
        "\n",
        "for chunk_dir in train_chunk_dirs:\n",
        "    chunk_name = chunk_dir.name\n",
        "    outdir = BOLTZ_OUT_TRAIN / chunk_name\n",
        "    print(f\"\\n=== TRAIN {chunk_name} ===\")\n",
        "    rc = run_cli(chunk_dir, outdir)\n",
        "    if rc != 0:\n",
        "        print(f\"[STOP] Train chunk failed: {chunk_name}. See logs in {outdir}\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c259b15",
      "metadata": {},
      "outputs": [],
      "source": [
        "# RUN TEST CHUNKS\n",
        "\n",
        "test_chunk_dirs = list_chunk_dirs(TEST_CHUNKS_ROOT)\n",
        "\n",
        "for chunk_dir in test_chunk_dirs:\n",
        "    chunk_name = chunk_dir.name\n",
        "    outdir = BOLTZ_OUT_TEST / chunk_name\n",
        "    print(f\"\\n=== TEST {chunk_name} ===\")\n",
        "    rc = run_cli(chunk_dir, outdir)\n",
        "    if rc != 0:\n",
        "        print(f\"[STOP] Test chunk failed: {chunk_name}. See logs in {outdir}\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "214cf602",
      "metadata": {},
      "outputs": [],
      "source": [
        "# RUN VAL CHUNK\n",
        "\n",
        "print(\"\\n=== VAL (single directory) ===\")\n",
        "rc = run_cli(YAML_DIR_VAL, BOLTZ_OUT_VAL / \"val_all\")\n",
        "if rc != 0:\n",
        "    print(f\"[FAIL] Val run failed. See logs in {BOLTZ_OUT_VAL / 'val_all'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89e80fcb",
      "metadata": {},
      "source": [
        "Creating Symlink Folders + Troubleshooting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d4d7ce9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import math\n",
        "import shutil\n",
        "from typing import Literal, Optional\n",
        "\n",
        "def chunk_yaml_directory(\n",
        "    src_dir: Path,\n",
        "    *,\n",
        "    chunk_size: int = 2000,\n",
        "    chunk_prefix: str = \"chunk_\",\n",
        "    dst_parent: Optional[Path] = None,\n",
        "    mode: Literal[\"symlink\", \"hardlink\", \"copy\", \"move\"] = \"symlink\",\n",
        "    pattern: str = \"*.yaml\",\n",
        "    overwrite: bool = False,\n",
        ") -> Path:\n",
        "    \"\"\"\n",
        "    Split a directory of YAML files into chunk subfolders.\n",
        "\n",
        "    By default, creates symlinks (fast, no duplication, does not break existing paths).\n",
        "\n",
        "    Args:\n",
        "        src_dir: Directory containing YAMLs (e.g., .../data/train).\n",
        "        chunk_size: Number of YAMLs per chunk folder.\n",
        "        chunk_prefix: Folder name prefix (chunk_000, chunk_001, ...).\n",
        "        dst_parent: Where to create chunk folders. Defaults to src_dir / \"_chunks\".\n",
        "        mode: One of {\"symlink\",\"hardlink\",\"copy\",\"move\"}.\n",
        "        pattern: Glob pattern for YAMLs.\n",
        "        overwrite: If True, overwrites existing links/files inside chunk dirs.\n",
        "\n",
        "    Returns:\n",
        "        Path to the chunk root directory.\n",
        "    \"\"\"\n",
        "    src_dir = Path(src_dir).resolve()\n",
        "    if not src_dir.exists():\n",
        "        raise FileNotFoundError(f\"src_dir does not exist: {src_dir}\")\n",
        "    if chunk_size <= 0:\n",
        "        raise ValueError(\"chunk_size must be > 0\")\n",
        "\n",
        "    yamls = sorted(src_dir.glob(pattern))\n",
        "    if not yamls:\n",
        "        raise ValueError(f\"No files matching {pattern} found in {src_dir}\")\n",
        "\n",
        "    # Put chunks under src_dir/_chunks by default (keeps original directory intact)\n",
        "    chunk_root = (dst_parent or (src_dir / \"_chunks\")).resolve()\n",
        "    chunk_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    n = len(yamls)\n",
        "    n_chunks = math.ceil(n / chunk_size)\n",
        "\n",
        "    def _place(src: Path, dst: Path):\n",
        "        if dst.exists() or dst.is_symlink():\n",
        "            if overwrite:\n",
        "                dst.unlink()\n",
        "            else:\n",
        "                return  # keep existing\n",
        "        if mode == \"symlink\":\n",
        "            os.symlink(src, dst)\n",
        "        elif mode == \"hardlink\":\n",
        "            os.link(src, dst)\n",
        "        elif mode == \"copy\":\n",
        "            shutil.copy2(src, dst)\n",
        "        elif mode == \"move\":\n",
        "            shutil.move(str(src), str(dst))\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown mode: {mode}\")\n",
        "\n",
        "    for i in range(n_chunks):\n",
        "        chunk_dir = chunk_root / f\"{chunk_prefix}{i:03d}\"\n",
        "        chunk_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        start = i * chunk_size\n",
        "        end = min((i + 1) * chunk_size, n)\n",
        "        for src in yamls[start:end]:\n",
        "            dst = chunk_dir / src.name\n",
        "            _place(src, dst)\n",
        "\n",
        "    return chunk_root\n",
        "\n",
        "\n",
        "def make_train_test_chunks(\n",
        "    base_data_dir: Path,\n",
        "    *,\n",
        "    train_subdir: str = \"train\",\n",
        "    test_subdir: str = \"test\",\n",
        "    chunk_size: int = 2000,\n",
        "    mode: Literal[\"symlink\", \"hardlink\", \"copy\", \"move\"] = \"symlink\",\n",
        ") -> tuple[Path, Path]:\n",
        "    \"\"\"\n",
        "    Create chunk folders for train and test YAML directories.\n",
        "    \"\"\"\n",
        "    base_data_dir = Path(base_data_dir).resolve()\n",
        "\n",
        "    train_dir = base_data_dir / train_subdir\n",
        "    test_dir  = base_data_dir / test_subdir\n",
        "\n",
        "    train_chunks = chunk_yaml_directory(train_dir, chunk_size=chunk_size, mode=mode)\n",
        "    test_chunks  = chunk_yaml_directory(test_dir,  chunk_size=chunk_size, mode=mode)\n",
        "\n",
        "    return train_chunks, test_chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffdf7af5",
      "metadata": {},
      "outputs": [],
      "source": [
        "BASE_DATA = Path(\"/home/natasha/multimodal_model/data\")\n",
        "train_chunks_root, test_chunks_root = make_train_test_chunks(\n",
        "    BASE_DATA,\n",
        "    chunk_size=2000,\n",
        "    mode=\"symlink\",  # recommended\n",
        ")\n",
        "print(\"Train chunks:\", train_chunks_root)\n",
        "print(\"Test chunks:\", test_chunks_root)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01099b8e",
      "metadata": {},
      "source": [
        "Find 'bad' YAML files (Files with missing sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3158f2ac",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bad YAMLs: 14\n",
            "pair_8118.yaml -> null/empty sequence at line 17: sequence:\n",
            "pair_11302.yaml -> null/empty sequence at line 17: sequence:\n",
            "pair_11083.yaml -> null/empty sequence at line 17: sequence:\n",
            "pair_9184.yaml -> null/empty sequence at line 17: sequence:\n",
            "pair_9558.yaml -> null/empty sequence at line 17: sequence:\n",
            "pair_10061.yaml -> null/empty sequence at line 17: sequence:\n",
            "pair_11244.yaml -> null/empty sequence at line 13: sequence:\n",
            "pair_8054.yaml -> null/empty sequence at line 17: sequence:\n",
            "pair_9005.yaml -> null/empty sequence at line 17: sequence:\n",
            "pair_10863.yaml -> null/empty sequence at line 17: sequence:\n",
            "pair_9004.yaml -> null/empty sequence at line 17: sequence:\n",
            "pair_8117.yaml -> null/empty sequence at line 17: sequence:\n",
            "pair_9559.yaml -> null/empty sequence at line 17: sequence:\n",
            "pair_8397.yaml -> null/empty sequence at line 17: sequence:\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "NULL_SEQ_PATTERNS = [\n",
        "    re.compile(r\"^\\s*sequence:\\s*(null|None)\\s*$\", re.IGNORECASE),\n",
        "    re.compile(r\"^\\s*sequence:\\s*$\"),  # sequence: (empty)\n",
        "]\n",
        "\n",
        "def find_yaml_with_null_sequences(folder: Path):\n",
        "    folder = Path(folder)\n",
        "    bad = []\n",
        "    for p in folder.glob(\"*.yaml\"):\n",
        "        try:\n",
        "            lines = p.read_text(errors=\"ignore\").splitlines()\n",
        "        except Exception:\n",
        "            bad.append((p, \"read_error\"))\n",
        "            continue\n",
        "        for i, line in enumerate(lines):\n",
        "            if any(rx.match(line) for rx in NULL_SEQ_PATTERNS):\n",
        "                bad.append((p, f\"null/empty sequence at line {i+1}: {line.strip()}\"))\n",
        "                break\n",
        "    return bad\n",
        "\n",
        "# Example:\n",
        "bad = find_yaml_with_null_sequences(YAML_DIR_TRAIN)  # or a chunk dir\n",
        "print(\"Bad YAMLs:\", len(bad))\n",
        "for p, reason in bad[:20]:\n",
        "    print(p.name, \"->\", reason)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "3c32932d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bad YAMLs: 0\n",
            "Bad YAMLs: 0\n"
          ]
        }
      ],
      "source": [
        "bad_2 = find_yaml_with_null_sequences(YAML_DIR_VAL)\n",
        "print(\"Bad YAMLs:\", len(bad_2))\n",
        "for p, reason in bad_2[:20]:\n",
        "    print(p.name, \"->\", reason)\n",
        "\n",
        "bad_3 = find_yaml_with_null_sequences(YAML_DIR_TEST)\n",
        "print(\"Bad YAMLs:\", len(bad_3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68cd25d1",
      "metadata": {},
      "source": [
        "Remove 'BAD' YAML files (Only need to do this once)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "69c94f17",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "BASE_DIR = Path(\"/home/natasha/multimodal_model\")\n",
        "\n",
        "DATA_DIR = BASE_DIR / \"data\"\n",
        "MANI_DIR = BASE_DIR / \"manifests\"\n",
        "\n",
        "YAML_DIR_TRAIN = DATA_DIR / \"train\"\n",
        "YAML_DIR_VAL   = DATA_DIR / \"val\"\n",
        "YAML_DIR_TEST  = DATA_DIR / \"test\"\n",
        "\n",
        "TRAIN_CHUNKS_ROOT = YAML_DIR_TRAIN / \"_chunks\"\n",
        "TEST_CHUNKS_ROOT  = YAML_DIR_TEST / \"_chunks\"\n",
        "\n",
        "MANIFEST_PATHS = {\n",
        "    \"train\": MANI_DIR / \"train_manifest.csv\",\n",
        "    \"val\":   MANI_DIR / \"val_manifest.csv\",\n",
        "    \"test\":  MANI_DIR / \"test_manifest.csv\",\n",
        "}\n",
        "\n",
        "BAD_YAMLS = {\n",
        "    \"pair_8118.yaml\",\n",
        "    \"pair_11302.yaml\",\n",
        "    \"pair_11083.yaml\",\n",
        "    \"pair_9184.yaml\",\n",
        "    \"pair_9558.yaml\",\n",
        "    \"pair_10061.yaml\",\n",
        "    \"pair_11244.yaml\",\n",
        "    \"pair_8054.yaml\",\n",
        "    \"pair_9005.yaml\",\n",
        "    \"pair_10863.yaml\",\n",
        "    \"pair_9004.yaml\",\n",
        "    \"pair_8117.yaml\",\n",
        "    \"pair_9559.yaml\",\n",
        "    \"pair_8397.yaml\",\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "abf2eea9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted from main dirs: 14\n",
            "Missing from main dirs (fine): 28\n",
            "Deleted from chunks: 0\n"
          ]
        }
      ],
      "source": [
        "def delete_files_if_present(paths):\n",
        "    deleted = []\n",
        "    missing = []\n",
        "    for p in paths:\n",
        "        if p.exists() or p.is_symlink():\n",
        "            try:\n",
        "                p.unlink()\n",
        "                deleted.append(p)\n",
        "            except Exception as e:\n",
        "                print(f\"[WARN] Could not delete {p}: {e}\")\n",
        "        else:\n",
        "            missing.append(p)\n",
        "    return deleted, missing\n",
        "\n",
        "# 1) delete from main YAML dirs\n",
        "main_targets = []\n",
        "for d in [YAML_DIR_TRAIN, YAML_DIR_VAL, YAML_DIR_TEST]:\n",
        "    for name in BAD_YAMLS:\n",
        "        main_targets.append(d / name)\n",
        "\n",
        "deleted_main, missing_main = delete_files_if_present(main_targets)\n",
        "\n",
        "print(f\"Deleted from main dirs: {len(deleted_main)}\")\n",
        "print(f\"Missing from main dirs (fine): {len(missing_main)}\")\n",
        "\n",
        "# 2) delete from chunk dirs (train/test only)\n",
        "chunk_targets = []\n",
        "for chunks_root in [TRAIN_CHUNKS_ROOT, TEST_CHUNKS_ROOT]:\n",
        "    if chunks_root.exists():\n",
        "        for name in BAD_YAMLS:\n",
        "            chunk_targets.extend(chunks_root.rglob(name))\n",
        "\n",
        "deleted_chunks, missing_chunks = delete_files_if_present(chunk_targets)\n",
        "\n",
        "print(f\"Deleted from chunks: {len(deleted_chunks)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "94bd8a8c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_manifest.csv: removed 14 rows (backup: train_manifest.csv.bak_20260119_153914)\n",
            "val_manifest.csv: removed 0 rows (backup: val_manifest.csv.bak_20260119_153914)\n",
            "test_manifest.csv: removed 0 rows (backup: test_manifest.csv.bak_20260119_153914)\n",
            "Total rows removed across manifests: 14\n"
          ]
        }
      ],
      "source": [
        "def strip_bad_from_manifest(csv_path: Path, bad_names: set[str]) -> int:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    if \"yaml_path\" not in df.columns:\n",
        "        raise ValueError(f\"{csv_path} does not have a 'yaml_path' column. Columns: {list(df.columns)}\")\n",
        "\n",
        "    before = len(df)\n",
        "    basenames = df[\"yaml_path\"].astype(str).apply(lambda x: Path(x).name)\n",
        "    df2 = df[~basenames.isin(bad_names)].copy()\n",
        "    after = len(df2)\n",
        "    removed = before - after\n",
        "\n",
        "    # backup then overwrite\n",
        "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    backup_path = csv_path.with_suffix(csv_path.suffix + f\".bak_{ts}\")\n",
        "    shutil.copy2(csv_path, backup_path)\n",
        "\n",
        "    df2.to_csv(csv_path, index=False)\n",
        "    print(f\"{csv_path.name}: removed {removed} rows (backup: {backup_path.name})\")\n",
        "    return removed\n",
        "\n",
        "total_removed = 0\n",
        "for split, path in MANIFEST_PATHS.items():\n",
        "    if path.exists():\n",
        "        total_removed += strip_bad_from_manifest(path, BAD_YAMLS)\n",
        "    else:\n",
        "        print(f\"[WARN] Manifest not found for {split}: {path}\")\n",
        "\n",
        "print(\"Total rows removed across manifests:\", total_removed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "d639afc8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train remaining bad references: []\n",
            "val remaining bad references: []\n",
            "test remaining bad references: []\n"
          ]
        }
      ],
      "source": [
        "# Confirm no manifest references remain\n",
        "for split, path in MANIFEST_PATHS.items():\n",
        "    if not path.exists():\n",
        "        continue\n",
        "    df = pd.read_csv(path)\n",
        "    basenames = df[\"yaml_path\"].astype(str).apply(lambda x: Path(x).name)\n",
        "    hits = sorted(set(basenames) & BAD_YAMLS)\n",
        "    print(split, \"remaining bad references:\", hits)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f89631f2",
      "metadata": {},
      "source": [
        "Old Code - Before Added Chunking Folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c6b31d12",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import subprocess, shutil, os\n",
        "import pandas as pd  # you use pd in cell 3\n",
        "import os, re, textwrap\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "import subprocess, shlex, sys, os, re\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# === Configure paths ===\n",
        "BASE_DIR = Path(\"/home/natasha/multimodal_model\") \n",
        "\n",
        "MANI_DIR  = BASE_DIR / \"manifests\"\n",
        "\n",
        "# change these\n",
        "MANIFEST_PATH_TRAIN = MANI_DIR / \"train_manifest.csv\"\n",
        "MANIFEST_PATH_VAL   = MANI_DIR / \"val_manifest.csv\"\n",
        "MANIFEST_PATH_TEST  = MANI_DIR / \"test_manifest.csv\"\n",
        "\n",
        "YAML_DIR_TRAIN = BASE_DIR / \"data\" / \"train\"\n",
        "YAML_DIR_VAL   = BASE_DIR / \"data\" / \"val\"\n",
        "YAML_DIR_TEST  = BASE_DIR / \"data\" / \"test\"\n",
        "\n",
        "\n",
        "# Boltz output directory\n",
        "\n",
        "RUN_ROOT = BASE_DIR / \"outputs\"\n",
        "RUN_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Boltz output directories split by train/val/test\n",
        "BOLTZ_OUT_TRAIN = RUN_ROOT / \"train\"\n",
        "BOLTZ_OUT_VAL   = RUN_ROOT / \"val\"\n",
        "BOLTZ_OUT_TEST  = RUN_ROOT / \"test\"\n",
        "\n",
        "BOLTZ_OUT_TRAIN.mkdir(parents=True, exist_ok=True)\n",
        "BOLTZ_OUT_VAL.mkdir(parents=True, exist_ok=True)\n",
        "BOLTZ_OUT_TEST.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79980941",
      "metadata": {},
      "source": [
        "Boltz Command and Execution Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c501b630",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose one: \"CLI\" (command line) or \"API\" (inprocess Python)\n",
        "RUN_MODE = \"CLI\"          # change to \"CLI\" to use local CLI\n",
        "NPROC = 5                # keep 1 for strictly step-by-step; increase later if desired\n",
        "\n",
        "# --- CLI config (if RUN_MODE == \"CLI\") ---\n",
        "# This calls your local repo code via `python -m ...`. Edit to match your runner.\n",
        "# Use full path to boltz from tcr-multimodal environment\n",
        "BOLTZ_CMD_TEMPLATE = (\n",
        "    \"conda run -n boltz-env --no-capture-output boltz predict {yaml} \"\n",
        "    \"--out_dir {outdir} \"\n",
        "    \"--accelerator gpu \"\n",
        "    \"--devices 1 \"\n",
        "    \"--model boltz2 \"\n",
        "    \"--recycling_steps 1 \"\n",
        "    \"--sampling_steps 20 \"\n",
        "    \"--diffusion_samples 1 \"\n",
        "    \"--max_parallel_samples 1 \"\n",
        "    \"--max_msa_seqs 64 \"\n",
        "    \"--num_subsampled_msa 64 \"\n",
        "    \"--override \"\n",
        "    \"--write_embeddings\"\n",
        ")\n",
        "\n",
        "# removed --no_kernels - supposedly boltz has default kernals it uses to speed up the intensive operations\n",
        "\n",
        "# --- API config (if RUN_MODE == \"API\") ---\n",
        "# Adjust import and call to your environment. The key is save_z=True.\n",
        "API_IMPORT = \"from boltz.predict import boltz_predict\"\n",
        "API_CALL   = \"boltz_predict\"            # callable name\n",
        "API_KWARGS = {\"save_z\": True}           # ensure 'z' is saved\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f9e4ed6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# trying to offload RAM memory\n",
        "def run_cli(yaml_path: Path, outdir: Path):\n",
        "    outdir.mkdir(parents=True, exist_ok=True)\n",
        "    cmd = BOLTZ_CMD_TEMPLATE.format(yaml=str(yaml_path), outdir=str(outdir))\n",
        "    print(\"CMD:\", cmd)\n",
        "    with open(outdir / \"stdout.log\", \"w\") as so, open(outdir / \"stderr.log\", \"w\") as se:\n",
        "        proc = subprocess.run(\n",
        "            shlex.split(cmd),\n",
        "            stdout=so,\n",
        "            stderr=se,\n",
        "            text=True,\n",
        "            cwd=str(BASE_DIR),\n",
        "        )\n",
        "    print(\"Return code:\", proc.returncode)\n",
        "    return proc.returncode\n",
        "\n",
        "\n",
        "def run_one(yaml_rel_path, YAML_DIR, BOLTZ_OUT_ROOT):\n",
        "    yaml_path = (YAML_DIR / Path(yaml_rel_path).name).resolve()\n",
        "    pair_id   = yaml_path.stem\n",
        "    outdir    = BOLTZ_OUT_ROOT / pair_id\n",
        "    if RUN_MODE.upper() == \"CLI\":\n",
        "        rc = run_cli(yaml_path, outdir)\n",
        "    else:\n",
        "        rc = run_api(yaml_path, outdir)\n",
        "    return pair_id, rc\n",
        "\n",
        "\n",
        "def execute_boltz_runs(manifest_df, YAML_DIR, BOLTZ_OUT_ROOT):\n",
        "    if NPROC == 1:\n",
        "        results = []\n",
        "        for _, row in manifest_df.iterrows():\n",
        "            results.append(run_one(row[\"yaml_path\"], YAML_DIR, BOLTZ_OUT_ROOT))\n",
        "    else:\n",
        "        results = []\n",
        "        with ThreadPoolExecutor(max_workers=NPROC) as ex:\n",
        "            futs = [ex.submit(run_one, y, YAML_DIR, BOLTZ_OUT_ROOT) for y in manifest_df[\"yaml_path\"]]\n",
        "            for fut in as_completed(futs):\n",
        "                results.append(fut.result())\n",
        "\n",
        "    print(\"Completed:\", sum(rc==0 for _, rc in results), \"/\", len(results), \"successes\")\n",
        "    results[:5]\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "41a4cb74",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pair_id</th>\n",
              "      <th>yaml_path</th>\n",
              "      <th>pep_len</th>\n",
              "      <th>tcra_len</th>\n",
              "      <th>tcrb_len</th>\n",
              "      <th>hla_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pair_000</td>\n",
              "      <td>data/train/pair_000.yaml</td>\n",
              "      <td>9</td>\n",
              "      <td>110</td>\n",
              "      <td>114</td>\n",
              "      <td>365</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    pair_id                 yaml_path  pep_len  tcra_len  tcrb_len  hla_len\n",
              "0  pair_000  data/train/pair_000.yaml        9       110       114      365"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test = pd.read_csv(MANIFEST_PATH_TRAIN)\n",
        "test = test.head(1)\n",
        "test = execute_boltz_runs(test, YAML_DIR_TRAIN, BOLTZ_OUT_TRAIN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6913659",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "boltz-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
