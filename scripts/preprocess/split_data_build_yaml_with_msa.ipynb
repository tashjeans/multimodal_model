{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4024919e",
   "metadata": {},
   "source": [
    "This script splits data into train, validate, test.\n",
    "The script splits into three categories:\n",
    "- unseen tcrs\n",
    "- unseen peptides\n",
    "- completely unseen pairs\n",
    "\n",
    "There are also some further limitations on the data:\n",
    "1. Keep most cross-reactive TCRs\n",
    "2. Makes sure HLA is as balanced as possible in train and val/test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3ea700",
   "metadata": {},
   "source": [
    "#### 0. Imports and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d875ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess, shutil, os\n",
    "import pandas as pd  # you use pd in cell 3\n",
    "import os, re, textwrap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2866c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paths ===\n",
    "#CSV_PATH = \"data/raw/HLA/full_positives_hla_seq.csv\"  # update if needed\n",
    "CSV_PATH = \"/home/natasha/multimodal_model/data/raw/HLA/full_positives_hla_seq.csv\"\n",
    "BASE_DIR  = Path(\"/home/natasha/multimodal_model\") #/ \"data\" / \"raw\"\n",
    "MSA_DIR   = BASE_DIR / \"data\" / \"raw\" / \"MSA\"\n",
    "TRAIN_DIR = BASE_DIR / \"data\" / \"train\"\n",
    "VAL_DIR   = BASE_DIR / \"data\" / \"val\"\n",
    "TEST_DIR  = BASE_DIR / \"data\" / \"test\"\n",
    "MANI_DIR  = BASE_DIR / \"data\" / \"manifests\"\n",
    "\n",
    "# create directories if they don't exist\n",
    "MSA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MANI_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TRAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === Load CSV and preview ===\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "#df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3b94222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Paths --------\n",
    "#BASE_DIR = Path(\"/home/natasha/multimodal_model\")\n",
    "DB_COMBINED = BASE_DIR / \"data\" / \"raw\" / \"MSA\" / \"big_combo_subset_tcrs_50000_w_mhc_seqs.fasta\"\n",
    "\n",
    "# use the split DBs created\n",
    "DBS = {\n",
    "    \"tcra\": BASE_DIR / \"data\" / \"raw\" / \"MSA\" / \"db_split\" / \"alpha.fasta\",\n",
    "    \"tcrb\": BASE_DIR / \"data\" / \"raw\" / \"MSA\" / \"db_split\" / \"beta.fasta\",\n",
    "    \"mhc\":  BASE_DIR / \"data\" / \"raw\" / \"MSA\" / \"db_split\" / \"mhc.fasta\",\n",
    "}\n",
    "def pick_db_for(stem: str) -> Path:\n",
    "    return DBS.get(stem, DB_COMBINED)\n",
    "\n",
    "OUT_ROOT = BASE_DIR / \"data\" / \"raw\" / \"MSA\" / \"jackhmmer_msas\"\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------- Controls --------\n",
    "VERBOSE = False              # set False to silence prints\n",
    "KEEP_INTERMEDIATES = False   # set False to delete .sto/.tbl/.a3m after filtering\n",
    "\n",
    "# -------- jackhmmer / filtering parameters --------\n",
    "JACK_ITERS = 1\n",
    "EVALUE = 1e-10\n",
    "CPU_THREADS = 4\n",
    "CPU_THREADS = os.cpu_count() or 4\n",
    "\n",
    "MAX_SEQS = 64\n",
    "# keep identity de-dup very relaxed so hhfilter mostly just caps:\n",
    "ID_THR_DEFAULT = 100        # 100 = no ID-based collapse\n",
    "COV_THR_TCR     = 50\n",
    "COV_THR_MHC     = 30\n",
    "\n",
    "def have(cmd): return shutil.which(cmd) is not None\n",
    "if VERBOSE:\n",
    "    print(\"Deps:\",\n",
    "          \"jackhmmer\" if have(\"jackhmmer\") else \"MISSING\",\n",
    "          \"esl-reformat\" if have(\"esl-reformat\") else (\"reformat.pl\" if have(\"reformat.pl\") else \"MISSING\"),\n",
    "          \"hhfilter\" if have(\"hhfilter\") else \"MISSING\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b99e8f4",
   "metadata": {},
   "source": [
    "#### 1. Split Data into Train, Validate, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "810b7c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load CSV and preview ===\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.head(3)\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "df[\"orig_row\"] = df.index.astype(int)\n",
    "df[\"pair_id\"] = df[\"orig_row\"].map(lambda i: f\"pair_{i:06d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20e3f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# 0. Hyperparams & constants\n",
    "# ============================================================\n",
    "\n",
    "RNG_SEED = 42\n",
    "rng = np.random.default_rng(RNG_SEED)\n",
    "\n",
    "# Peptides you *never* want to end up in unseen categories\n",
    "PEPTIDES_TO_KEEP = [\n",
    "    \"KLGGALQAK\", \"GILGFVFTL\", \"AVFDRKSDAK\", \"RAKFKQLL\", \"SPRWYFYYL\",\n",
    "    \"YLQPRTFLL\", \"TTDPSFLGRY\", \"GLCTLVAML\", \"RVRAYTYSK\", \"IVTDFSVIK\",\n",
    "    \"LLWNGPMAV\", \"LLLDRLNQL\", \"NLVPMVATV\", \"LLAGIGTVPI\", \"RLRAEAQVK\",\n",
    "    \"ELAGIGILTV\", \"YVLDHLIVV\", \"LTDEMIAQY\", \"CINGVCWTV\", \"TPRVTGGGAM\",\n",
    "    \"VMATRRNVL\", \"KTFPPTEPK\", \"QYIKWPWYI\", \"DATYQRTRALVR\", \"NQKLIANQF\",\n",
    "    \"FLRGRAYGL\", \"CTELKLSDY\", \"ATDALMTGF\", \"RPPIFIRRL\", \"NYNYLYRLF\",\n",
    "    \"FLYALALLL\", \"VMTTVLATL\", \"CLGGLLTMV\", \"KSKRTPMGF\", \"RPHERNGFTVL\",\n",
    "    \"MEVTPSGTWL\", \"FTSDYYQLY\", \"RPIIRPATL\", \"ALAGIGILTV\", \"LLYDANYFL\",\n",
    "    \"HPVTKYIM\", \"RLPGVLPRA\", \"RFPLTFGWCF\", \"VYFLQSINF\", \"PTDNYITTY\",\n",
    "    \"ALWEIQQVV\", \"QAKWRLQTL\", \"RTATKQYNV\", \"LLFGYPVYV\"\n",
    "]\n",
    "\n",
    "# Backbone HLAs that should always stay in train (can also appear in val/test,\n",
    "# but not as \"unseen_HLA\" regimes)\n",
    "BACKBONE_HLAS = {\n",
    "    'HLA-A*02:01',\n",
    "    'HLA-A*01:01',\n",
    "    'HLA-A*24:02',\n",
    "    'HLA-B*07:02',\n",
    "    'HLA-B*08:01',\n",
    "    'HLA-A*03:01',\n",
    "    'HLA-A*11:01',\n",
    "}\n",
    "\n",
    "# How many HLAs to use as explicit unseen-HLA regimes\n",
    "N_TEST_UNSEEN_HLA = 2\n",
    "N_VAL_UNSEEN_HLA  = 2\n",
    "\n",
    "# Minimum peptides for an HLA to be eligible as an \"unseen HLA\" regime\n",
    "MIN_PEPTIDES_UNSEEN_HLA_TEST = 10\n",
    "MIN_PEPTIDES_UNSEEN_HLA_VAL  = 10  # you can lower this if needed\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Pre-processing\n",
    "# ============================================================\n",
    "\n",
    "def preprocess_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean up TCR chains, build TCR_full and masks.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1. Fill empty/nan values with 'X' token\n",
    "    df['TCRa'] = df['TCRa'].fillna('X')\n",
    "    df['TCRb'] = df['TCRb'].fillna('X')\n",
    "\n",
    "    # Replace empty strings with 'X'\n",
    "    df.loc[df['TCRa'] == '', 'TCRa'] = 'X'\n",
    "    df.loc[df['TCRb'] == '', 'TCRb'] = 'X'\n",
    "\n",
    "    # Build TCR_full and alpha/beta masks\n",
    "    df['TCR_full'] = df['TCRa'] + df['TCRb']\n",
    "    df['m_alpha'] = 1\n",
    "    df['m_beta'] = 1\n",
    "    df.loc[df['TCRa'] == 'X', 'm_alpha'] = 0\n",
    "    df.loc[df['TCRb'] == 'X', 'm_beta'] = 0\n",
    "\n",
    "    # Remove rows with invalid TCR_full entries\n",
    "    df = df.dropna(subset=['TCR_full'])\n",
    "    df = df[df['TCR_full'] != ' ']\n",
    "    df = df[df['TCR_full'] != 'nan']\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Category builder for unseen TCR / unseen peptide / completely unseen\n",
    "# ============================================================\n",
    "\n",
    "def make_unseen_tcr_peptide_categories(\n",
    "    df_in: pd.DataFrame,\n",
    "    peptides_to_keep,\n",
    "    pct_tcr_cat1=0.02,\n",
    "    pct_tcr_cat2=0.05,\n",
    "    pct_peptide_cat3=0.01,\n",
    "    category_col='category',\n",
    "    prefix=''\n",
    "):\n",
    "    \"\"\"\n",
    "    Build three categories:\n",
    "      - completely_unseen   (unseen TCR & unseen peptide)\n",
    "      - unseen_TCR          (TCR unseen in train, peptide seen in train)\n",
    "      - unseen_peptide      (peptide unseen in train, TCR seen in train)\n",
    "\n",
    "    Returns:\n",
    "      cat1_df, cat2_df, cat3_df, remaining_df\n",
    "    \"\"\"\n",
    "    df = df_in.copy()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Category 1: Completely unseen\n",
    "    # -----------------------------\n",
    "    unique_tcrs = df['TCR_full'].unique()\n",
    "    if len(unique_tcrs) == 0:\n",
    "        return (df.iloc[0:0].copy(),) * 4  # all empty\n",
    "\n",
    "    n_cat1_tcrs = max(1, int(len(unique_tcrs) * pct_tcr_cat1))\n",
    "    selected_tcrs_cat1 = set(rng.choice(unique_tcrs, size=n_cat1_tcrs, replace=False))\n",
    "\n",
    "    tcr_pairs = df[df['TCR_full'].isin(selected_tcrs_cat1)]\n",
    "    # remove peptides we insist must remain in training\n",
    "    tcr_pairs = tcr_pairs[~tcr_pairs['Peptide'].isin(peptides_to_keep)]\n",
    "\n",
    "    selected_peptides_cat1 = set(tcr_pairs['Peptide'].unique())\n",
    "    selected_tcrs_cat1 = set(tcr_pairs['TCR_full'].unique())\n",
    "\n",
    "    cat1_df = df[\n",
    "        df['TCR_full'].isin(selected_tcrs_cat1) &\n",
    "        df['Peptide'].isin(selected_peptides_cat1)\n",
    "    ].copy()\n",
    "    cat1_df[category_col] = f'{prefix}completely_unseen'\n",
    "\n",
    "    # Candidates for other categories = everything that does NOT use these TCRs OR these peptides\n",
    "    train_candidate = df[\n",
    "        ~(df['TCR_full'].isin(selected_tcrs_cat1) | df['Peptide'].isin(selected_peptides_cat1))\n",
    "    ].copy()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Category 2: Unseen TCR (peptide seen)\n",
    "    # -----------------------------\n",
    "    remaining_unique_tcrs = train_candidate['TCR_full'].unique()\n",
    "    if len(remaining_unique_tcrs) > 0:\n",
    "        n_cat2_tcrs = max(1, int(len(remaining_unique_tcrs) * pct_tcr_cat2))\n",
    "        selected_tcrs_cat2 = set(\n",
    "            rng.choice(remaining_unique_tcrs, size=n_cat2_tcrs, replace=False)\n",
    "        )\n",
    "    else:\n",
    "        selected_tcrs_cat2 = set()\n",
    "\n",
    "    cat2_df = train_candidate[train_candidate['TCR_full'].isin(selected_tcrs_cat2)].copy()\n",
    "    cat2_df[category_col] = f'{prefix}unseen_TCR'\n",
    "\n",
    "    # Remove all these rows from candidate pool\n",
    "    train_candidate = train_candidate.drop(cat2_df.index)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Category 3: Unseen peptide (TCR seen)\n",
    "    # -----------------------------\n",
    "    remaining_unique_peptides = train_candidate['Peptide'].unique()\n",
    "    if len(remaining_unique_peptides) > 0:\n",
    "        n_cat3_peptides = max(1, int(len(remaining_unique_peptides) * pct_peptide_cat3))\n",
    "        selected_peptides_cat3 = set(\n",
    "            rng.choice(remaining_unique_peptides, size=n_cat3_peptides, replace=False)\n",
    "        )\n",
    "        selected_peptides_cat3 = selected_peptides_cat3 - set(peptides_to_keep)\n",
    "    else:\n",
    "        selected_peptides_cat3 = set()\n",
    "\n",
    "    cat3_df = train_candidate[train_candidate['Peptide'].isin(selected_peptides_cat3)].copy()\n",
    "    cat3_df[category_col] = f'{prefix}unseen_peptide'\n",
    "\n",
    "    # Remove all rows containing these peptides from candidate pool\n",
    "    remaining_df = train_candidate.drop(cat3_df.index)\n",
    "\n",
    "    return cat1_df, cat2_df, cat3_df, remaining_df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Top-level split function\n",
    "# ============================================================\n",
    "\n",
    "def split_tcr_dataset(\n",
    "    df_raw: pd.DataFrame,\n",
    "    peptides_to_keep=PEPTIDES_TO_KEEP,\n",
    "    backbone_hlas=BACKBONE_HLAS,\n",
    "    n_test_unseen_hla=N_TEST_UNSEEN_HLA,\n",
    "    n_val_unseen_hla=N_VAL_UNSEEN_HLA,\n",
    "    min_peptides_unseen_hla_test=MIN_PEPTIDES_UNSEEN_HLA_TEST,\n",
    "    min_peptides_unseen_hla_val=MIN_PEPTIDES_UNSEEN_HLA_VAL\n",
    "):\n",
    "    \"\"\"\n",
    "    Main entry point:\n",
    "      - cleans df\n",
    "      - builds test (HLA unseen + Cat1/2/3)\n",
    "      - builds validation (HLA unseen + Cat1/2/3 on remaining)\n",
    "      - builds final train by globally removing all \"unseen\" entities\n",
    "    \"\"\"\n",
    "    df = preprocess_df(df_raw)\n",
    "\n",
    "    total_pairs = len(df)\n",
    "    print(f\"Total pairs after cleaning: {total_pairs}\")\n",
    "    print(f\"Unique TCRs: {df['TCR_full'].nunique()}\")\n",
    "    print(f\"Unique Peptides: {df['Peptide'].nunique()}\")\n",
    "    print(f\"Unique HLAs: {df['HLA'].nunique()}\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 3.1 HLA summary on full data\n",
    "    # --------------------------------------------------------\n",
    "    hla_table = df.groupby('HLA').agg(\n",
    "        TCR_full=('TCR_full', 'nunique'),\n",
    "        Peptide=('Peptide', 'nunique')\n",
    "    ).reset_index().sort_values('Peptide', ascending=False)\n",
    "\n",
    "    print(\"\\nTop HLAs by peptide count:\")\n",
    "    print(hla_table.head(10))\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 3.2 Choose unseen HLAs for TEST\n",
    "    # --------------------------------------------------------\n",
    "    candidate_hlas_test = set(\n",
    "        hla_table.loc[\n",
    "            (hla_table['Peptide'] >= min_peptides_unseen_hla_test) &\n",
    "            (~hla_table['HLA'].isin(backbone_hlas)),\n",
    "            'HLA'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if len(candidate_hlas_test) == 0:\n",
    "        print(\"\\n[WARN] No HLAs eligible for unseen-HLA TEST regime with current threshold.\")\n",
    "        test_unseen_hlas = set()\n",
    "    else:\n",
    "        n_test = min(n_test_unseen_hla, len(candidate_hlas_test))\n",
    "        test_unseen_hlas = set(\n",
    "            rng.choice(list(candidate_hlas_test), size=n_test, replace=False)\n",
    "        )\n",
    "\n",
    "    print(\"\\nTest unseen HLAs chosen:\", test_unseen_hlas)\n",
    "\n",
    "    # All rows with these HLAs go straight to test (HLA-unseen category)\n",
    "    cat0_test_df = df[df['HLA'].isin(test_unseen_hlas)].copy()\n",
    "    cat0_test_df['test_category'] = 'unseen_HLA'\n",
    "\n",
    "    # Remaining pool for test Cat1/2/3\n",
    "    pool_after_test_hla = df[~df['HLA'].isin(test_unseen_hlas)].copy()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 3.3 Build Cat1/Cat2/Cat3 for TEST\n",
    "    # --------------------------------------------------------\n",
    "    cat1_test, cat2_test, cat3_test, _ = make_unseen_tcr_peptide_categories(\n",
    "        pool_after_test_hla,\n",
    "        peptides_to_keep=peptides_to_keep,\n",
    "        category_col='test_category',\n",
    "        prefix=''\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTest Cat1 (completely_unseen) pairs: {len(cat1_test)}\")\n",
    "    print(f\"Test Cat2 (unseen_TCR) pairs: {len(cat2_test)}\")\n",
    "    print(f\"Test Cat3 (unseen_peptide) pairs: {len(cat3_test)}\")\n",
    "\n",
    "    other_test_df = pd.concat([cat1_test, cat2_test, cat3_test]).drop_duplicates()\n",
    "    test_df = pd.concat([cat0_test_df, other_test_df]).drop_duplicates()\n",
    "\n",
    "    print(f\"\\nTotal TEST pairs: {len(test_df)} \"\n",
    "          f\"({len(test_df) / total_pairs * 100:.2f}%)\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 3.4 Build initial dev pool (for val + future train)\n",
    "    # BUT: we will later *globally* exclude unseen entities again for train_df.\n",
    "    # For val construction, it's enough to remove test rows by index.\n",
    "    # --------------------------------------------------------\n",
    "    dev_pool = df[~df.index.isin(test_df.index)].copy()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 3.5 Choose unseen HLAs for VALIDATION from dev_pool\n",
    "    # We must exclude ANY HLA that appears in test (not just test_unseen_hlas)\n",
    "    # to avoid overlap of unseen HLA regimes.\n",
    "    # --------------------------------------------------------\n",
    "    test_hlas = set(test_df['HLA'])\n",
    "\n",
    "    hla_table_dev = dev_pool.groupby('HLA').agg(\n",
    "        TCR_full=('TCR_full', 'nunique'),\n",
    "        Peptide=('Peptide', 'nunique')\n",
    "    ).reset_index()\n",
    "\n",
    "    candidate_hlas_val = set(\n",
    "        hla_table_dev.loc[\n",
    "            (hla_table_dev['Peptide'] >= min_peptides_unseen_hla_val) &\n",
    "            (~hla_table_dev['HLA'].isin(backbone_hlas)) &\n",
    "            (~hla_table_dev['HLA'].isin(test_hlas)),\n",
    "            'HLA'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if len(candidate_hlas_val) == 0:\n",
    "        print(\"\\n[WARN] No HLAs eligible for unseen-HLA VAL regime with current threshold.\")\n",
    "        val_unseen_hlas = set()\n",
    "    else:\n",
    "        n_val = min(n_val_unseen_hla, len(candidate_hlas_val))\n",
    "        val_unseen_hlas = set(\n",
    "            rng.choice(list(candidate_hlas_val), size=n_val, replace=False)\n",
    "        )\n",
    "\n",
    "    print(\"\\nVal unseen HLAs chosen:\", val_unseen_hlas)\n",
    "\n",
    "    cat0_val_df = dev_pool[dev_pool['HLA'].isin(val_unseen_hlas)].copy()\n",
    "    cat0_val_df['val_category'] = 'unseen_HLA'\n",
    "\n",
    "    pool_after_val_hla = dev_pool[~dev_pool['HLA'].isin(val_unseen_hlas)].copy()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 3.6 Build Cat1/Cat2/Cat3 for VALIDATION\n",
    "    # --------------------------------------------------------\n",
    "    cat1_val, cat2_val, cat3_val, _ = make_unseen_tcr_peptide_categories(\n",
    "        pool_after_val_hla,\n",
    "        peptides_to_keep=peptides_to_keep,\n",
    "        category_col='val_category',\n",
    "        prefix=''\n",
    "    )\n",
    "\n",
    "    print(f\"\\nVal Cat1 (completely_unseen) pairs: {len(cat1_val)}\")\n",
    "    print(f\"Val Cat2 (unseen_TCR) pairs: {len(cat2_val)}\")\n",
    "    print(f\"Val Cat3 (unseen_peptide) pairs: {len(cat3_val)}\")\n",
    "\n",
    "    val_df = pd.concat([cat0_val_df, cat1_val, cat2_val, cat3_val]).drop_duplicates()\n",
    "\n",
    "    print(f\"\\nTotal VAL pairs: {len(val_df)} \"\n",
    "          f\"({len(val_df) / total_pairs * 100:.2f}%)\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 3.7 Build FINAL TRAIN by globally excluding all \"unseen\" entities\n",
    "    # --------------------------------------------------------\n",
    "    # TCRs/peptides used in unseen regimes must never appear in train\n",
    "\n",
    "    # Test unseen sets\n",
    "    test_cat1_tcrs = set(cat1_test['TCR_full'])\n",
    "    test_cat1_peps = set(cat1_test['Peptide'])\n",
    "\n",
    "    test_cat2_tcrs = set(cat2_test['TCR_full'])\n",
    "    test_cat3_peps = set(cat3_test['Peptide'])\n",
    "\n",
    "    # Val unseen sets\n",
    "    val_cat1_tcrs = set(cat1_val['TCR_full'])\n",
    "    val_cat1_peps = set(cat1_val['Peptide'])\n",
    "\n",
    "    val_cat2_tcrs = set(cat2_val['TCR_full'])\n",
    "    val_cat3_peps = set(cat3_val['Peptide'])\n",
    "\n",
    "    forbidden_tcrs_for_train = (\n",
    "        test_cat1_tcrs | test_cat2_tcrs |\n",
    "        val_cat1_tcrs | val_cat2_tcrs\n",
    "    )\n",
    "    forbidden_peps_for_train = (\n",
    "        test_cat1_peps | test_cat3_peps |\n",
    "        val_cat1_peps | val_cat3_peps\n",
    "    )\n",
    "    forbidden_hlas_for_train = test_unseen_hlas | val_unseen_hlas\n",
    "\n",
    "    train_df = df[\n",
    "        (~df.index.isin(test_df.index)) &\n",
    "        (~df.index.isin(val_df.index)) &\n",
    "        (~df['TCR_full'].isin(forbidden_tcrs_for_train)) &\n",
    "        (~df['Peptide'].isin(forbidden_peps_for_train)) &\n",
    "        (~df['HLA'].isin(forbidden_hlas_for_train))\n",
    "    ].copy()\n",
    "\n",
    "    print(f\"\\nTotal TRAIN pairs: {len(train_df)} \"\n",
    "          f\"({len(train_df) / total_pairs * 100:.2f}%)\")\n",
    "\n",
    "    meta = {\n",
    "        'test_unseen_hlas': test_unseen_hlas,\n",
    "        'val_unseen_hlas': val_unseen_hlas,\n",
    "        'backbone_hlas': backbone_hlas,\n",
    "    }\n",
    "\n",
    "    return train_df, val_df, test_df, meta\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. Sanity checks\n",
    "# ============================================================\n",
    "\n",
    "def run_sanity_checks(train_df, val_df, test_df):\n",
    "    print(\"\\n========== SANITY CHECKS ==========\")\n",
    "\n",
    "    train_tcrs = set(train_df['TCR_full'])\n",
    "    train_peps = set(train_df['Peptide'])\n",
    "\n",
    "    # --- Test categories\n",
    "    if 'test_category' in test_df.columns:\n",
    "        for cat_name, label in [\n",
    "            (\"Completely Unseen\", 'completely_unseen'),\n",
    "            (\"Unseen TCR\", 'unseen_TCR'),\n",
    "            (\"Unseen Peptide\", 'unseen_peptide')\n",
    "        ]:\n",
    "            cat = test_df[test_df['test_category'] == label]\n",
    "            tcrs = set(cat['TCR_full'])\n",
    "            peps = set(cat['Peptide'])\n",
    "\n",
    "            overlap_tcr = len(tcrs & train_tcrs)\n",
    "            overlap_pep = len(peps & train_peps)\n",
    "\n",
    "            print(f\"\\n[TEST] {cat_name} ({label})\")\n",
    "            print(f\"  #pairs: {len(cat)}\")\n",
    "            print(f\"  Overlap TCR with TRAIN: {overlap_tcr}\")\n",
    "            print(f\"  Overlap Peptide with TRAIN: {overlap_pep}\")\n",
    "\n",
    "    # --- HLA overlaps\n",
    "    train_hlas = set(train_df['HLA'])\n",
    "    val_hlas   = set(val_df['HLA'])\n",
    "    test_hlas  = set(test_df['HLA'])\n",
    "\n",
    "    print(\"\\nHLA counts:\")\n",
    "    print(\"  Train HLAs:\", len(train_hlas))\n",
    "    print(\"  Val HLAs:  \", len(val_hlas))\n",
    "    print(\"  Test HLAs: \", len(test_hlas))\n",
    "\n",
    "    unseen_hlas_test = test_hlas - train_hlas\n",
    "    unseen_hlas_val  = val_hlas - train_hlas\n",
    "\n",
    "    print(\"\\nUnseen HLAs in TEST vs TRAIN:\", unseen_hlas_test)\n",
    "    print(\"Unseen HLAs in VAL vs TRAIN:\", unseen_hlas_val)\n",
    "    print(\"Overlap of unseen-HLA between VAL and TEST:\",\n",
    "          unseen_hlas_test & unseen_hlas_val)\n",
    "\n",
    "    print(\"===================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f3bb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs after cleaning: 39926\n",
      "Unique TCRs: 36197\n",
      "Unique Peptides: 1457\n",
      "Unique HLAs: 71\n",
      "\n",
      "Top HLAs by peptide count:\n",
      "               HLA  TCR_full  Peptide\n",
      "1      HLA-A*02:01     12012      864\n",
      "0      HLA-A*01:01      1961      122\n",
      "23     HLA-B*07:02      1861      107\n",
      "15     HLA-A*24:02       736       89\n",
      "25     HLA-B*08:01      2114       59\n",
      "3   HLA-A*02:01:48        40       35\n",
      "29     HLA-B*27:05        24       31\n",
      "32     HLA-B*35:01        60       22\n",
      "54     HLA-B*57:03         1       22\n",
      "12     HLA-A*11:01      2344       18\n",
      "\n",
      "Test unseen HLAs chosen: {np.str_('HLA-A*02:01:48'), np.str_('HLA-B*57:01')}\n",
      "\n",
      "Test Cat1 (completely_unseen) pairs: 114\n",
      "Test Cat2 (unseen_TCR) pairs: 1863\n",
      "Test Cat3 (unseen_peptide) pairs: 32\n",
      "\n",
      "Total TEST pairs: 2214 (5.55%)\n",
      "\n",
      "Val unseen HLAs chosen: {np.str_('HLA-B*35:08'), np.str_('HLA-B*57:03')}\n",
      "\n",
      "Val Cat1 (completely_unseen) pairs: 112\n",
      "Val Cat2 (unseen_TCR) pairs: 1769\n",
      "Val Cat3 (unseen_peptide) pairs: 28\n",
      "\n",
      "Total VAL pairs: 1968 (4.93%)\n",
      "\n",
      "Total TRAIN pairs: 33991 (85.13%)\n",
      "\n",
      "========== SANITY CHECKS ==========\n",
      "\n",
      "[TEST] Completely Unseen (completely_unseen)\n",
      "  #pairs: 114\n",
      "  Overlap TCR with TRAIN: 0\n",
      "  Overlap Peptide with TRAIN: 0\n",
      "\n",
      "[TEST] Unseen TCR (unseen_TCR)\n",
      "  #pairs: 1863\n",
      "  Overlap TCR with TRAIN: 0\n",
      "  Overlap Peptide with TRAIN: 197\n",
      "\n",
      "[TEST] Unseen Peptide (unseen_peptide)\n",
      "  #pairs: 32\n",
      "  Overlap TCR with TRAIN: 5\n",
      "  Overlap Peptide with TRAIN: 0\n",
      "\n",
      "HLA counts:\n",
      "  Train HLAs: 59\n",
      "  Val HLAs:   29\n",
      "  Test HLAs:  33\n",
      "\n",
      "Unseen HLAs in TEST vs TRAIN: {'HLA-A*02:01:48', 'HLA-B*57:01', 'HLA-B*35:01:45', 'HLA-A*02:06'}\n",
      "Unseen HLAs in VAL vs TRAIN: {'HLA-B*57:03', 'HLA-A*11:01:18', 'HLA-B*35:08', 'HLA-B*81:01', 'HLA-B*42:01'}\n",
      "Overlap of unseen-HLA between VAL and TEST: set()\n",
      "===================================\n",
      "\n",
      "Meta: {'test_unseen_hlas': {np.str_('HLA-A*02:01:48'), np.str_('HLA-B*57:01')}, 'val_unseen_hlas': {np.str_('HLA-B*35:08'), np.str_('HLA-B*57:03')}, 'backbone_hlas': {'HLA-B*07:02', 'HLA-A*24:02', 'HLA-A*11:01', 'HLA-A*02:01', 'HLA-A*01:01', 'HLA-B*08:01', 'HLA-A*03:01'}}\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df, meta = split_tcr_dataset(df)\n",
    "\n",
    "run_sanity_checks(train_df, val_df, test_df)\n",
    "print(\"Meta:\", meta)\n",
    "\n",
    "# # save to data\n",
    "# train_df.to_csv(TRAIN_DIR / 'train_df.csv', index=False)\n",
    "# val_df.to_csv(VAL_DIR / 'val_df.csv', index=False)\n",
    "# test_df.to_csv(TEST_DIR / 'test_df.csv', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "479f05e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additionally, remove any rows where the whole TCR is missing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f330c5",
   "metadata": {},
   "source": [
    "#### 2. Build manifest file and YAML files for Boltz runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4b6659",
   "metadata": {},
   "source": [
    "##### 2a) Split the fasta file into alpha, beta, and mhc (only need to do this once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22a76064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split the fasta file into alpha, beta, and mhc (only need to do this once)\n",
    "# from pathlib import Path\n",
    "\n",
    "# src = Path(\"/home/natasha/multimodal_model/data/raw/MSA/big_combo_subset_tcrs_50000_w_mhc_seqs.fasta\")\n",
    "# outdir = Path(\"/home/natasha/multimodal_model/data/raw/MSA/db_split\")\n",
    "# outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# fa_alpha = outdir / \"alpha.fasta\"\n",
    "# fa_beta  = outdir / \"beta.fasta\"\n",
    "# fa_mhc   = outdir / \"mhc.fasta\"\n",
    "\n",
    "# with src.open() as fin, \\\n",
    "#      fa_alpha.open(\"w\") as fa, \\\n",
    "#      fa_beta.open(\"w\") as fb, \\\n",
    "#      fa_mhc.open(\"w\") as fm:\n",
    "#     hdr, seq = None, []\n",
    "#     for ln in fin:\n",
    "#         if ln.startswith(\">\"):\n",
    "#             if hdr:\n",
    "#                 s = \"\".join(seq)\n",
    "#                 if hdr.endswith(\"_a\"):\n",
    "#                     fa.write(hdr + \"\\n\" + s + \"\\n\")\n",
    "#                 elif hdr.endswith(\"_b\"):\n",
    "#                     fb.write(hdr + \"\\n\" + s + \"\\n\")\n",
    "#                 elif \"mhc\" in hdr.lower():\n",
    "#                     fm.write(hdr + \"\\n\" + s + \"\\n\")\n",
    "#             hdr, seq = ln.strip(), []\n",
    "#         else:\n",
    "#             seq.append(ln.strip())\n",
    "#     # write last record\n",
    "#     if hdr:\n",
    "#         s = \"\".join(seq)\n",
    "#         if hdr.endswith(\"_a\"):\n",
    "#             fa.write(hdr + s + \"\\n\")\n",
    "#         elif hdr.endswith(\"_b\"):\n",
    "#             fb.write(hdr + s + \"\\n\")\n",
    "#         elif \"mhc\" in hdr.lower():\n",
    "#             fm.write(hdr + s + \"\\n\")\n",
    "\n",
    "# print(\"Split complete:\")\n",
    "# print(\"α:\", fa_alpha.stat().st_size, \"bytes\")\n",
    "# print(\"β:\", fa_beta.stat().st_size, \"bytes\")\n",
    "# print(\"MHC:\", fa_mhc.stat().st_size, \"bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983b6e4",
   "metadata": {},
   "source": [
    "##### 2b) Build MSA + YAML + Manifest Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db512c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run this on GPU - would it speed up?\n",
    "# unlikely, as it's the search that is slow not the actual memory compute\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os, re, shutil, subprocess, textwrap\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "MSA_ROOT = BASE_DIR / \"data\" / \"raw\" / \"MSA\" / \"jackhmmer_msas\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "925d6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframes\n",
    "# save to data\n",
    "train_df = pd.read_csv(TRAIN_DIR / 'train_df.csv')\n",
    "val_df   = pd.read_csv(VAL_DIR / 'val_df.csv')\n",
    "test_df  = pd.read_csv(TEST_DIR / 'test_df.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a81eed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where both TCRa and TCRb are <unk>\n",
    "train_df = train_df[~((train_df['TCRa'] == \"<unk>\") & (train_df['TCRb'] == \"<unk>\"))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "784b784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(TRAIN_DIR / 'train_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a45217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Build MSA files, YAML and manifest files --------\n",
    "# Correcting for missing chains, skip in YAML if no chain is present\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class MSAConfig:\n",
    "    out_root: Path\n",
    "    db_combined: Path\n",
    "    dbs: Dict[str, Path]                 # {\"tcra\": ..., \"tcrb\": ..., \"mhc\": ...}\n",
    "\n",
    "    verbose: bool = False\n",
    "    keep_intermediates: bool = False\n",
    "\n",
    "    jack_iters: int = 1\n",
    "    evalue: float = 1e-10\n",
    "    cpu_threads: int = os.cpu_count() or 4\n",
    "\n",
    "    max_seqs: int = 64\n",
    "    id_thr: int = 100\n",
    "    cov_thr_tcr: int = 50\n",
    "    cov_thr_mhc: int = 30\n",
    "\n",
    "\n",
    "def have(cmd: str) -> bool:\n",
    "    return shutil.which(cmd) is not None\n",
    "\n",
    "\n",
    "def run(cmd):\n",
    "    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    out, err = p.communicate()\n",
    "    return p.returncode, out, err\n",
    "\n",
    "\n",
    "def pick_db_for(cfg: MSAConfig, stem: str) -> Path:\n",
    "    return cfg.dbs.get(stem, cfg.db_combined)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Sequence cleaning / presence\n",
    "# ----------------------------\n",
    "def clean_seq(s) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    return re.sub(r\"[^A-Za-z]\", \"\", s).upper()\n",
    "\n",
    "\n",
    "def has_seq(s: str, min_len: int = 1) -> bool:\n",
    "    return isinstance(s, str) and len(s) >= min_len\n",
    "\n",
    "\n",
    "MISSING_TOKENS = {\n",
    "    \"\", \"<unk>\", \"<UNK>\", \"UNK\", \"UNKNOWN\",\n",
    "    \"NA\", \"N/A\", \"NONE\", \"NULL\", \"NAN\"\n",
    "}\n",
    "\n",
    "def normalise_seq_for_yaml(s) -> str:\n",
    "    s0 = \"\" if not isinstance(s, str) else s.strip()\n",
    "    if s0 in {\"<unk>\", \"<UNK>\"}:\n",
    "        return \"\"\n",
    "    s = clean_seq(s0)     # strips <> and lowercases etc -> \"UNK\"\n",
    "    return \"\" if s in MISSING_TOKENS else s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# MSA helpers\n",
    "# ----------------------------\n",
    "def sto_to_a3m(sto_path: Path, a3m_path: Path) -> bool:\n",
    "    if have(\"esl-reformat\"):\n",
    "        code, out, err = run([\"esl-reformat\", \"a3m\", str(sto_path)])\n",
    "        if code == 0 and out:\n",
    "            a3m_path.write_text(out)\n",
    "            return True\n",
    "    if have(\"reformat.pl\"):\n",
    "        code, out, err = run([\"reformat.pl\", \"sto\", \"a3m\", str(sto_path), str(a3m_path)])\n",
    "        return code == 0 and a3m_path.exists()\n",
    "    return False\n",
    "\n",
    "\n",
    "def count_a3m(p: Path) -> int:\n",
    "    if not p.exists():\n",
    "        return -1\n",
    "    return sum(1 for ln in p.open() if ln.startswith(\">\"))\n",
    "\n",
    "\n",
    "def hhfilter_cap(cfg: MSAConfig, in_a3m: Path, out_a3m: Path, cov_thr: int) -> bool:\n",
    "    \"\"\"\n",
    "    Runs hhfilter if available; otherwise COPY (not move!) input → output.\n",
    "    \"\"\"\n",
    "    if not have(\"hhfilter\"):\n",
    "        if cfg.verbose:\n",
    "            print(\"WARN: hhfilter not found on PATH; copying input → output\")\n",
    "        shutil.copyfile(in_a3m, out_a3m)\n",
    "        return True\n",
    "\n",
    "    # detect which flag is supported\n",
    "    code, out, err = run([\"hhfilter\", \"-h\"])\n",
    "    use_maxseq = (\"-maxseq\" in (out or \"\")) or (\"-maxseq\" in (err or \"\"))\n",
    "\n",
    "    cmd = [\n",
    "        \"hhfilter\",\n",
    "        \"-i\", str(in_a3m),\n",
    "        \"-o\", str(out_a3m),\n",
    "        \"-id\", str(cfg.id_thr),\n",
    "        \"-cov\", str(cov_thr),\n",
    "    ]\n",
    "    cmd += ([\"-maxseq\", str(cfg.max_seqs)] if use_maxseq else [\"-n\", str(cfg.max_seqs)])\n",
    "\n",
    "    if cfg.verbose:\n",
    "        print(\"[CMD]\", \" \".join(map(str, cmd)))\n",
    "    code, out, err = run(cmd)\n",
    "    if cfg.verbose and err:\n",
    "        print(\"[HHFILTER][stderr]\\n\", err.strip()[:800])\n",
    "    return code == 0 and out_a3m.exists()\n",
    "\n",
    "\n",
    "def build_msa_for_chain(cfg: MSAConfig, seq: str, out_dir: Path, stem: str) -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Returns Path to filtered .a3m OR None if seq is missing/empty.\n",
    "    \"\"\"  \n",
    "    seq = normalise_seq_for_yaml(seq)\n",
    "    if not has_seq(seq, min_len=1):\n",
    "        return None\n",
    "\n",
    "\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    qfa = out_dir / f\"{stem}.fa\"\n",
    "    qfa.write_text(f\">{stem}\\n{seq}\\n\")\n",
    "\n",
    "    sto = out_dir / f\"{stem}.sto\"\n",
    "    raw_a3m = out_dir / f\"{stem}.a3m\"\n",
    "    filt_a3m = out_dir / f\"{stem}.filt.a3m\"\n",
    "    tbl = out_dir / f\"{stem}.tbl\"\n",
    "\n",
    "    db_fasta = pick_db_for(cfg, stem)\n",
    "\n",
    "    # jackhmmer\n",
    "    cmd = [\n",
    "        \"jackhmmer\",\n",
    "        \"-N\", str(cfg.jack_iters),\n",
    "        \"-A\", str(sto),\n",
    "        \"--tblout\", str(tbl),\n",
    "        \"-E\", str(cfg.evalue),\n",
    "        \"--incE\", str(cfg.evalue),\n",
    "        \"--incdomE\", str(cfg.evalue),\n",
    "        \"--cpu\", str(cfg.cpu_threads),\n",
    "        str(qfa), str(db_fasta),\n",
    "    ]\n",
    "    if cfg.verbose:\n",
    "        print(f\"\\n=== {stem} ===\")\n",
    "        print(\"[CMD]\", \" \".join(map(str, cmd)))\n",
    "\n",
    "    code, out, err = run(cmd)\n",
    "\n",
    "    # If jackhmmer failed or STO empty → just use single-seq A3M\n",
    "    if code != 0 or (not sto.exists()) or sto.stat().st_size < 200:\n",
    "        if cfg.verbose:\n",
    "            print(\"WARN: bad/empty .sto → falling back to single-seq A3M\")\n",
    "            if err:\n",
    "                print(\"[JACK][stderr]\\n\", err.strip()[:800])\n",
    "        raw_a3m.write_text(f\">{stem}\\n{seq}\\n\")\n",
    "        raw = raw_a3m\n",
    "    else:\n",
    "        ok = sto_to_a3m(sto, raw_a3m)\n",
    "        if not ok:\n",
    "            if cfg.verbose:\n",
    "                print(\"WARN: sto->a3m failed; using single-seq fallback\")\n",
    "            raw_a3m.write_text(f\">{stem}\\n{seq}\\n\")\n",
    "        raw = raw_a3m\n",
    "\n",
    "    # hhfilter with chain-specific coverage\n",
    "    cov_thr = cfg.cov_thr_tcr if stem in (\"tcra\", \"tcrb\") else cfg.cov_thr_mhc\n",
    "    ok = hhfilter_cap(cfg, raw, filt_a3m, cov_thr=cov_thr)\n",
    "\n",
    "    if cfg.verbose:\n",
    "        print(\"[CHK] filt a3m:\", filt_a3m, \"nseq=\", count_a3m(filt_a3m))\n",
    "\n",
    "    # cleanup\n",
    "    if not cfg.keep_intermediates:\n",
    "        for p in (sto, tbl, raw_a3m, qfa):\n",
    "            try:\n",
    "                p.unlink()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return filt_a3m if ok else None\n",
    "\n",
    "\n",
    "def relpath_from_base(p: Path, base: Path) -> str:\n",
    "    return str(p.relative_to(base))\n",
    "\n",
    "\n",
    "def make_yaml(\n",
    "    base_dir: Path,\n",
    "    tcra_seq: str,\n",
    "    tcrb_seq: str,\n",
    "    pep: str,\n",
    "    mhc_seq: str,\n",
    "    tcra_msa: Optional[Path],\n",
    "    tcrb_msa: Optional[Path],\n",
    "    mhc_msa: Optional[Path],\n",
    ") -> str:\n",
    "    def msa_field(p: Optional[Path]) -> str:\n",
    "        return \"empty\" if p is None else relpath_from_base(p, base_dir)\n",
    "\n",
    "    # normalise sequences (turn UNK/NA/etc into \"\")\n",
    "    tcra_seq = normalise_seq_for_yaml(tcra_seq)\n",
    "    tcrb_seq = normalise_seq_for_yaml(tcrb_seq)\n",
    "    pep      = normalise_seq_for_yaml(pep)\n",
    "    mhc_seq  = normalise_seq_for_yaml(mhc_seq)\n",
    "\n",
    "    lines = [\"version: 1\", \"sequences:\"]\n",
    "\n",
    "    def add(pid: str, seq: str, msa: str):\n",
    "        lines.append(textwrap.dedent(f\"\"\"\\\n",
    "        - protein:\n",
    "            id: {pid}\n",
    "            sequence: {seq}\n",
    "            msa: {msa}\n",
    "        \"\"\").rstrip())\n",
    "\n",
    "    # Only include a chain if sequence exists\n",
    "    if tcra_seq:\n",
    "        add(\"A\", tcra_seq, msa_field(tcra_msa))\n",
    "    if tcrb_seq:\n",
    "        add(\"B\", tcrb_seq, msa_field(tcrb_msa))\n",
    "    if pep:\n",
    "        add(\"C\", pep, \"empty\")          # keep peptide msa empty\n",
    "    if mhc_seq:\n",
    "        add(\"D\", mhc_seq, msa_field(mhc_msa))\n",
    "\n",
    "    return \"\\n\".join(lines) + \"\\n\"\n",
    "\n",
    "\n",
    "\n",
    "def _pair_id(i: int, pad: int = 3) -> str:\n",
    "    return f\"pair_{i:0{pad}d}\"\n",
    "\n",
    "\n",
    "def get_pair_id(row, i: int, pad: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Prefer an existing pair_id column; otherwise fall back to index-based IDs.\n",
    "    \"\"\"\n",
    "    pid = row.get(\"pair_id\", None)\n",
    "    if isinstance(pid, str) and pid.strip():\n",
    "        return pid.strip()\n",
    "    return f\"pair_{i:0{pad}d}\"\n",
    "\n",
    "\n",
    "def _yaml_done(yml_path: Path) -> bool:\n",
    "    # “Done” = YAML exists and is non-trivial size\n",
    "    return yml_path.exists() and yml_path.stat().st_size > 50\n",
    "\n",
    "\n",
    "def _msas_done(pair_msa_dir: Path, stems=(\"tcra\", \"tcrb\", \"mhc\")) -> bool:\n",
    "    # “Done” = for every stem: either MSA exists OR the chain may be missing.\n",
    "    # We can’t know missingness from filesystem alone, so this is best-effort.\n",
    "    # Use only if you want aggressive skipping when MSAs are present.\n",
    "    for st in stems:\n",
    "        if (pair_msa_dir / f\"{st}.filt.a3m\").exists():\n",
    "            continue\n",
    "        # if no file exists, consider not done (so it can be rebuilt)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def _process_one_row(args):\n",
    "    (i, row_dict, split_name, base_dir_str, yaml_dir_str, msa_root_str, cfg, pad) = args\n",
    "    base_dir = Path(base_dir_str)\n",
    "    yaml_dir = Path(yaml_dir_str)\n",
    "    msa_root = Path(msa_root_str)\n",
    "\n",
    "    pair_id = get_pair_id(row_dict, i=i, pad=pad)\n",
    "\n",
    "    yml_path = yaml_dir / f\"{pair_id}.yaml\"\n",
    "    pair_msa_dir = msa_root / split_name / pair_id\n",
    "\n",
    "    pep  = normalise_seq_for_yaml(row_dict.get(\"Peptide\", \"\"))\n",
    "    mhc  = normalise_seq_for_yaml(row_dict.get(\"HLA_sequence\", \"\"))\n",
    "    tcra = normalise_seq_for_yaml(row_dict.get(\"TCRa\", \"\"))\n",
    "    tcrb = normalise_seq_for_yaml(row_dict.get(\"TCRb\", \"\"))\n",
    "\n",
    "    tcra_a3m = build_msa_for_chain(cfg, tcra, pair_msa_dir, \"tcra\")\n",
    "    tcrb_a3m = build_msa_for_chain(cfg, tcrb, pair_msa_dir, \"tcrb\")\n",
    "    mhc_a3m  = build_msa_for_chain(cfg, mhc,  pair_msa_dir, \"mhc\")\n",
    "\n",
    "    yml_text = make_yaml(\n",
    "        base_dir=base_dir,\n",
    "        tcra_seq=tcra,\n",
    "        tcrb_seq=tcrb,\n",
    "        pep=pep,\n",
    "        mhc_seq=mhc,\n",
    "        tcra_msa=tcra_a3m,\n",
    "        tcrb_msa=tcrb_a3m,\n",
    "        mhc_msa=mhc_a3m,\n",
    "    )\n",
    "    yml_path.write_text(yml_text)\n",
    "\n",
    "    return pair_id, True, \"\"\n",
    "\n",
    "\n",
    "\n",
    "def process_split_parallel_resume(\n",
    "    df: pd.DataFrame,\n",
    "    split_name: str,\n",
    "    base_dir: Path,\n",
    "    yaml_dir: Path,\n",
    "    msa_root: Path,\n",
    "    cfg: MSAConfig,\n",
    "    *,\n",
    "    pad: int = 3,\n",
    "    resume: bool = True,\n",
    "    skip_if_yaml_exists: bool = True,\n",
    "    skip_if_msas_exist: bool = False,\n",
    "    max_workers: Optional[int] = None,\n",
    "    mani_dir: Optional[Path] = None,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    yaml_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if mani_dir is not None:\n",
    "        mani_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if max_workers is None:\n",
    "        ncpu = os.cpu_count() or 8\n",
    "        max_workers = max(2, min(6, ncpu // max(1, cfg.cpu_threads)))\n",
    "\n",
    "    manifest_rows = []\n",
    "    df2 = df.reset_index(drop=True)\n",
    "\n",
    "    # --- build manifest rows (pair_id-stable)\n",
    "    for i, row in df2.iterrows():\n",
    "        pair_id = get_pair_id(row, i=i, pad=pad)\n",
    "\n",
    "        pep  = normalise_seq_for_yaml(row.get(\"Peptide\", \"\"))\n",
    "        mhc  = normalise_seq_for_yaml(row.get(\"HLA_sequence\", \"\"))\n",
    "        tcra = normalise_seq_for_yaml(row.get(\"TCRa\", \"\"))\n",
    "        tcrb = normalise_seq_for_yaml(row.get(\"TCRb\", \"\"))\n",
    "\n",
    "        yml_path = yaml_dir / f\"{pair_id}.yaml\"\n",
    "        try:\n",
    "            yml_rel = str(yml_path.relative_to(base_dir))\n",
    "        except ValueError:\n",
    "            yml_rel = str(yml_path)\n",
    "\n",
    "        manifest_rows.append({\n",
    "            \"pair_id\": pair_id,\n",
    "            \"yaml_path\": yml_rel,\n",
    "            \"pep_len\": len(pep),\n",
    "            \"tcra_len\": len(tcra),\n",
    "            \"tcrb_len\": len(tcrb),\n",
    "            \"hla_len\": len(mhc),\n",
    "        })\n",
    "\n",
    "    # --- build todo list (resume-aware)\n",
    "    todo = []\n",
    "    for i, row in df2.iterrows():\n",
    "        pair_id = get_pair_id(row, i=i, pad=pad)\n",
    "\n",
    "        yml_path = yaml_dir / f\"{pair_id}.yaml\"\n",
    "        pair_msa_dir = msa_root / split_name / pair_id\n",
    "\n",
    "        if resume:\n",
    "            if skip_if_yaml_exists and _yaml_done(yml_path):\n",
    "                continue\n",
    "            if skip_if_msas_exist and _msas_done(pair_msa_dir):\n",
    "                continue\n",
    "\n",
    "        todo.append((i, row.to_dict(), split_name, str(base_dir), str(yaml_dir), str(msa_root), cfg, pad))\n",
    "\n",
    "    print(f\"[{split_name}] total rows: {len(df2)} | to process: {len(todo)} | workers: {max_workers}\")\n",
    "\n",
    "    n_ok = 0\n",
    "    n_fail = 0\n",
    "    if todo:\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futures = [ex.submit(_process_one_row, t) for t in todo]\n",
    "            for fut in as_completed(futures):\n",
    "                try:\n",
    "                    pair_id, wrote, err = fut.result()\n",
    "                    n_ok += int(wrote)\n",
    "                except Exception as e:\n",
    "                    n_fail += 1\n",
    "                    print(\"[ERR]\", repr(e))\n",
    "\n",
    "    print(f\"[{split_name}] completed: {n_ok} | failed: {n_fail} | skipped: {len(df2) - len(todo)}\")\n",
    "\n",
    "    manifest_df = pd.DataFrame(manifest_rows)\n",
    "    if mani_dir is not None:\n",
    "        manifest_df.to_csv(mani_dir / f\"{split_name}_manifest.csv\", index=False)\n",
    "\n",
    "    return manifest_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efac3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"Peptide\", \"HLA_sequence\", \"TCRa\", \"TCRb\"]:\n",
    "    train_df[col] = train_df[col].apply(clean_seq)\n",
    "    val_df[col]   = val_df[col].apply(clean_seq)\n",
    "    test_df[col]  = test_df[col].apply(clean_seq)\n",
    "\n",
    "\n",
    "cfg = MSAConfig(\n",
    "    out_root=OUT_ROOT,                 # (not used in functions, but fine to keep)\n",
    "    db_combined=DB_COMBINED,\n",
    "    dbs=DBS,\n",
    "    verbose=VERBOSE,\n",
    "    keep_intermediates=KEEP_INTERMEDIATES,\n",
    "    jack_iters=JACK_ITERS,\n",
    "    evalue=EVALUE,\n",
    "    cpu_threads=CPU_THREADS,\n",
    "    max_seqs=MAX_SEQS,\n",
    "    id_thr=ID_THR_DEFAULT,\n",
    "    cov_thr_tcr=COV_THR_TCR,\n",
    "    cov_thr_mhc=COV_THR_MHC,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_manifest = process_split_parallel_resume(\n",
    "    train_df, \"train\", BASE_DIR, TRAIN_DIR, MSA_ROOT, cfg,\n",
    "    resume=True,\n",
    "    skip_if_yaml_exists=True,\n",
    "    max_workers=6,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "val_manifest = process_split_parallel_resume(\n",
    "    val_df, \"val\", BASE_DIR, VAL_DIR, MSA_ROOT, cfg,\n",
    "    resume=True, skip_if_yaml_exists=True\n",
    ")\n",
    "test_manifest = process_split_parallel_resume(\n",
    "    test_df, \"test\", BASE_DIR, TEST_DIR, MSA_ROOT, cfg,\n",
    "    resume=True, skip_if_yaml_exists=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6351c2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = MSAConfig(\n",
    "    out_root=OUT_ROOT,                 # (not used in functions, but fine to keep)\n",
    "    db_combined=DB_COMBINED,\n",
    "    dbs=DBS,\n",
    "    verbose=VERBOSE,\n",
    "    keep_intermediates=KEEP_INTERMEDIATES,\n",
    "    jack_iters=JACK_ITERS,\n",
    "    evalue=EVALUE,\n",
    "    cpu_threads=CPU_THREADS,\n",
    "    max_seqs=MAX_SEQS,\n",
    "    id_thr=ID_THR_DEFAULT,\n",
    "    cov_thr_tcr=COV_THR_TCR,\n",
    "    cov_thr_mhc=COV_THR_MHC,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fc8da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] total rows: 32691 | to process: 3907 | workers: 6\n",
      "[train] completed: 3907 | failed: 0 | skipped: 28784\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_manifest = process_split_parallel_resume(\n",
    "    train_df, \"train\", BASE_DIR, TRAIN_DIR, MSA_ROOT, cfg,\n",
    "    resume=True,\n",
    "    skip_if_yaml_exists=True,\n",
    "    max_workers=6,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4347b3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] total rows: 1968 | to process: 1968 | workers: 2\n",
      "[val] completed: 1968 | failed: 0 | skipped: 0\n",
      "[test] total rows: 2214 | to process: 2214 | workers: 2\n",
      "[test] completed: 2214 | failed: 0 | skipped: 0\n"
     ]
    }
   ],
   "source": [
    "val_manifest = process_split_parallel_resume(\n",
    "    val_df, \"val\", BASE_DIR, VAL_DIR, MSA_ROOT, cfg,\n",
    "    resume=True, skip_if_yaml_exists=True\n",
    ")\n",
    "test_manifest = process_split_parallel_resume(\n",
    "    test_df, \"test\", BASE_DIR, TEST_DIR, MSA_ROOT, cfg,\n",
    "    resume=True, skip_if_yaml_exists=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79425e9",
   "metadata": {},
   "source": [
    "Fix 'bad' YAML files that have 'UNK' in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ee87708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yaml_has_unk(yml_path: Path) -> bool:\n",
    "    if not yml_path.exists():\n",
    "        return False\n",
    "    try:\n",
    "        txt = yml_path.read_text()\n",
    "    except Exception:\n",
    "        return False\n",
    "    return (\n",
    "        \"sequence: UNK\" in txt\n",
    "        or \"sequence: UNKNOWN\" in txt\n",
    "        or \"sequence: NA\" in txt\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ebdedaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_bad_yaml(yml_path: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Rewrite a YAML file in-place if it contains UNK/UNKNOWN sequences.\n",
    "    Returns True if rewritten, False otherwise.\n",
    "    \"\"\"\n",
    "    if not yaml_has_unk(yml_path):\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        lines = yml_path.read_text().splitlines()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not read {yml_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "    new_lines = []\n",
    "    i = 0\n",
    "    n = len(lines)\n",
    "\n",
    "    while i < n:\n",
    "        line = lines[i]\n",
    "\n",
    "        # detect start of a protein block\n",
    "        if line.strip() == \"- protein:\":\n",
    "            block = [line]\n",
    "            i += 1\n",
    "            while i < n and not lines[i].startswith(\"- protein:\"):\n",
    "                block.append(lines[i])\n",
    "                i += 1\n",
    "\n",
    "            # decide whether to keep this block\n",
    "            block_text = \"\\n\".join(block)\n",
    "            if (\n",
    "                \"sequence: UNK\" in block_text\n",
    "                or \"sequence: UNKNOWN\" in block_text\n",
    "                or \"sequence: NA\" in block_text\n",
    "            ):\n",
    "                # drop this entire protein block\n",
    "                continue\n",
    "            else:\n",
    "                new_lines.extend(block)\n",
    "        else:\n",
    "            new_lines.append(line)\n",
    "            i += 1\n",
    "\n",
    "    # Write back only if something actually changed\n",
    "    new_text = \"\\n\".join(new_lines).rstrip() + \"\\n\"\n",
    "    yml_path.write_text(new_text)\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0372d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_bad_yamls_in_dir(yaml_dir: Path) -> None:\n",
    "    yamls = sorted(yaml_dir.glob(\"*.yaml\"))\n",
    "    n_bad = 0\n",
    "    n_fixed = 0\n",
    "\n",
    "    for yml in yamls:\n",
    "        if yaml_has_unk(yml):\n",
    "            n_bad += 1\n",
    "            if rewrite_bad_yaml(yml):\n",
    "                n_fixed += 1\n",
    "\n",
    "    print(f\"[CLEANUP] {yaml_dir}\")\n",
    "    print(f\"  bad YAMLs detected: {n_bad}\")\n",
    "    print(f\"  rewritten:          {n_fixed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dffc62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLEANUP] /home/natasha/multimodal_model/data/train\n",
      "  bad YAMLs detected: 4845\n",
      "  rewritten:          4845\n",
      "[CLEANUP] /home/natasha/multimodal_model/data/val\n",
      "  bad YAMLs detected: 230\n",
      "  rewritten:          230\n",
      "[CLEANUP] /home/natasha/multimodal_model/data/test\n",
      "  bad YAMLs detected: 220\n",
      "  rewritten:          220\n"
     ]
    }
   ],
   "source": [
    "rewrite_bad_yamls_in_dir(TRAIN_DIR)\n",
    "rewrite_bad_yamls_in_dir(VAL_DIR)\n",
    "rewrite_bad_yamls_in_dir(TEST_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a0edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE\n",
    "\n",
    "# TO DO:\n",
    "\n",
    "# Remove YAML files where both TCRa abd TCR b are missing - so ones that only have two proteins tagged in them\n",
    "# Check that YAML file names are the same as in the paid_id in the manifest for Boltz runs and correct lengths in model \n",
    "# Save manifest files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e642c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove YAML files where both TCRa and TCRb are missing\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import yaml\n",
    "\n",
    "def _load_yaml(yml_path: Path) -> dict:\n",
    "    return yaml.safe_load(yml_path.read_text())\n",
    "\n",
    "def _extract_protein_seq(doc: dict, pid: str) -> str:\n",
    "    # returns \"\" if missing\n",
    "    seqs = doc.get(\"sequences\", []) or []\n",
    "    for item in seqs:\n",
    "        prot = (item or {}).get(\"protein\", {}) or {}\n",
    "        if str(prot.get(\"id\", \"\")).strip() == pid:\n",
    "            s = prot.get(\"sequence\", \"\")\n",
    "            return \"\" if s is None else str(s).strip()\n",
    "    return \"\"\n",
    "\n",
    "def remove_no_tcr_yamls(\n",
    "    yaml_dir: Path,\n",
    "    *,\n",
    "    quarantine_dir: Path | None = None,\n",
    "    dry_run: bool = True,\n",
    ") -> dict:\n",
    "    yamls = sorted(yaml_dir.glob(\"*.yaml\"))\n",
    "    n_total = 0\n",
    "    n_removed = 0\n",
    "    bad_parse = 0\n",
    "\n",
    "    if quarantine_dir is not None:\n",
    "        quarantine_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for yml in yamls:\n",
    "        n_total += 1\n",
    "        try:\n",
    "            doc = _load_yaml(yml)\n",
    "        except Exception:\n",
    "            bad_parse += 1\n",
    "            continue\n",
    "\n",
    "        a = _extract_protein_seq(doc, \"A\")\n",
    "        b = _extract_protein_seq(doc, \"B\")\n",
    "\n",
    "        no_tcr = (len(a) == 0) and (len(b) == 0)\n",
    "        if not no_tcr:\n",
    "            continue\n",
    "\n",
    "        n_removed += 1\n",
    "        if dry_run:\n",
    "            continue\n",
    "\n",
    "        if quarantine_dir is not None:\n",
    "            shutil.move(str(yml), str(quarantine_dir / yml.name))\n",
    "        else:\n",
    "            yml.unlink(missing_ok=True)\n",
    "\n",
    "    summary = {\n",
    "        \"yaml_dir\": str(yaml_dir),\n",
    "        \"total_yaml\": n_total,\n",
    "        \"bad_parse\": bad_parse,\n",
    "        \"no_tcr_found\": n_removed,\n",
    "        \"action\": \"dry_run\" if dry_run else (\"moved\" if quarantine_dir else \"deleted\"),\n",
    "    }\n",
    "    print(summary)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "820ae329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yaml_dir': '/home/natasha/multimodal_model/data/train', 'total_yaml': 32691, 'bad_parse': 0, 'no_tcr_found': 1409, 'action': 'dry_run'}\n",
      "{'yaml_dir': '/home/natasha/multimodal_model/data/val', 'total_yaml': 1968, 'bad_parse': 0, 'no_tcr_found': 16, 'action': 'dry_run'}\n",
      "{'yaml_dir': '/home/natasha/multimodal_model/data/test', 'total_yaml': 2214, 'bad_parse': 0, 'no_tcr_found': 18, 'action': 'dry_run'}\n",
      "{'yaml_dir': '/home/natasha/multimodal_model/data/train', 'total_yaml': 32691, 'bad_parse': 0, 'no_tcr_found': 1409, 'action': 'moved'}\n",
      "{'yaml_dir': '/home/natasha/multimodal_model/data/val', 'total_yaml': 1968, 'bad_parse': 0, 'no_tcr_found': 16, 'action': 'moved'}\n",
      "{'yaml_dir': '/home/natasha/multimodal_model/data/test', 'total_yaml': 2214, 'bad_parse': 0, 'no_tcr_found': 18, 'action': 'moved'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'yaml_dir': '/home/natasha/multimodal_model/data/test',\n",
       " 'total_yaml': 2214,\n",
       " 'bad_parse': 0,\n",
       " 'no_tcr_found': 18,\n",
       " 'action': 'moved'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DIR = Path(\"/home/natasha/multimodal_model/data/train\")\n",
    "VAL_DIR   = Path(\"/home/natasha/multimodal_model/data/val\")\n",
    "TEST_DIR  = Path(\"/home/natasha/multimodal_model/data/test\")\n",
    "\n",
    "# 1) Dry-run first (recommended)\n",
    "remove_no_tcr_yamls(TRAIN_DIR, dry_run=True)\n",
    "remove_no_tcr_yamls(VAL_DIR, dry_run=True)\n",
    "remove_no_tcr_yamls(TEST_DIR, dry_run=True)\n",
    "\n",
    "# 2) Then actually move them somewhere (safer than delete)\n",
    "remove_no_tcr_yamls(TRAIN_DIR, quarantine_dir=TRAIN_DIR/\"_quarantine_no_tcr\", dry_run=False)\n",
    "remove_no_tcr_yamls(VAL_DIR,   quarantine_dir=VAL_DIR/\"_quarantine_no_tcr\",   dry_run=False)\n",
    "remove_no_tcr_yamls(TEST_DIR,  quarantine_dir=TEST_DIR/\"_quarantine_no_tcr\",  dry_run=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03522415",
   "metadata": {},
   "source": [
    "##### Function to write manifest files manually (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8dbdf68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "def write_manifest_from_yamls(\n",
    "    yaml_dir: Path,\n",
    "    *,\n",
    "    base_dir: Path | None = None,\n",
    "    split_name: str | None = None,\n",
    "    mani_dir: Path | None = None,\n",
    "    df: pd.DataFrame | None = None,\n",
    "    label_cols: list[str] | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a manifest exactly matching YAMLs in yaml_dir.\n",
    "\n",
    "    If df is provided, it will try to attach label columns by mapping pair_id -> row index\n",
    "    (pair_000, pair_001, ...). This assumes your original pair_id numbering came from df row order.\n",
    "    \"\"\"\n",
    "    yamls = sorted(yaml_dir.glob(\"*.yaml\"))\n",
    "    rows = []\n",
    "    bad_parse = 0\n",
    "\n",
    "    for yml in yamls:\n",
    "        pair_id = yml.stem  # \"pair_000123\" etc.\n",
    "\n",
    "        try:\n",
    "            doc = yaml.safe_load(yml.read_text())\n",
    "        except Exception:\n",
    "            bad_parse += 1\n",
    "            continue\n",
    "\n",
    "        a = _extract_protein_seq(doc, \"A\")\n",
    "        b = _extract_protein_seq(doc, \"B\")\n",
    "        c = _extract_protein_seq(doc, \"C\")\n",
    "        d = _extract_protein_seq(doc, \"D\")\n",
    "\n",
    "        # yaml path in manifest: relative to base_dir if provided; otherwise absolute\n",
    "        if base_dir is not None:\n",
    "            try:\n",
    "                yaml_path = str(yml.relative_to(base_dir))\n",
    "            except ValueError:\n",
    "                yaml_path = str(yml)\n",
    "        else:\n",
    "            yaml_path = str(yml)\n",
    "\n",
    "        row = {\n",
    "            \"pair_id\": pair_id,\n",
    "            \"yaml_path\": yaml_path,\n",
    "            \"pep_len\": len(c),\n",
    "            \"tcra_len\": len(a),\n",
    "            \"tcrb_len\": len(b),\n",
    "            \"hla_len\": len(d),\n",
    "        }\n",
    "\n",
    "        # Optional: join labels from original df by decoding pair_id index\n",
    "        if df is not None and label_cols:\n",
    "            try:\n",
    "                idx = int(pair_id.split(\"_\")[1])\n",
    "                for col in label_cols:\n",
    "                    row[col] = df.iloc[idx][col]\n",
    "            except Exception:\n",
    "                for col in label_cols:\n",
    "                    row[col] = None\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    manifest_df = pd.DataFrame(rows)\n",
    "    print(f\"[MANIFEST] {yaml_dir} | yamls={len(yamls)} | rows={len(manifest_df)} | bad_parse={bad_parse}\")\n",
    "\n",
    "    if mani_dir is not None:\n",
    "        mani_dir.mkdir(parents=True, exist_ok=True)\n",
    "        if split_name is None:\n",
    "            split_name = yaml_dir.name\n",
    "        out = mani_dir / f\"{split_name}_manifest.csv\"\n",
    "        manifest_df.to_csv(out, index=False)\n",
    "        print(f\"[MANIFEST] wrote: {out}\")\n",
    "\n",
    "    return manifest_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af300126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MANIFEST] /home/natasha/multimodal_model/data/train | yamls=31282 | rows=31282 | bad_parse=0\n",
      "[MANIFEST] wrote: /home/natasha/multimodal_model/manifests/train_manifest.csv\n",
      "[MANIFEST] /home/natasha/multimodal_model/data/val | yamls=1952 | rows=1952 | bad_parse=0\n",
      "[MANIFEST] wrote: /home/natasha/multimodal_model/manifests/val_manifest.csv\n",
      "[MANIFEST] /home/natasha/multimodal_model/data/test | yamls=2196 | rows=2196 | bad_parse=0\n",
      "[MANIFEST] wrote: /home/natasha/multimodal_model/manifests/test_manifest.csv\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path(\"/home/natasha/multimodal_model\")\n",
    "MANI_DIR = BASE_DIR / \"manifests\"\n",
    "\n",
    "train_manifest = write_manifest_from_yamls(TRAIN_DIR, base_dir=BASE_DIR, split_name=\"train\", mani_dir=MANI_DIR)\n",
    "val_manifest   = write_manifest_from_yamls(VAL_DIR,   base_dir=BASE_DIR, split_name=\"val\",   mani_dir=MANI_DIR)\n",
    "test_manifest  = write_manifest_from_yamls(TEST_DIR,  base_dir=BASE_DIR, split_name=\"test\",  mani_dir=MANI_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014076d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>yaml_path</th>\n",
       "      <th>pep_len</th>\n",
       "      <th>tcra_len</th>\n",
       "      <th>tcrb_len</th>\n",
       "      <th>hla_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pair_000</td>\n",
       "      <td>data/train/pair_000.yaml</td>\n",
       "      <td>9</td>\n",
       "      <td>110</td>\n",
       "      <td>114</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pair_002</td>\n",
       "      <td>data/train/pair_002.yaml</td>\n",
       "      <td>10</td>\n",
       "      <td>112</td>\n",
       "      <td>114</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pair_003</td>\n",
       "      <td>data/train/pair_003.yaml</td>\n",
       "      <td>10</td>\n",
       "      <td>112</td>\n",
       "      <td>114</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pair_004</td>\n",
       "      <td>data/train/pair_004.yaml</td>\n",
       "      <td>9</td>\n",
       "      <td>114</td>\n",
       "      <td>115</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pair_005</td>\n",
       "      <td>data/train/pair_005.yaml</td>\n",
       "      <td>10</td>\n",
       "      <td>113</td>\n",
       "      <td>115</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pair_006</td>\n",
       "      <td>data/train/pair_006.yaml</td>\n",
       "      <td>10</td>\n",
       "      <td>113</td>\n",
       "      <td>115</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pair_007</td>\n",
       "      <td>data/train/pair_007.yaml</td>\n",
       "      <td>10</td>\n",
       "      <td>113</td>\n",
       "      <td>115</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pair_008</td>\n",
       "      <td>data/train/pair_008.yaml</td>\n",
       "      <td>10</td>\n",
       "      <td>113</td>\n",
       "      <td>115</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pair_009</td>\n",
       "      <td>data/train/pair_009.yaml</td>\n",
       "      <td>10</td>\n",
       "      <td>113</td>\n",
       "      <td>115</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pair_010</td>\n",
       "      <td>data/train/pair_010.yaml</td>\n",
       "      <td>10</td>\n",
       "      <td>114</td>\n",
       "      <td>119</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pair_id                 yaml_path  pep_len  tcra_len  tcrb_len  hla_len\n",
       "0  pair_000  data/train/pair_000.yaml        9       110       114      365\n",
       "1  pair_002  data/train/pair_002.yaml       10       112       114      365\n",
       "2  pair_003  data/train/pair_003.yaml       10       112       114      365\n",
       "3  pair_004  data/train/pair_004.yaml        9       114       115      365\n",
       "4  pair_005  data/train/pair_005.yaml       10       113       115      365\n",
       "5  pair_006  data/train/pair_006.yaml       10       113       115      365\n",
       "6  pair_007  data/train/pair_007.yaml       10       113       115      365\n",
       "7  pair_008  data/train/pair_008.yaml       10       113       115      365\n",
       "8  pair_009  data/train/pair_009.yaml       10       113       115      365\n",
       "9  pair_010  data/train/pair_010.yaml       10       114       119      365"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_manifest.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e233a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_manifest_from_df(df: pd.DataFrame, split_name: str, base_dir: Path, yaml_dir: Path, mani_dir: Path, *, pad: int = 3) -> pd.DataFrame:\n",
    "    mani_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    manifest_rows = []\n",
    "    df2 = df.reset_index(drop=True)\n",
    "\n",
    "    for i, row in df2.iterrows():\n",
    "        pair_id = f\"pair_{i:0{pad}d}\"\n",
    "\n",
    "        pep  = normalise_seq_for_yaml(row.get(\"Peptide\", \"\"))\n",
    "        mhc  = normalise_seq_for_yaml(row.get(\"HLA_sequence\", \"\"))\n",
    "        tcra = normalise_seq_for_yaml(row.get(\"TCRa\", \"\"))\n",
    "        tcrb = normalise_seq_for_yaml(row.get(\"TCRb\", \"\"))\n",
    "\n",
    "        yml_path = yaml_dir / f\"{pair_id}.yaml\"\n",
    "        try:\n",
    "            yml_rel = str(yml_path.relative_to(base_dir))\n",
    "        except ValueError:\n",
    "            yml_rel = str(yml_path)\n",
    "\n",
    "        manifest_rows.append({\n",
    "            \"pair_id\": pair_id,\n",
    "            \"yaml_path\": yml_rel,\n",
    "            \"pep_len\": len(pep),\n",
    "            \"tcra_len\": len(tcra),\n",
    "            \"tcrb_len\": len(tcrb),\n",
    "            \"hla_len\": len(mhc),\n",
    "        })\n",
    "\n",
    "    manifest_df = pd.DataFrame(manifest_rows)\n",
    "    manifest_df.to_csv(mani_dir / f\"{split_name}_manifest.csv\", index=False)\n",
    "    return manifest_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da470ae3",
   "metadata": {},
   "source": [
    "Add an explicit 'pair_id' column to the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf6ca00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be21d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE\n",
    "\n",
    "# What I need to do:\n",
    "\n",
    "# I need to rewrite the code so it skips the alpha or beta chain if it's missing - check this for the manifest file creation, would it spot if there was no chain to log as length = 0?\n",
    "# I need to also then decide whether or not to remove the whole row and therefore data point if the TCR is completely missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6534d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSA_ROOT = BASE_DIR / \"data\" / \"raw\" / \"MSA\" / \"jackhmmer_msas\"\n",
    "\n",
    "# train_manifest = process_split(train_df, \"train\", BASE_DIR, TRAIN_DIR, MSA_ROOT, cfg)\n",
    "# val_manifest   = process_split(val_df,   \"val\",   BASE_DIR, VAL_DIR,   MSA_ROOT, cfg)\n",
    "# test_manifest  = process_split(test_df,  \"test\",  BASE_DIR, TEST_DIR,  MSA_ROOT, cfg)\n",
    "\n",
    "# train_manifest.to_csv(MANI_DIR / \"train_manifest.csv\", index=False)\n",
    "# val_manifest.to_csv(MANI_DIR / \"val_manifest.csv\", index=False)\n",
    "# test_manifest.to_csv(MANI_DIR / \"test_manifest.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685ef958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcr-multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
