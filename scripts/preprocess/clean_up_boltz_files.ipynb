{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69dac9d1",
   "metadata": {},
   "source": [
    "This script cleans Boltz outputs for freeing up disk space. Below is a single cleanup function for an entire run directory (all chunks)\n",
    "\n",
    "This will:\n",
    "\n",
    "1. rewrite embeddings to “z-only” (skipping already-done)\n",
    "2. delete the heavy extra files\n",
    "3. optionally delete any stray *.tmp.npz files\n",
    "4. give you a summary of space reclaimed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "609791a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "HEAVY_PATTERNS = [\n",
    "    \"pae_*_model_0.npz\",\n",
    "    \"pde_*_model_0.npz\",\n",
    "    \"plddt_*_model_0.npz\",\n",
    "    \"*_model_0.cif\",\n",
    "]\n",
    "\n",
    "def keep_only_z_in_npz_skip(npz_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Returns: \"rewritten\" | \"already_z\" | \"missing_z\" | \"fail\"\n",
    "    \"\"\"\n",
    "    npz_path = Path(npz_path)\n",
    "    try:\n",
    "        with np.load(npz_path, allow_pickle=False) as arr:\n",
    "            # already cleaned\n",
    "            if arr.files == [\"z\"]:\n",
    "                return \"already_z\"\n",
    "            if \"z\" not in arr.files:\n",
    "                return \"missing_z\"\n",
    "            z = arr[\"z\"]\n",
    "    except Exception as e:\n",
    "        print(\"FAIL load:\", npz_path, \"err:\", repr(e))\n",
    "        return \"fail\"\n",
    "\n",
    "    tmp = npz_path.with_name(npz_path.name + \".tmp.npz\")\n",
    "    try:\n",
    "        np.savez_compressed(tmp, z=z)\n",
    "        os.replace(tmp, npz_path)\n",
    "        return \"rewritten\"\n",
    "    except Exception as e:\n",
    "        print(\"FAIL write:\", npz_path, \"err:\", repr(e))\n",
    "        try:\n",
    "            if tmp.exists():\n",
    "                tmp.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return \"fail\"\n",
    "\n",
    "\n",
    "def cleanup_boltz_predictions(run_root: Path, *, keep_json=True, dry_run=False):\n",
    "    \"\"\"\n",
    "    run_root example:\n",
    "      /home/natasha/multimodal_model/outputs/train\n",
    "    This will walk ALL chunks under run_root and clean each predictions/pair_xxx folder.\n",
    "    \"\"\"\n",
    "    run_root = Path(run_root)\n",
    "\n",
    "    # find all pair prediction dirs that contain embeddings_*.npz\n",
    "    emb_files = list(run_root.rglob(\"predictions/pair_*/embeddings_pair_*.npz\"))\n",
    "    print(\"Found embeddings:\", len(emb_files))\n",
    "\n",
    "    # size before\n",
    "    def total_size(paths):\n",
    "        s = 0\n",
    "        for p in paths:\n",
    "            try:\n",
    "                s += p.stat().st_size\n",
    "            except Exception:\n",
    "                pass\n",
    "        return s\n",
    "\n",
    "    before = total_size(emb_files)\n",
    "\n",
    "    stats = {\"rewritten\":0, \"already_z\":0, \"missing_z\":0, \"fail\":0}\n",
    "    deleted_files = 0\n",
    "    deleted_bytes = 0\n",
    "\n",
    "    # 1) rewrite embeddings to z-only\n",
    "    for p in emb_files:\n",
    "        status = keep_only_z_in_npz_skip(p) if not dry_run else \"dry_run\"\n",
    "        if status in stats:\n",
    "            stats[status] += 1\n",
    "\n",
    "    # 2) delete heavy extras in the same pair dirs\n",
    "    pair_dirs = sorted({p.parent for p in emb_files})  # predictions/pair_xxx\n",
    "    for d in pair_dirs:\n",
    "        # heavy artifacts\n",
    "        for pat in HEAVY_PATTERNS:\n",
    "            for f in d.glob(pat):\n",
    "                try:\n",
    "                    sz = f.stat().st_size\n",
    "                    if not dry_run:\n",
    "                        f.unlink()\n",
    "                    deleted_files += 1\n",
    "                    deleted_bytes += sz\n",
    "                except Exception as e:\n",
    "                    print(\"WARN could not delete:\", f, \"err:\", repr(e))\n",
    "\n",
    "        # optional: if you *don't* want json\n",
    "        if not keep_json:\n",
    "            for f in d.glob(\"confidence_*.json\"):\n",
    "                try:\n",
    "                    sz = f.stat().st_size\n",
    "                    if not dry_run:\n",
    "                        f.unlink()\n",
    "                    deleted_files += 1\n",
    "                    deleted_bytes += sz\n",
    "                except Exception as e:\n",
    "                    print(\"WARN could not delete:\", f, \"err:\", repr(e))\n",
    "\n",
    "    # 3) remove any stray temp files from previous attempts\n",
    "    tmp_files = list(run_root.rglob(\"*.tmp.npz\"))\n",
    "    for f in tmp_files:\n",
    "        try:\n",
    "            sz = f.stat().st_size\n",
    "            if not dry_run:\n",
    "                f.unlink()\n",
    "            deleted_files += 1\n",
    "            deleted_bytes += sz\n",
    "        except Exception as e:\n",
    "            print(\"WARN could not delete tmp:\", f, \"err:\", repr(e))\n",
    "\n",
    "    # size after (embeddings only)\n",
    "    after = total_size(emb_files)\n",
    "\n",
    "    print(\"\\n=== Embedding rewrite stats ===\")\n",
    "    for k,v in stats.items():\n",
    "        print(f\"{k:>10}: {v}\")\n",
    "\n",
    "    print(\"\\n=== Deletions ===\")\n",
    "    print(\"files deleted:\", deleted_files)\n",
    "    print(\"GB deleted (approx):\", deleted_bytes / (1024**3))\n",
    "\n",
    "    print(\"\\n=== Embeddings total size ===\")\n",
    "    print(\"before (GB):\", before / (1024**3))\n",
    "    print(\"after  (GB):\", after / (1024**3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d1972ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings: 4288\n",
      "\n",
      "=== Embedding rewrite stats ===\n",
      " rewritten: 0\n",
      " already_z: 0\n",
      " missing_z: 0\n",
      "      fail: 0\n",
      "\n",
      "=== Deletions ===\n",
      "files deleted: 17152\n",
      "GB deleted (approx): 11.51417211163789\n",
      "\n",
      "=== Embeddings total size ===\n",
      "before (GB): 653.1688089426607\n",
      "after  (GB): 653.1688089426607\n"
     ]
    }
   ],
   "source": [
    "cleanup_boltz_predictions(\n",
    "    Path(\"/home/natasha/multimodal_model/outputs/train\"),\n",
    "    keep_json=True,\n",
    "    dry_run=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2fb773c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings: 4288\n",
      "\n",
      "=== Embedding rewrite stats ===\n",
      " rewritten: 4286\n",
      " already_z: 1\n",
      " missing_z: 1\n",
      "      fail: 0\n",
      "\n",
      "=== Deletions ===\n",
      "files deleted: 17152\n",
      "GB deleted (approx): 11.51417211163789\n",
      "\n",
      "=== Embeddings total size ===\n",
      "before (GB): 653.1688089426607\n",
      "after  (GB): 649.8201619535685\n"
     ]
    }
   ],
   "source": [
    "cleanup_boltz_predictions(\n",
    "    Path(\"/home/natasha/multimodal_model/outputs/train\"),\n",
    "    keep_json=True,\n",
    "    dry_run=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a154fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be12ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcr-multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
