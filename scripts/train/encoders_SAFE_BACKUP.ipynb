{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e2b02a3",
   "metadata": {},
   "source": [
    "##### This script defines the encoders for the multimodal model\n",
    "##### Overview:\n",
    "- The inputs are the ESMC and Boltz embeddings \n",
    "- 3 encoders from PEFT of ESMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91aae1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlined imports - removing duplicates\n",
    "import esm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import subprocess\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, update_display\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ESM imports\n",
    "from esm.models.esmc import ESMC\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig, ESMProteinTensor\n",
    "from esm.models.esmc import _BatchedESMProteinTensor\n",
    "\n",
    "# Tokenizer imports\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import decoders\n",
    "\n",
    "# PEFT imports\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from peft.tuners.lora import LoraConfig, LoraModel\n",
    "\n",
    "# Set environment variable\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31bb77ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/natasha/multimodal_model/scripts/train\n",
      "Project root: /home/natasha/multimodal_model\n",
      "Models directory at: /home/natasha/multimodal_model/models\n",
      "Checkpoints directory at: /home/natasha/multimodal_model/models/checkpoints\n"
     ]
    }
   ],
   "source": [
    "# Get current working directory and create models folder\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "# Get the project root (go up one level from scripts/train)\n",
    "project_root = Path(current_dir).parent.parent\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Create models directory in project root\n",
    "models_dir = project_root / \"models\"\n",
    "if not models_dir.exists():\n",
    "    print(f\"Models directory does not exist, creating it at: {models_dir}\")\n",
    "    models_dir.mkdir(exist_ok=True)\n",
    "print(f\"Models directory at: {models_dir}\")\n",
    "\n",
    "# Also create a checkpoints subdirectory for saving model checkpoints\n",
    "checkpoints_dir = models_dir / \"checkpoints\"\n",
    "if not checkpoints_dir.exists():\n",
    "    print(f\"Checkpoints directory does not exist, creating it at: {checkpoints_dir}\")\n",
    "    checkpoints_dir.mkdir(exist_ok=True)\n",
    "print(f\"Checkpoints directory at: {checkpoints_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07943542",
   "metadata": {},
   "source": [
    "##### Get ESM Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d270e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to load ESM C with LM head enabled \n",
    "# expose final token embeddings before the logits head (is logits head the LM head, LM head=language modelling head)?\n",
    "# collator returns: input_ids, attention_mask\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# load model and allow lora (rather than eval mode?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe86febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from esm.models.esmc import ESMC\n",
    "# model = ESMC.from_pretrained(\"esmc_300m\").eval()\n",
    "\n",
    "# print([a for a in dir(model) if \"token\" in a.lower()])\n",
    "# # Common: 'tokenizer' shows up; then introspect it:\n",
    "# tok = getattr(model, \"tokenizer\", None)\n",
    "# print(type(tok))\n",
    "# print([a for a in dir(tok) if not a.startswith(\"_\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50c00556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use model's input embeddings to get the vocab \n",
    "# print(model.named_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7fc0d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(getattr(tok, \"vocab_size\", None))\n",
    "\n",
    "# get_vocab = getattr(tok, \"get_vocab\", None)\n",
    "# if callable(get_vocab):\n",
    "#     vocab = get_vocab()\n",
    "#     print(\"num tokens:\", len(vocab))\n",
    "#     print(\"sample tokens:\", list(vocab.keys())[:50])\n",
    "\n",
    "# # Special tokens (common names; guarded)\n",
    "# for name in [\"bos_token\", \"eos_token\", \"unk_token\", \"pad_token\", \"mask_token\"]:\n",
    "#     print(name, getattr(tok, name, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e8b5d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq = \"ARNDCQEGHILKMFPSTWYV<pad><mask><unk>\"\n",
    "# ids = tok.encode(seq) if hasattr(tok, \"encode\") else None\n",
    "# print(\"ids:\", ids)\n",
    "# if ids is not None and hasattr(tok, \"decode\"):\n",
    "#     print(\"decoded:\", tok.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25fb75e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load base protein language model (esm2 8M params, 6 layers) with specified PEFT methods. See the esm2 repo for more model size options\n",
    "\n",
    "# protein = ESMProtein(sequence=\"AACGTATTTA<unk>\")\n",
    "# model = ESMC.from_pretrained(\"esmc_300m\").to(\"cuda\") # or \"cpu\"\n",
    "# protein_tensor = model.encode(protein)\n",
    "# logits_output = model.logits(\n",
    "#    protein_tensor, LogitsConfig(sequence=True, return_embeddings=True)\n",
    "# )\n",
    "# print(logits_output.logits, logits_output.embeddings.size())\n",
    "# #protein_batch_converter = protein_LM_alphabet.get_batch_converter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17ffa245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size = [1,7,960], always the case?\n",
    "# size = [1, 12, 960]\n",
    "# size is I think batch number, sequence length, embedding dimension\n",
    "\n",
    "df = pd.read_csv('/home/natasha/multimodal_model/data/raw/HLA/boltz_100_runs.csv')\n",
    "# Fill empty/nan values with <unk> token\n",
    "df['TCRa'] = df['TCRa'].fillna('<unk>')\n",
    "df['TCRb'] = df['TCRb'].fillna('<unk>')\n",
    "\n",
    "# Replace empty strings with <unk>\n",
    "df.loc[df['TCRa'] == '', 'TCRa'] = '<unk>'\n",
    "df.loc[df['TCRb'] == '', 'TCRb'] = '<unk>'\n",
    "\n",
    "df['TCR_full'] = df['TCRa'] + df['TCRb']\n",
    "#df.to_csv('/home/natasha/multimodal_model/data/raw/HLA/boltz_100_runs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ca97902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hugging face dataset?\n",
    "class TCR_dataset(Dataset):\n",
    "    \"\"\"Dataset for TCR data, for use in encoder training to propagate through to NC model\"\"\"\n",
    "    def __init__(self, data_path, column_name='TCR_full', include_label=False):\n",
    "        self.data_path = data_path\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.column_name = column_name  # Store column name here\n",
    "        self.include_label = include_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):  # Remove column_name parameter\n",
    "        row = self.data.iloc[idx]  # Fix: self.data, not self.csv\n",
    "        protein = row[self.column_name]  # Use stored column name\n",
    "        protein_idx = f'TCR_{idx}'\n",
    "        if self.include_label:\n",
    "            return protein_idx, protein, row.get('Binding', -1)\n",
    "        #return protein_idx, protein\n",
    "        return protein\n",
    "\n",
    "class peptide_dataset(Dataset):\n",
    "    def __init__(self, data_path, column_name='Peptide', include_label=False):\n",
    "        self.data_path = data_path\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.column_name = column_name\n",
    "        self.include_label = include_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        peptide = row[self.column_name]\n",
    "        peptide_idx = f'peptide_{idx}'\n",
    "        if self.include_label:\n",
    "            return peptide_idx, peptide, row.get('Binding', -1)\n",
    "        #return peptide_idx, peptide\n",
    "        return peptide\n",
    "\n",
    "class HLA_dataset(Dataset):\n",
    "    def __init__(self, data_path, column_name='HLA', include_label=False):\n",
    "        self.data_path = data_path\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.column_name = column_name\n",
    "        self.include_label = include_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        hla = row[self.column_name]\n",
    "        hla_idx = f'hla_{idx}'\n",
    "        if self.include_label:\n",
    "            return hla_idx, hla, row.get('Binding', -1)\n",
    "        #return hla_idx, hla\n",
    "        return hla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b671bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcr = TCR_dataset(data_path='/home/natasha/multimodal_model/data/raw/HLA/boltz_100_runs.csv', column_name='TCR_full')\n",
    "peptide = peptide_dataset(data_path='/home/natasha/multimodal_model/data/raw/HLA/boltz_100_runs.csv', column_name='Peptide')\n",
    "hla = HLA_dataset(data_path='/home/natasha/multimodal_model/data/raw/HLA/boltz_100_runs.csv', column_name='HLA_sequence')\n",
    "\n",
    "# not sure I reallt need these classes??? Hmmmmmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94e53763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13e173c7ac646828a5a74613f5aafab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tcrs = [ESMProtein(sequence=s) for s in tcr.data['TCR_full']]\n",
    "peptides = [ESMProtein(sequence=s) for s in peptide.data['Peptide']]\n",
    "hlas = [ESMProtein(sequence=s) for s in hla.data['HLA_sequence']]\n",
    "\n",
    "# can batch at the forward step, not the encoding step\n",
    "\n",
    "model = ESMC.from_pretrained(\"esmc_300m\").to(device).eval()\n",
    "\n",
    "tcrs_data = [seq for seq in tcr]\n",
    "peptides_data = [seq for seq in peptide]\n",
    "hlas_data = [seq for seq in hla]\n",
    "\n",
    "encoded_tcrs = [model.encode(p) for p in tcrs]\n",
    "encoded_peptides = [model.encode(p) for p in peptides]\n",
    "encoded_hlas = [model.encode(p) for p in hlas]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b45d13e",
   "metadata": {},
   "source": [
    "##### Mask Data and Collate Data for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eaf8da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do for entire dataset\n",
    "# do we also want to output attention_mask from the tokenizer?\n",
    "\n",
    "tok = model.tokenizer\n",
    "CLS_ID = tok.cls_token_id\n",
    "EOS_ID = tok.eos_token_id\n",
    "PAD_ID = tok.pad_token_id\n",
    "MASK_ID = tok.mask_token_id\n",
    "\n",
    "AA_IDS =  [5,10,17,13,23,16,9,6,21,12,4,15,20,18,14,8,11,22,19,7]\n",
    "\n",
    "\n",
    "class EncodedSeqDataset(Dataset):\n",
    "    def __init__(self, sequences, enc):     # ← now takes two arguments\n",
    "        self.sequences = sequences          # list[str]\n",
    "        self.input_ids = enc['input_ids']\n",
    "        self.attention_mask = enc['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"sequence\": self.sequences[idx],  # raw sequence string\n",
    "            \"input_ids\": torch.as_tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            \"attention_mask\": torch.as_tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0865bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMProteinCollator:\n",
    "    def __init__(self, *, cls_id, eos_id, pad_id, mask_id, amino_acids,\n",
    "                 p=0.15, min_per_seq=2, max_per_seq=45, aa_frac=0.20):\n",
    "        self.CLS = cls_id\n",
    "        self.EOS = eos_id\n",
    "        self.PAD = pad_id\n",
    "        self.MASK = mask_id\n",
    "        self.aa = torch.as_tensor(amino_acids, dtype=torch.long)\n",
    "        self.p = p\n",
    "        self.min_per_seq = min_per_seq\n",
    "        self.max_per_seq = max_per_seq\n",
    "        self.aa_frac = aa_frac\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def mask_batch(self, input_ids, attention_mask):\n",
    "        device = input_ids.device\n",
    "        aa = self.aa.to(device)\n",
    "\n",
    "        B, L = input_ids.shape\n",
    "        valid_mask = attention_mask.bool() \\\n",
    "                   & (input_ids != self.PAD) \\\n",
    "                   & (input_ids != self.CLS) \\\n",
    "                   & (input_ids != self.EOS)\n",
    "\n",
    "        masked_input_ids = input_ids.clone()\n",
    "        labels = torch.full_like(input_ids, -100)\n",
    "\n",
    "        for i in range(B):\n",
    "            vmask = valid_mask[i]\n",
    "            if not vmask.any():\n",
    "                continue\n",
    "\n",
    "            valid_idx = vmask.nonzero(as_tuple=False).squeeze(1)  # (L_valid,)\n",
    "            L_valid = valid_idx.numel()\n",
    "\n",
    "            # how many to mask: floor(p*L_valid), clamped to [2, 45] but never > L_valid\n",
    "            n = torch.floor(self.p * torch.tensor(L_valid, device=device, dtype=torch.float32)).to(torch.int64)\n",
    "            n = torch.clamp(n, min=self.min_per_seq, max=min(self.max_per_seq, L_valid))\n",
    "            if n.item() == 0:\n",
    "                continue\n",
    "\n",
    "            # choose n distinct valid positions\n",
    "            chosen = valid_idx[torch.randperm(L_valid, device=device)[:n]]\n",
    "\n",
    "            # split into AA vs MASK; ensure >=1 AA if n>=2\n",
    "            n_amino = torch.floor(self.aa_frac * n).to(torch.int64)\n",
    "            if n.item() >= 2:\n",
    "                n_amino = torch.clamp(n_amino, min=1)\n",
    "            n_mask = n - n_amino\n",
    "\n",
    "            order = torch.randperm(n.item(), device=device)\n",
    "            mask_pos  = chosen[order[:n_mask]]\n",
    "            amino_pos = chosen[order[n_mask:]]\n",
    "\n",
    "            # labels only at supervised positions\n",
    "            labels[i, chosen] = input_ids[i, chosen]\n",
    "\n",
    "            # apply edits\n",
    "            if n_mask.item() > 0:\n",
    "                masked_input_ids[i, mask_pos] = self.MASK\n",
    "            if n_amino.item() > 0:\n",
    "                r_idx = torch.randint(high=aa.numel(), size=(n_amino.item(),), device=device)\n",
    "                masked_input_ids[i, amino_pos] = aa[r_idx]\n",
    "\n",
    "        return masked_input_ids, labels\n",
    "\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_ids = torch.stack([f[\"input_ids\"] for f in features], dim=0)\n",
    "        attention_mask = torch.stack([f[\"attention_mask\"] for f in features], dim=0)\n",
    "        sequences = [f[\"sequence\"] for f in features]\n",
    "        proteins = [ESMProtein(sequence=f[\"sequence\"]) for f in features]\n",
    "        batched_clean = _BatchedESMProteinTensor(sequence=input_ids)\n",
    "\n",
    "\n",
    "        masked_input_ids, labels = self.mask_batch(input_ids, attention_mask)\n",
    "\n",
    "        # build masked sequences as strings (keep <mask>, drop CLS/EOS/PAD)\n",
    "        masked_sequences = []\n",
    "        for row in masked_input_ids.tolist():\n",
    "            toks = collator.tokenizer.convert_ids_to_tokens(row, skip_special_tokens=False)\n",
    "            aa = []\n",
    "            for t in toks:\n",
    "                if t in (collator.tokenizer.cls_token, collator.tokenizer.eos_token, collator.tokenizer.pad_token):\n",
    "                    continue\n",
    "                aa.append(t)  # AA tokens are single letters; keep \"<mask>\" as is\n",
    "            masked_sequences.append(\"\".join(aa))\n",
    "\n",
    "        proteins_masked = [ESMProtein(sequence=s) for s in masked_sequences]\n",
    "        batched_masked = _BatchedESMProteinTensor(sequence=masked_input_ids)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"masked_input_ids\": masked_input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"clean_input_ids\": input_ids.clone(),\n",
    "            \"clean_sequences\": sequences,                 # clean strings\n",
    "            \"masked_sequences\": masked_sequences,   # masked strings  ← NEW\n",
    "            \"clean_sequences_ESMprotein\": proteins,\n",
    "            \"masked_sequences_ESMprotein\": proteins_masked,\n",
    "            \"masked_input_ids_ESMprotein_batched\": batched_masked,\n",
    "            \"clean_input_ids_ESMprotein_batched\": batched_clean,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146109ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your existing BatchEncodings:\n",
    "# clean_tcrs_tokenized, clean_peptides_tokenized, clean_hlas_tokenized\n",
    "clean_tcrs_tokenized = model.tokenizer(tcrs_data, return_tensors='pt', padding=True)\n",
    "clean_peptides_tokenized = model.tokenizer(peptides_data, return_tensors='pt', padding=True)\n",
    "clean_hlas_tokenized = model.tokenizer(hlas_data, return_tensors='pt', padding=True)\n",
    "\n",
    "tcr_ds = EncodedSeqDataset(tcrs_data,clean_tcrs_tokenized)\n",
    "pep_ds = EncodedSeqDataset(peptides_data, clean_peptides_tokenized)\n",
    "hla_ds = EncodedSeqDataset(hlas_data, clean_hlas_tokenized)\n",
    "\n",
    "collator = MLMProteinCollator(\n",
    "    cls_id=CLS_ID, eos_id=EOS_ID, pad_id=PAD_ID, mask_id=MASK_ID,\n",
    "    amino_acids=AA_IDS, p=0.15, min_per_seq=2, max_per_seq=45, aa_frac=0.20\n",
    ")\n",
    "collator.tokenizer = model.tokenizer\n",
    "\n",
    "\n",
    "tcr_loader = DataLoader(tcr_ds, batch_size=8, shuffle=True, num_workers=4, collate_fn=collator)\n",
    "pep_loader = DataLoader(pep_ds, batch_size=8, shuffle=True, num_workers=4, collate_fn=collator)\n",
    "hla_loader = DataLoader(hla_ds, batch_size=8, shuffle=True, num_workers=4, collate_fn=collator)\n",
    "\n",
    "\n",
    "# gives a batch dict from the collator with 4 keys\n",
    "# input_ids, labels (original tokens only at masked positions, -100 everywhere else)\n",
    "# attention_mask (0,1 for padding), clean_input_ids (clean copy of the input for clean forward pass using boltz for NC loss)\n",
    "\n",
    "model.to(\"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebfb3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define LoRA model\n",
    "\n",
    "use_amp = torch.cuda.is_available()\n",
    "\n",
    "base = ESMC.from_pretrained(\"esmc_300m\").to(device)\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "    target_modules=[\"out_proj\", \"layernorm_qkv.1\"],  # inner Linear inside the Sequential\n",
    ")\n",
    "\n",
    "model_tcr = LoraModel(base, lora_cfg, adapter_name=\"tcr\")\n",
    "\n",
    "# freeze everything; unfreeze only LoRA params\n",
    "for p in model_tcr.parameters():\n",
    "    p.requires_grad = False\n",
    "for name, p in model_tcr.named_parameters():\n",
    "    if \"lora_A\" in name or \"lora_B\" in name:\n",
    "        p.requires_grad = True\n",
    "\n",
    "model_tcr.train()\n",
    "\n",
    "optim_tcr = torch.optim.AdamW(\n",
    "    (p for p in model_tcr.parameters() if p.requires_grad),\n",
    "    lr=1e-3, weight_decay=0.01\n",
    ")\n",
    "#scaler = GradScaler(enabled=use_amp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d9497b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2904721/3875696438.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler  = GradScaler(enabled=False)                     # <-- no scaler\n"
     ]
    }
   ],
   "source": [
    "from torch.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler  = GradScaler(enabled=False)                     # <-- no scaler\n",
    "\n",
    "for batch in tcr_loader:\n",
    "    input_ids = batch[\"masked_input_ids\"].to(device, dtype=torch.long)\n",
    "    labels    = batch[\"labels\"].to(device)\n",
    "\n",
    "    with autocast(\"cuda\", enabled=use_amp, dtype=torch.bfloat16):  # <-- bf16\n",
    "        out    = model_tcr(input_ids)\n",
    "        logits = out.sequence_logits\n",
    "        #print(type(logits), logits.shape, logits.requires_grad)  # expect: Tensor, [B,L,V], True\n",
    "\n",
    "        loss   = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            labels.view(-1),\n",
    "            ignore_index=-100\n",
    "        )\n",
    "\n",
    "    loss.backward()                                     # <-- standard backward\n",
    "    optim_tcr.step(); optim_tcr.zero_grad(set_to_none=True)\n",
    "\n",
    "\n",
    "# 1.1 seconds for 100\n",
    "# 11 for 1000\n",
    "# 110 for 10000\n",
    "# 350 for 35000 - 5.8 minutes to train one encoder on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eab3529d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to tcr_encoder_checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "# save model and offload to CPU\n",
    "\n",
    "# save tcr model\n",
    "\n",
    "checkpoint_filename = 'tcr_encoder_checkpoint.pth'\n",
    "\n",
    "checkpoint_dict = {\n",
    "    #'epoch': num_epochs,\n",
    "    'tcr_model_state_dict': model_tcr.state_dict(),\n",
    "    #'pep_model_state_dict': peptide_model_nc.state_dict(), \n",
    "    'optimizer_state_dict': optim_tcr.state_dict(),\n",
    "    #'final_loss': avg_epoch_loss,\n",
    "    #'binding_threshold': threshold  # Add threshold to saved state\n",
    "}\n",
    "\n",
    "torch.save(checkpoint_dict, checkpoints_dir/checkpoint_filename)\n",
    "\n",
    "print(f\"Checkpoint saved to {checkpoint_filename}\")\n",
    "\n",
    "model_tcr.to(\"cpu\")\n",
    "torch.cuda.empty_cache()  # Free up GPU memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd0beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# peptide encoder\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337c46c0",
   "metadata": {},
   "source": [
    "1. Model selection & freeze policy\n",
    "- Load ESM-C base.\n",
    "- Freeze embeddings + lower N blocks; leave top K blocks trainable (or use adapters/LoRA). Record N, K.\n",
    "2. Two forwards you will run\n",
    "- Masked forward (for loss): feed masked_sequences → get logits over vocab (B, L, V).\n",
    "- Clean forward (for caching embeddings): feed sequences → get token_hidden_states (B, L, d_model).\n",
    "3. Loss and metrics\n",
    "- Compute token-level cross-entropy on logits vs labels (ignore_index = −100).\n",
    "- Track masked-token accuracy and perplexity per modality.\n",
    "4. Pooling you will later reuse\n",
    "- From token_hidden_states (clean forward), compute a pooled sequence vector per item using a masked mean over non-special tokens → (B, d_model).\n",
    "- Save both: token states (for optional analysis) and pooled vectors (for projection/alignment).\n",
    "5. Optimisation\n",
    "- AdamW; no weight decay on LayerNorm/bias; gradient clip ~1.0; fp16/bf16 if available.\n",
    "- LR schedule: warm-up then cosine/linear decay.\n",
    "- Validation sets per modality with identical masking policy.\n",
    "6. Checkpoints\n",
    "- Save model weights + adapter/LoRA if used, optimizer, scheduler, tokenizer hash, mask rate, freeze policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad95af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900fb2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f0263053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before model loading:\n",
      "\n",
      "GPU Memory Usage:\n",
      "Allocated: 1168.29 MB\n",
      "Cached: 1272.00 MB\n",
      "Total GPU Memory: 16376.00 MB\n",
      "Available: 15104.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Check GPU memory\n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\nGPU Memory Usage:\")\n",
    "        print(f\"Allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "        print(f\"Cached: {torch.cuda.memory_reserved()/1024**2:.2f} MB\")\n",
    "        \n",
    "        # Get total GPU memory\n",
    "        import subprocess\n",
    "        result = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.total', '--format=csv,nounits,noheader'])\n",
    "        total_memory = float(result.decode('utf-8').strip())\n",
    "        print(f\"Total GPU Memory: {total_memory:.2f} MB\")\n",
    "        print(f\"Available: {total_memory - torch.cuda.memory_reserved()/1024**2:.2f} MB\")\n",
    "\n",
    "# Check memory before model loading\n",
    "print(\"Before model loading:\")\n",
    "print_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf91b7",
   "metadata": {},
   "source": [
    "##### Get Boltz Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cca6b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Boltz embeddings\n",
    "file_path = '/home/natasha/multimodal_model/outputs/boltz_runs/positives/pair_000/boltz_results_pair_000/predictions/pair_000/embeddings_pair_000.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f77032b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s', 'z']\n",
      "(1, 604, 384)\n",
      "(1, 604, 604, 128)\n",
      "[[[[ -88.07086     -52.60885     108.844284   ...  190.84561\n",
      "      36.47628    -110.61696   ]\n",
      "   [ -57.74353     -54.678726    -54.20086    ...  -43.508102\n",
      "       5.2886047   -22.915913  ]\n",
      "   [  -5.9032555   -26.453156     -5.7282715  ...  -50.97639\n",
      "      46.78975     -54.079994  ]\n",
      "   ...\n",
      "   [  -7.059223     20.620789      9.03931    ...  -15.813938\n",
      "      20.766754    -12.156269  ]\n",
      "   [  -4.7150135    15.798729      8.588043   ...  -16.762444\n",
      "      14.437408     -7.1301365 ]\n",
      "   [ -21.511803     37.621155     17.605013   ...   14.496872\n",
      "      17.363136     -8.831755  ]]\n",
      "\n",
      "  [[ -35.09397       7.525936     71.0813     ...   47.07572\n",
      "      20.585098     46.14451   ]\n",
      "   [ -31.209412     -7.549141     64.2861     ...   86.585236\n",
      "      -2.7985687  -130.94954   ]\n",
      "   [ -10.455566     -6.6848755    30.168354   ...   14.787491\n",
      "      17.332607    -59.565666  ]\n",
      "   ...\n",
      "   [   0.5986786    -1.5232773    -2.8792143  ...   -5.399969\n",
      "      14.654137    -17.36309   ]\n",
      "   [   3.5514011     3.6222076    -1.7381783  ...   -9.660883\n",
      "      11.036015    -15.285324  ]\n",
      "   [  -6.451168     19.231964     -9.6842575  ...  -16.713417\n",
      "      22.679386    -15.390038  ]]\n",
      "\n",
      "  [[  -4.183182     18.095776     31.155787   ...    1.8823948\n",
      "     -10.666845     32.82949   ]\n",
      "   [   8.762455      3.9998856    92.092415   ...   40.20427\n",
      "     -31.933193     63.093636  ]\n",
      "   [  18.744068    -10.173111     -7.6059     ...   12.93276\n",
      "      47.68093     -86.15998   ]\n",
      "   ...\n",
      "   [   2.5435104    -0.6375122     3.3156757  ...    4.4594154\n",
      "       7.1347427   -20.035213  ]\n",
      "   [   2.2527695     1.8355942     0.71240616 ...   -1.6064396\n",
      "       3.044733    -16.235222  ]\n",
      "   [  -2.4156704    15.234161     -3.0182457  ...  -10.692051\n",
      "      13.672258    -18.697435  ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ -10.618769      6.3300858    -2.0357761  ...  -25.738693\n",
      "       1.3816185   -21.03988   ]\n",
      "   [  22.322556      2.907875      3.7629757  ...  -35.693233\n",
      "       7.501652    -14.499756  ]\n",
      "   [   9.398445     -0.73744583    1.6039333  ...  -16.67471\n",
      "      -0.2987976    -5.7997646 ]\n",
      "   ...\n",
      "   [  62.22261      15.584785     22.177864   ...   19.858303\n",
      "       9.700764    -44.796318  ]\n",
      "   [  20.971699    -19.182281     25.623428   ...    4.197109\n",
      "     -28.488178     -5.0478573 ]\n",
      "   [  -4.5326843    17.996338     16.738602   ...    5.3960323\n",
      "     -21.068195     -9.235603  ]]\n",
      "\n",
      "  [[  -8.581436      1.0289612     7.363783   ...  -29.210743\n",
      "       8.203732    -24.788233  ]\n",
      "   [  21.394268     -0.44820404   14.176565   ...  -33.418686\n",
      "       7.5276947   -17.125202  ]\n",
      "   [   4.7290955    -3.7472992     6.7510567  ...  -19.456345\n",
      "       4.0339527    -9.591824  ]\n",
      "   ...\n",
      "   [  17.308985     -5.4827785    56.10473    ...   45.01352\n",
      "     -30.126228     33.470757  ]\n",
      "   [  39.02382     -13.856113    -25.422417   ...   38.45839\n",
      "       7.2366104   -51.851883  ]\n",
      "   [  24.037872     -1.37117      14.813293   ...    4.685913\n",
      "     -40.81143      15.620903  ]]\n",
      "\n",
      "  [[ -16.29834      21.513409      2.277874   ...    0.7921219\n",
      "      13.70676     -16.13976   ]\n",
      "   [  14.243572      7.6536217    11.241619   ...  -46.023537\n",
      "      17.750835    -13.09779   ]\n",
      "   [   2.9636593    -0.5283241     8.876747   ...  -31.474586\n",
      "       4.863846     -8.9659    ]\n",
      "   ...\n",
      "   [   4.770996    -19.04021      31.73148    ...   24.754692\n",
      "       0.5246353    28.99849   ]\n",
      "   [ -17.289707      1.5685883    44.81977    ...   61.652763\n",
      "     -13.438297     50.15661   ]\n",
      "   [ -36.63765     -79.430664     34.72692    ...  123.28517\n",
      "     -57.991817   -137.73175   ]]]]\n"
     ]
    }
   ],
   "source": [
    "# shape of s is the sum of the lengths of the TCR, HLA and peptide [batch, sum_of_len, dim (384)]\n",
    "# shape of z is the length of the TCR, HLA and peptide twice [batch, sum_of_len, sum_of_len, dim (128)]\n",
    "\n",
    "with np.load(file_path) as data:\n",
    "    print(list(data.keys()))\n",
    "    print(data['s'].shape)\n",
    "    print(data['z'].shape)\n",
    "    s = data['s']\n",
    "    z = data['z']\n",
    "\n",
    "print(z)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafcf2cf",
   "metadata": {},
   "source": [
    "##### Old/Unused Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1754784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not necessary, ESM C has their own much more complex attention layers\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, num_heads=4):\n",
    "        \"\"\"\n",
    "        hidden_dim: Dimensionality of the input\n",
    "        num_heads: Number of attention heads to split the input into\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n",
    "        self.Wv = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Value part\n",
    "        self.Wk = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Key part\n",
    "        self.Wq = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Query part\n",
    "        self.Wo = nn.Linear(hidden_dim, hidden_dim, bias=False) # the output part\n",
    "\n",
    "    def check_sdpa_inputs(self, x):\n",
    "        assert x.size(1) == self.num_heads, f\"Expected shape (-1, {self.num_heads}, -1, {self.hidden_dim // self.num_heads}), got {tuple(x.size())}\"\n",
    "        assert x.size(3) == self.hidden_dim // self.num_heads\n",
    "\n",
    "    def scaled_dot_product_attention(self,\n",
    "            query,\n",
    "            key,\n",
    "            value,\n",
    "            attention_mask=None,\n",
    "            key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        query: tensor of shape (batch_size, num_heads, query_sequence_length, hidden_dim // num_heads)\n",
    "        key: tensor of shape (batch_size, num_heads, key_sequence_length, hidden_dim // num_heads)\n",
    "        value: tensor of shape (batch_size, num_heads, value_sequence_length, hidden_dim // num_heads)\n",
    "        attention_mask: tensor of shape (query_sequence_length, key_sequence_length)\n",
    "        key_padding_mask: tensor of shape (sequence_length, key_sequence_length)\n",
    "        \"\"\"\n",
    "        self.check_sdpa_inputs(query)\n",
    "        self.check_sdpa_inputs(key)\n",
    "        self.check_sdpa_inputs(value)\n",
    "\n",
    "        d_k = query.size(-1) #size of the last dimension of the query tensor\n",
    "        tgt_len, src_len = query.size(-2), key.size(-2)\n",
    "        #logits = (B, H, tgt_len, E) * (B, H, E, src_len) = (B, H, tgt_len, src_len)\n",
    "        logits = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        \n",
    "        # Attention mask\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.dim() == 2:\n",
    "                assert attention_mask.size() == (tgt_len, src_len)\n",
    "                attention_mask = attention_mask.unsqueeze(0)\n",
    "                logits = logits + attention_mask\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid size of attention_mask: {attention_mask.size()}\")\n",
    "\n",
    "        # Key padding mask\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2) # Broadcast over batch size, num heads\n",
    "            logits = logits + key_padding_mask\n",
    "\n",
    "        attention = torch.softmax(logits, dim=-1)\n",
    "        output = torch.matmul(attention, value) # (batch_size, num_heads, sequence_length, hidden_dim)\n",
    "\n",
    "        return output, attention\n",
    "\n",
    "    def split_into_heads(self, x, num_heads):\n",
    "        batch_size, seq_length, hidden_dim = x.size()\n",
    "        x = x.view(batch_size, seq_length, num_heads, hidden_dim // num_heads)\n",
    "\n",
    "        return x.transpose(1, 2) # (batch_size, num_heads, seq_length, hidden_dim // num_heads)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, num_heads, seq_length, head_hidden_dim = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, num_heads * head_hidden_dim)\n",
    "    \n",
    "    def forward(\n",
    "            self,\n",
    "            q,\n",
    "            k,\n",
    "            v,\n",
    "            attention_mask=None,\n",
    "            key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        q: tensor of shape (batch_size, query_sequence_length, hidden_dim)\n",
    "        k: tensor of shape (batch_size, key_sequence_length, hidden_dim)\n",
    "        v: tensor of shape (batch_size, value_sequence_length, hidden_dim)\n",
    "        attention_mask: tensor of shape (query_sequence_length, key_sequence_length)\n",
    "        key_padding_mask: tensor of shape (sequence_length, key_sequence_length)\n",
    "        \"\"\"\n",
    "        q = self.Wq(q)\n",
    "        k = self.Wk(k)\n",
    "        v = self.Wv(v)\n",
    "\n",
    "        q = self.split_into_heads(q, self.num_heads) # (batch_size, num_heads, query_sequence_length, hidden_dim // num_heads)\n",
    "        k = self.split_into_heads(k, self.num_heads) # (batch_size, num_heads, key_sequence_length, hidden_dim // num_heads)\n",
    "        v = self.split_into_heads(v, self.num_heads) # (batch_size, num_heads, value_sequence_length, hidden_dim // num_heads)\n",
    "        \n",
    "        attn_values, attn_weights = self.scaled_dot_product_attention(\n",
    "            query=q,\n",
    "            key=k,\n",
    "            value=v,\n",
    "            attention_mask=attention_mask,\n",
    "            key_padding_mask=key_padding_mask\n",
    "        )\n",
    "        grouped = self.combine_heads(attn_values)\n",
    "        output = self.Wo(grouped)\n",
    "\n",
    "        self.attention_weights = attn_weights\n",
    "\n",
    "        return output\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49d4881",
   "metadata": {},
   "outputs": [],
   "source": [
    "### masking for one sequence\n",
    "\n",
    "# # tokenize sequences\n",
    "# clean_tcrs_tokenized = model.tokenizer(tcrs_data, return_tensors='pt', padding=True)\n",
    "# clean_peptides_tokenized = model.tokenizer(peptides_data, return_tensors='pt', padding=True)\n",
    "# clean_hlas_tokenized = model.tokenizer(hlas_data, return_tensors='pt', padding=True)\n",
    "\n",
    "# # clean_tcrs_tokenized.keys()\n",
    "# # ids = clean_tcrs_tokenized['input_ids']\n",
    "# # attention_mask = clean_tcrs_tokenized['attention_mask']\n",
    "\n",
    "# #copies for masking\n",
    "# masked_tcrs_tokenized = clean_tcrs_tokenized.copy()\n",
    "# masked_peptides_tokenized = clean_peptides_tokenized.copy()\n",
    "# masked_hlas_tokenized = clean_hlas_tokenized.copy()\n",
    "\n",
    "# # special tokens and amino acids\n",
    "# CLS = model.tokenizer.cls_token_id\n",
    "# EOS = model.tokenizer.eos_token_id\n",
    "# PAD = model.tokenizer.pad_token_id\n",
    "# amino_acids = torch.tensor([5, 10, 17, 13, 23, 16, 9, 6, 21, 12, 4, 15, 20, 18, 14, 8, 11, 22, 19, 7], device=ids.device)\n",
    "# mask_token = 32\n",
    "\n",
    "# # beginning of per sequence masking\n",
    "# # IDS\n",
    "# ids = masked_tcrs_tokenized['input_ids'][0]\n",
    "# valid_mask = (ids != CLS) & (ids != EOS) & (ids != PAD)\n",
    "# valid_idx = valid_mask.nonzero(as_tuple=False).squeeze(1)\n",
    "\n",
    "# #L_valid = valid_mask.sum()\n",
    "# L_valid = valid_idx.numel()\n",
    "# if L_valid == 0:\n",
    "#     # nothing to mask, make labels -100 to be ignored in the loss\n",
    "#     masked_ids = ids.clone()\n",
    "#     labels = torch.full_like(ids, -100)\n",
    "# else:\n",
    "#     # L_valid = valid_mask.sum(dim=1) # for batch, but currently only 1 dimension\n",
    "#     n = torch.floor(0.15 * torch.tensor(L_valid, device=ids.device, dtype=torch.float32)).to(torch.int64)\n",
    "#     n = n.clamp(min=2, max=min(45, L_valid))\n",
    "#     # n = torch.floor(\n",
    "#     #         0.15 * torch.tensor(L_valid, device=ids.device, dtype=torch.float32)\n",
    "#     #         ).to(torch.int64).clamp(min=2, max=45)\n",
    "    \n",
    "#     # choose n distinct valid positions \n",
    "#     # reshuffle\n",
    "#     perm = torch.randperm(L_valid, device=ids.device)\n",
    "#     # choose first n\n",
    "#     chosen_local = perm[:n]\n",
    "#     # get correct indices based off of valid indices\n",
    "#     chosen = valid_idx[chosen_local]\n",
    "\n",
    "#     # 4) split into amino acids and mask tokens\n",
    "#     n_amino = torch.floor(0.2 * n).to(torch.int64).clamp(min=1)\n",
    "#     n_mask = n - n_amino\n",
    "\n",
    "#     # 5) shuffle and create positions\n",
    "#     # need to shuffle as otherwise will always have the first part being masked with the mask token\n",
    "#     # and the same goes for the amino acids, always masked at the second part of the sequence\n",
    "#     shuffle   = torch.randperm(n.item(), device=ids.device)\n",
    "#     mask_pos  = chosen[shuffle[:n_mask]]\n",
    "#     amino_pos = chosen[shuffle[n_mask:]]\n",
    "\n",
    "#     # 6) build labels - originals at selected positions, -100 for non-masked positions\n",
    "#     masked_ids = ids.clone()\n",
    "#     labels     = torch.full_like(ids, -100)\n",
    "#     labels[chosen] = ids[chosen]\n",
    "\n",
    "#     # 7) apply mask\n",
    "#     masked_ids[mask_pos] = mask_token\n",
    "\n",
    "#     # 8) assign amino acids masks from random amino acids\n",
    "#     if n_amino > 0:\n",
    "#         rand_idx = torch.randint(high=amino_acids.numel(), size=(n_amino.item(),), device=ids.device)\n",
    "#         masked_ids[amino_pos] = amino_acids[rand_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975843ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # forward pass\n",
    "\n",
    "# # for batch in tcr_loader:\n",
    "# #     batched = batch[\"masked_input_ids_ESMprotein_batched\"]\n",
    "# #     batched = batched.to(device)\n",
    "# #     labels = batch['labels'].to(device)\n",
    "\n",
    "# #     logits_output = model_tcr.logits(\n",
    "# #         batched, LogitsConfig(sequence=True, return_embeddings=True)\n",
    "# #         )\n",
    "    \n",
    "# #     logits_s = logits_output.logits.sequence\n",
    "# #     loss = F.cross_entropy(\n",
    "# #         logits_s.view(-1, logits_s.size(-1)), # [B*L, V]\n",
    "# #         labels.view(-1),                      # [B*L]\n",
    "# #         ignore_index=-100\n",
    "# #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f28783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to print the modules within linear layers to find what to add Lora to\n",
    "\n",
    "for name, mod in base.named_modules():\n",
    "    if isinstance(mod, torch.nn.Linear):\n",
    "        print(name, mod.weight.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcr-multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
