{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e2b02a3",
   "metadata": {},
   "source": [
    "##### This script defines the encoders for the multimodal model\n",
    "##### Overview:\n",
    "- The inputs are the ESMC and Boltz embeddings \n",
    "- 3 encoders from PEFT of ESMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91aae1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/natasha/miniconda3/envs/tcr-multimodal/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "# Streamlined imports - removing duplicates\n",
    "import esm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import subprocess\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, update_display\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ESM imports\n",
    "from esm.models.esmc import ESMC\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig, ESMProteinTensor\n",
    "from esm.models.esmc import _BatchedESMProteinTensor\n",
    "\n",
    "# Tokenizer imports\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import decoders\n",
    "\n",
    "# PEFT imports\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from peft.tuners.lora import LoraConfig, LoraModel\n",
    "\n",
    "# Set environment variable\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31bb77ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/natasha/multimodal_model/scripts/train\n",
      "Project root: /home/natasha/multimodal_model\n",
      "Models directory at: /home/natasha/multimodal_model/models\n",
      "Checkpoints directory at: /home/natasha/multimodal_model/models/checkpoints\n"
     ]
    }
   ],
   "source": [
    "# Get current working directory and create models folder\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "# Get the project root (go up one level from scripts/train)\n",
    "project_root = Path(current_dir).parent.parent\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Create models directory in project root\n",
    "models_dir = project_root / \"models\"\n",
    "if not models_dir.exists():\n",
    "    print(f\"Models directory does not exist, creating it at: {models_dir}\")\n",
    "    models_dir.mkdir(exist_ok=True)\n",
    "print(f\"Models directory at: {models_dir}\")\n",
    "\n",
    "# Also create a checkpoints subdirectory for saving model checkpoints\n",
    "checkpoints_dir = models_dir / \"checkpoints\"\n",
    "if not checkpoints_dir.exists():\n",
    "    print(f\"Checkpoints directory does not exist, creating it at: {checkpoints_dir}\")\n",
    "    checkpoints_dir.mkdir(exist_ok=True)\n",
    "print(f\"Checkpoints directory at: {checkpoints_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07943542",
   "metadata": {},
   "source": [
    "##### Get ESM Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d270e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to load ESM C with LM head enabled \n",
    "# expose final token embeddings before the logits head (is logits head the LM head, LM head=language modelling head)?\n",
    "# collator returns: input_ids, attention_mask\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# load model and allow lora (rather than eval mode?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17ffa245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size = [1,7,960], always the case?\n",
    "# size = [1, 12, 960]\n",
    "# size is I think batch number, sequence length, embedding dimension\n",
    "\n",
    "df = pd.read_csv('/home/natasha/multimodal_model/data/raw/HLA/boltz_100_runs.csv')\n",
    "# Fill empty/nan values with <unk> token\n",
    "df['TCRa'] = df['TCRa'].fillna('X')\n",
    "df['TCRb'] = df['TCRb'].fillna('X')\n",
    "\n",
    "# Replace empty strings with <unk>\n",
    "df.loc[df['TCRa'] == '', 'TCRa'] = 'X'\n",
    "df.loc[df['TCRb'] == '', 'TCRb'] = 'X'\n",
    "\n",
    "df['TCR_full'] = df['TCRa'] + df['TCRb']\n",
    "df['m_alpha'] = 1\n",
    "df['m_beta'] = 1\n",
    "df.loc[df['TCRa'] == 'X', 'm_alpha'] = 0\n",
    "df.loc[df['TCRb'] == 'X', 'm_beta'] = 0\n",
    "#df.to_csv('/home/natasha/multimodal_model/data/raw/HLA/boltz_100_runs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ca97902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hugging face dataset?\n",
    "class TCR_dataset(Dataset):\n",
    "    \"\"\"Dataset for TCR data, for use in encoder training to propagate through to NC model\"\"\"\n",
    "    def __init__(self, data_path, column_name='TCR_full', include_label=False):\n",
    "        self.data_path = data_path\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.column_name = column_name  # Store column name here\n",
    "        self.include_label = include_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):  # Remove column_name parameter\n",
    "        row = self.data.iloc[idx]  # Fix: self.data, not self.csv\n",
    "        protein = row[self.column_name]  # Use stored column name\n",
    "        protein_idx = f'TCR_{idx}'\n",
    "        if self.include_label:\n",
    "            return protein_idx, protein, row.get('Binding', -1)\n",
    "        #return protein_idx, protein\n",
    "        return protein\n",
    "\n",
    "class peptide_dataset(Dataset):\n",
    "    def __init__(self, data_path, column_name='Peptide', include_label=False):\n",
    "        self.data_path = data_path\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.column_name = column_name\n",
    "        self.include_label = include_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        peptide = row[self.column_name]\n",
    "        peptide_idx = f'peptide_{idx}'\n",
    "        if self.include_label:\n",
    "            return peptide_idx, peptide, row.get('Binding', -1)\n",
    "        #return peptide_idx, peptide\n",
    "        return peptide\n",
    "\n",
    "class HLA_dataset(Dataset):\n",
    "    def __init__(self, data_path, column_name='HLA', include_label=False):\n",
    "        self.data_path = data_path\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.column_name = column_name\n",
    "        self.include_label = include_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        hla = row[self.column_name]\n",
    "        hla_idx = f'hla_{idx}'\n",
    "        if self.include_label:\n",
    "            return hla_idx, hla, row.get('Binding', -1)\n",
    "        #return hla_idx, hla\n",
    "        return hla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b671bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcr = TCR_dataset(data_path='/home/natasha/multimodal_model/data/raw/HLA/boltz_100_runs.csv', column_name='TCR_full')\n",
    "peptide = peptide_dataset(data_path='/home/natasha/multimodal_model/data/raw/HLA/boltz_100_runs.csv', column_name='Peptide')\n",
    "hla = HLA_dataset(data_path='/home/natasha/multimodal_model/data/raw/HLA/boltz_100_runs.csv', column_name='HLA_sequence')\n",
    "\n",
    "# not sure I reallt need these classes??? Hmmmmmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94e53763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8095b0ba9e4e3f878280aabcc87e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tcrs = [ESMProtein(sequence=s) for s in tcr.data['TCR_full']]\n",
    "peptides = [ESMProtein(sequence=s) for s in peptide.data['Peptide']]\n",
    "hlas = [ESMProtein(sequence=s) for s in hla.data['HLA_sequence']]\n",
    "\n",
    "# can batch at the forward step, not the encoding step\n",
    "\n",
    "#model = ESMC.from_pretrained(\"esmc_300m\").to(device).eval()\n",
    "model = ESMC.from_pretrained(\"esmc_300m\").eval()\n",
    "\n",
    "tcrs_data = [seq for seq in tcr]\n",
    "peptides_data = [seq for seq in peptide]\n",
    "hlas_data = [seq for seq in hla]\n",
    "\n",
    "encoded_tcrs = [model.encode(p) for p in tcrs]\n",
    "encoded_peptides = [model.encode(p) for p in peptides]\n",
    "encoded_hlas = [model.encode(p) for p in hlas]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b45d13e",
   "metadata": {},
   "source": [
    "##### Mask Data and Collate for MLM for Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eaf8da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do for entire dataset\n",
    "# do we also want to output attention_mask from the tokenizer?\n",
    "\n",
    "tok = model.tokenizer\n",
    "CLS_ID = tok.cls_token_id\n",
    "EOS_ID = tok.eos_token_id\n",
    "PAD_ID = tok.pad_token_id\n",
    "MASK_ID = tok.mask_token_id\n",
    "\n",
    "AA_IDS =  [5,10,17,13,23,16,9,6,21,12,4,15,20,18,14,8,11,22,19,7]\n",
    "\n",
    "\n",
    "class EncodedSeqDataset(Dataset):\n",
    "    def __init__(self, sequences, enc):     # ← now takes two arguments\n",
    "        self.sequences = sequences          # list[str]\n",
    "        self.input_ids = enc['input_ids']\n",
    "        self.attention_mask = enc['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"sequence\": self.sequences[idx],  # raw sequence string\n",
    "            \"input_ids\": torch.as_tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            \"attention_mask\": torch.as_tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0865bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMProteinCollator:\n",
    "    def __init__(self, *, cls_id, eos_id, pad_id, mask_id, amino_acids,\n",
    "                 p=0.15, min_per_seq=2, max_per_seq=45, aa_frac=0.20):\n",
    "        self.CLS = cls_id\n",
    "        self.EOS = eos_id\n",
    "        self.PAD = pad_id\n",
    "        self.MASK = mask_id\n",
    "        self.aa = torch.as_tensor(amino_acids, dtype=torch.long)\n",
    "        self.p = p\n",
    "        self.min_per_seq = min_per_seq\n",
    "        self.max_per_seq = max_per_seq\n",
    "        self.aa_frac = aa_frac\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def mask_batch(self, input_ids, attention_mask):\n",
    "        device = input_ids.device\n",
    "        aa = self.aa.to(device)\n",
    "\n",
    "        B, L = input_ids.shape\n",
    "        valid_mask = attention_mask.bool() \\\n",
    "                   & (input_ids != self.PAD) \\\n",
    "                   & (input_ids != self.CLS) \\\n",
    "                   & (input_ids != self.EOS)\n",
    "\n",
    "        masked_input_ids = input_ids.clone()\n",
    "        labels = torch.full_like(input_ids, -100)\n",
    "\n",
    "        for i in range(B):\n",
    "            vmask = valid_mask[i]\n",
    "            if not vmask.any():\n",
    "                continue\n",
    "\n",
    "            valid_idx = vmask.nonzero(as_tuple=False).squeeze(1)  # (L_valid,)\n",
    "            L_valid = valid_idx.numel()\n",
    "\n",
    "            # how many to mask: floor(p*L_valid), clamped to [2, 45] but never > L_valid\n",
    "            n = torch.floor(self.p * torch.tensor(L_valid, device=device, dtype=torch.float32)).to(torch.int64)\n",
    "            n = torch.clamp(n, min=self.min_per_seq, max=min(self.max_per_seq, L_valid))\n",
    "            if n.item() == 0:\n",
    "                continue\n",
    "\n",
    "            # choose n distinct valid positions\n",
    "            chosen = valid_idx[torch.randperm(L_valid, device=device)[:n]]\n",
    "\n",
    "            # split into AA vs MASK; ensure >=1 AA if n>=2\n",
    "            n_amino = torch.floor(self.aa_frac * n).to(torch.int64)\n",
    "            if n.item() >= 2:\n",
    "                n_amino = torch.clamp(n_amino, min=1)\n",
    "            n_mask = n - n_amino\n",
    "\n",
    "            order = torch.randperm(n.item(), device=device)\n",
    "            mask_pos  = chosen[order[:n_mask]]\n",
    "            amino_pos = chosen[order[n_mask:]]\n",
    "\n",
    "            # labels only at supervised positions\n",
    "            labels[i, chosen] = input_ids[i, chosen]\n",
    "\n",
    "            # apply edits\n",
    "            if n_mask.item() > 0:\n",
    "                masked_input_ids[i, mask_pos] = self.MASK\n",
    "            if n_amino.item() > 0:\n",
    "                r_idx = torch.randint(high=aa.numel(), size=(n_amino.item(),), device=device)\n",
    "                masked_input_ids[i, amino_pos] = aa[r_idx]\n",
    "\n",
    "        return masked_input_ids, labels\n",
    "\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_ids = torch.stack([f[\"input_ids\"] for f in features], dim=0)\n",
    "        attention_mask = torch.stack([f[\"attention_mask\"] for f in features], dim=0)\n",
    "        sequences = [f[\"sequence\"] for f in features]\n",
    "        proteins = [ESMProtein(sequence=f[\"sequence\"]) for f in features]\n",
    "        batched_clean = _BatchedESMProteinTensor(sequence=input_ids)\n",
    "\n",
    "\n",
    "        masked_input_ids, labels = self.mask_batch(input_ids, attention_mask)\n",
    "\n",
    "        # build masked sequences as strings (keep <mask>, drop CLS/EOS/PAD)\n",
    "        masked_sequences = []\n",
    "        for row in masked_input_ids.tolist():\n",
    "            toks = collator.tokenizer.convert_ids_to_tokens(row, skip_special_tokens=False)\n",
    "            aa = []\n",
    "            for t in toks:\n",
    "                if t in (collator.tokenizer.cls_token, collator.tokenizer.eos_token, collator.tokenizer.pad_token):\n",
    "                    continue\n",
    "                aa.append(t)  # AA tokens are single letters; keep \"<mask>\" as is\n",
    "            masked_sequences.append(\"\".join(aa))\n",
    "\n",
    "        proteins_masked = [ESMProtein(sequence=s) for s in masked_sequences]\n",
    "        batched_masked = _BatchedESMProteinTensor(sequence=masked_input_ids)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"masked_input_ids\": masked_input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"clean_input_ids\": input_ids.clone(),\n",
    "            \"clean_sequences\": sequences,                 # clean strings\n",
    "            \"masked_sequences\": masked_sequences,   # masked strings  ← NEW\n",
    "            \"clean_sequences_ESMprotein\": proteins,\n",
    "            \"masked_sequences_ESMprotein\": proteins_masked,\n",
    "            \"masked_input_ids_ESMprotein_batched\": batched_masked,\n",
    "            \"clean_input_ids_ESMprotein_batched\": batched_clean,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "146109ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your existing BatchEncodings:\n",
    "# clean_tcrs_tokenized, clean_peptides_tokenized, clean_hlas_tokenized\n",
    "clean_tcrs_tokenized = model.tokenizer(tcrs_data, return_tensors='pt', padding=True)\n",
    "clean_peptides_tokenized = model.tokenizer(peptides_data, return_tensors='pt', padding=True)\n",
    "clean_hlas_tokenized = model.tokenizer(hlas_data, return_tensors='pt', padding=True)\n",
    "\n",
    "tcr_ds = EncodedSeqDataset(tcrs_data,clean_tcrs_tokenized)\n",
    "pep_ds = EncodedSeqDataset(peptides_data, clean_peptides_tokenized)\n",
    "hla_ds = EncodedSeqDataset(hlas_data, clean_hlas_tokenized)\n",
    "\n",
    "collator = MLMProteinCollator(\n",
    "    cls_id=CLS_ID, eos_id=EOS_ID, pad_id=PAD_ID, mask_id=MASK_ID,\n",
    "    amino_acids=AA_IDS, p=0.15, min_per_seq=2, max_per_seq=45, aa_frac=0.20\n",
    ")\n",
    "collator.tokenizer = model.tokenizer\n",
    "\n",
    "\n",
    "tcr_loader = DataLoader(tcr_ds, batch_size=8, shuffle=True, num_workers=4, collate_fn=collator)\n",
    "pep_loader = DataLoader(pep_ds, batch_size=8, shuffle=True, num_workers=4, collate_fn=collator)\n",
    "hla_loader = DataLoader(hla_ds, batch_size=8, shuffle=True, num_workers=4, collate_fn=collator)\n",
    "\n",
    "\n",
    "# gives a batch dict from the collator with 4 keys\n",
    "# input_ids, labels (original tokens only at masked positions, -100 everywhere else)\n",
    "# attention_mask (0,1 for padding), clean_input_ids (clean copy of the input for clean forward pass using boltz for NC loss)\n",
    "\n",
    "# model.to(\"cpu\")\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "def optimizer_to_cpu(optim):\n",
    "    for st in optim.state.values():\n",
    "        for k, v in list(st.items()):\n",
    "            if torch.is_tensor(v):\n",
    "                st[k] = v.detach().to(\"cpu\")\n",
    "\n",
    "# move model to CPU and delete, as now we have the correct inputs \n",
    "model.to(\"cpu\")\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d9497b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_874613/627727058.py:33: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler  = GradScaler(enabled=False)                     # <-- no scaler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to tcr_encoder_checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "use_amp = torch.cuda.is_available()\n",
    "\n",
    "#base = ESMC.from_pretrained(\"esmc_300m\").to(device)\n",
    "#base = ESMC.from_pretrained(\"esmc_300m\")\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "    target_modules=[\"out_proj\", \"layernorm_qkv.1\"],  # inner Linear inside the Sequential\n",
    ")\n",
    "\n",
    "model_tcr = LoraModel(ESMC.from_pretrained(\"esmc_300m\"), lora_cfg, adapter_name=\"tcr\")\n",
    "\n",
    "\n",
    "#model_tcr = LoraModel(base, lora_cfg, adapter_name=\"tcr\")\n",
    "\n",
    "# freeze everything; unfreeze only LoRA params\n",
    "for p in model_tcr.parameters():\n",
    "    p.requires_grad = False\n",
    "for name, p in model_tcr.named_parameters():\n",
    "    if \"lora_A\" in name or \"lora_B\" in name:\n",
    "        p.requires_grad = True\n",
    "\n",
    "model_tcr.to(\"cuda\")\n",
    "model_tcr.train()\n",
    "\n",
    "optim_tcr = torch.optim.AdamW(\n",
    "    (p for p in model_tcr.parameters() if p.requires_grad),\n",
    "    lr=1e-3, weight_decay=0.01\n",
    ")\n",
    "\n",
    "\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler  = GradScaler(enabled=False)                     # <-- no scaler\n",
    "\n",
    "for batch in tcr_loader:\n",
    "    input_ids = batch[\"masked_input_ids\"].to(device, dtype=torch.long)\n",
    "    labels    = batch[\"labels\"].to(device)\n",
    "\n",
    "    with autocast(\"cuda\", enabled=use_amp, dtype=torch.bfloat16):  # <-- bf16\n",
    "        out    = model_tcr(input_ids)\n",
    "        logits = out.sequence_logits\n",
    "        #print(type(logits), logits.shape, logits.requires_grad)  # expect: Tensor, [B,L,V], True\n",
    "\n",
    "        loss   = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            labels.view(-1),\n",
    "            ignore_index=-100\n",
    "        )\n",
    "\n",
    "    loss.backward()                                     # <-- standard backward\n",
    "    optim_tcr.step(); optim_tcr.zero_grad(set_to_none=True)\n",
    "\n",
    "    del out, logits, loss, input_ids, labels, batch\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "# 1.1 seconds for 100\n",
    "# 11 for 1000\n",
    "# 110 for 10000\n",
    "# 350 for 35000 - 5.8 minutes to train one encoder on full dataset\n",
    "\n",
    "optimizer_to_cpu(optim_tcr)\n",
    "\n",
    "# save tcr model\n",
    "\n",
    "checkpoint_filename = 'tcr_encoder_checkpoint.pth'\n",
    "\n",
    "checkpoint_dict = {\n",
    "    #'epoch': num_epochs,\n",
    "    'tcr_model_state_dict': model_tcr.state_dict(),\n",
    "    #'pep_model_state_dict': peptide_model_nc.state_dict(), \n",
    "    'optimizer_state_dict': optim_tcr.state_dict(),\n",
    "    #'final_loss': avg_epoch_loss,\n",
    "    #'binding_threshold': threshold  # Add threshold to saved state\n",
    "}\n",
    "\n",
    "torch.save(checkpoint_dict, checkpoints_dir/checkpoint_filename)\n",
    "\n",
    "print(f\"Checkpoint saved to {checkpoint_filename}\")\n",
    "\n",
    "model_tcr.to(\"cpu\")\n",
    "\n",
    "del optim_tcr, model_tcr, checkpoint_dict, checkpoint_filename\n",
    "\n",
    "torch.cuda.empty_cache()  # Free up GPU memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bd0beba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_874613/363416641.py:32: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler  = GradScaler(enabled=False)                     # <-- no scaler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to peptide_encoder_checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "# peptide encoder\n",
    "\n",
    "use_amp = torch.cuda.is_available()\n",
    "\n",
    "#base = ESMC.from_pretrained(\"esmc_300m\").to(device)\n",
    "\n",
    "## use the same lora config for peptide and HLA - modify later\n",
    "# lora_cfg = LoraConfig(\n",
    "#     r=8, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "#     target_modules=[\"out_proj\", \"layernorm_qkv.1\"],  # inner Linear inside the Sequential\n",
    "# )\n",
    "\n",
    "model_pep = LoraModel(ESMC.from_pretrained(\"esmc_300m\"), lora_cfg, adapter_name=\"pep\")\n",
    "#model_pep = LoraModel(base, lora_cfg, adapter_name=\"peptide\")\n",
    "\n",
    "# freeze everything; unfreeze only LoRA params\n",
    "for p in model_pep.parameters():\n",
    "    p.requires_grad = False\n",
    "for name, p in model_pep.named_parameters():\n",
    "    if \"lora_A\" in name or \"lora_B\" in name:\n",
    "        p.requires_grad = True\n",
    "\n",
    "model_pep.to(\"cuda\")\n",
    "model_pep.train()\n",
    "\n",
    "optim_pep = torch.optim.AdamW(\n",
    "    (p for p in model_pep.parameters() if p.requires_grad),\n",
    "    lr=1e-3, weight_decay=0.01\n",
    ")\n",
    "\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler  = GradScaler(enabled=False)                     # <-- no scaler\n",
    "\n",
    "for batch in pep_loader:\n",
    "    input_ids = batch[\"masked_input_ids\"].to(device, dtype=torch.long)\n",
    "    labels    = batch[\"labels\"].to(device)\n",
    "\n",
    "    with autocast(\"cuda\", enabled=use_amp, dtype=torch.bfloat16):  # <-- bf16\n",
    "        out    = model_pep(input_ids)\n",
    "        logits = out.sequence_logits\n",
    "        #print(type(logits), logits.shape, logits.requires_grad)  # expect: Tensor, [B,L,V], True\n",
    "\n",
    "        loss   = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            labels.view(-1),\n",
    "            ignore_index=-100\n",
    "        )\n",
    "\n",
    "    loss.backward()                                     # <-- standard backward\n",
    "    optim_pep.step(); optim_pep.zero_grad(set_to_none=True)\n",
    "\n",
    "    del out, logits, loss, input_ids, labels, batch\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "optimizer_to_cpu(optim_pep)\n",
    "\n",
    "# save peptide model\n",
    "\n",
    "checkpoint_filename = 'peptide_encoder_checkpoint.pth'\n",
    "\n",
    "checkpoint_dict = {\n",
    "    #'epoch': num_epochs,\n",
    "    'peptide_model_state_dict': model_pep.state_dict(),\n",
    "    'optimizer_state_dict': optim_pep.state_dict(),\n",
    "    #'final_loss': avg_epoch_loss,\n",
    "    #'binding_threshold': threshold  # Add threshold to saved state\n",
    "}\n",
    "\n",
    "torch.save(checkpoint_dict, checkpoints_dir/checkpoint_filename)\n",
    "\n",
    "print(f\"Checkpoint saved to {checkpoint_filename}\")\n",
    "\n",
    "model_pep.to(\"cpu\")\n",
    "del optim_pep, model_pep, checkpoint_dict, checkpoint_filename\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "#print(torch.cuda.memory_summary())           # “Active” bytes should drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b319d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_874613/2523016605.py:33: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler  = GradScaler(enabled=False)                     # <-- no scaler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to hla_encoder_checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "# HLA encoder\n",
    "\n",
    "use_amp = torch.cuda.is_available()\n",
    "\n",
    "#base = ESMC.from_pretrained(\"esmc_300m\").to(device)\n",
    "\n",
    "## use the same lora config for peptide and HLA - modify later\n",
    "# lora_cfg = LoraConfig(\n",
    "#     r=8, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "#     target_modules=[\"out_proj\", \"layernorm_qkv.1\"],  # inner Linear inside the Sequential\n",
    "# )\n",
    "\n",
    "#model_hla = LoraModel(base, lora_cfg, adapter_name=\"hla\")\n",
    "model_hla = LoraModel(ESMC.from_pretrained(\"esmc_300m\"), lora_cfg, adapter_name=\"hla\")\n",
    "\n",
    "\n",
    "# freeze everything; unfreeze only LoRA params\n",
    "for p in model_hla.parameters():\n",
    "    p.requires_grad = False\n",
    "for name, p in model_hla.named_parameters():\n",
    "    if \"lora_A\" in name or \"lora_B\" in name:\n",
    "        p.requires_grad = True\n",
    "\n",
    "model_hla.to(\"cuda\")\n",
    "model_hla.train()\n",
    "\n",
    "optim_hla = torch.optim.AdamW(\n",
    "    (p for p in model_hla.parameters() if p.requires_grad),\n",
    "    lr=1e-3, weight_decay=0.01\n",
    ")\n",
    "\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler  = GradScaler(enabled=False)                     # <-- no scaler\n",
    "\n",
    "for batch in hla_loader:\n",
    "    input_ids = batch[\"masked_input_ids\"].to(device, dtype=torch.long)\n",
    "    labels    = batch[\"labels\"].to(device)\n",
    "\n",
    "    with autocast(\"cuda\", enabled=use_amp, dtype=torch.bfloat16):  # <-- bf16\n",
    "        out    = model_hla(input_ids)\n",
    "        logits = out.sequence_logits\n",
    "        #print(type(logits), logits.shape, logits.requires_grad)  # expect: Tensor, [B,L,V], True\n",
    "\n",
    "        loss   = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            labels.view(-1),\n",
    "            ignore_index=-100\n",
    "        )\n",
    "\n",
    "    loss.backward()                                     # <-- standard backward\n",
    "    optim_hla.step()\n",
    "    optim_hla.zero_grad(set_to_none=True)\n",
    "\n",
    "    # clear memory\n",
    "    del out, logits, loss, input_ids, labels, batch\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "optimizer_to_cpu(optim_hla)\n",
    "\n",
    "# save HLA model\n",
    "\n",
    "checkpoint_filename = 'hla_encoder_checkpoint.pth'\n",
    "\n",
    "checkpoint_dict = {\n",
    "    #'epoch': num_epochs,\n",
    "    'hla_model_state_dict': model_hla.state_dict(),\n",
    "    'optimizer_state_dict': optim_hla.state_dict(),\n",
    "    #'final_loss': avg_epoch_loss,\n",
    "    #'binding_threshold': threshold  # Add threshold to saved state\n",
    "}\n",
    "\n",
    "torch.save(checkpoint_dict, checkpoints_dir/checkpoint_filename)\n",
    "\n",
    "print(f\"Checkpoint saved to {checkpoint_filename}\")\n",
    "\n",
    "model_hla.to(\"cpu\")\n",
    "del optim_hla, model_hla, checkpoint_dict, checkpoint_filename\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a9ef0",
   "metadata": {},
   "source": [
    "#### Projection Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3939828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f38124f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Encoders loaded successfully as fixed feature extractors!\n",
      "  (Optimizer state skipped - not needed for this use case)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LOADING ENCODERS FOR USE AS FIXED FEATURE EXTRACTORS\n",
    "\n",
    "Use case: You're NOT continuing to train the encoders. Instead, you'll use them\n",
    "as fixed/pre-trained components in the next part of your training pipeline (likely\n",
    "the multimodal model that combines TCR, peptide, and HLA embeddings).\n",
    "\n",
    "WHAT YOU NEED:\n",
    "✅ Model weights (to get the fine-tuned LoRA adapters)\n",
    "✅ Set to .eval() mode (to disable dropout, use batch norm stats, etc.)\n",
    "\n",
    "WHAT YOU DON'T NEED:\n",
    "❌ Optimizer state - only needed if you were continuing to train the encoders\n",
    "\n",
    "NOTE: The checkpoint contains 'optimizer_state_dict', but we're ignoring it\n",
    "since we're using these models as frozen feature extractors.\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Load the checkpoint dictionaries (just Python dicts from disk)\n",
    "# checkpoint_tcr = torch.load(checkpoints_dir/'tcr_encoder_checkpoint.pth', map_location=device)\n",
    "# checkpoint_pep = torch.load(checkpoints_dir/'peptide_encoder_checkpoint.pth', map_location=device)\n",
    "# checkpoint_hla = torch.load(checkpoints_dir/'hla_encoder_checkpoint.pth', map_location=device)\n",
    "checkpoint_tcr = torch.load(checkpoints_dir/'tcr_encoder_checkpoint.pth', map_location='cpu')\n",
    "checkpoint_pep = torch.load(checkpoints_dir/'peptide_encoder_checkpoint.pth', map_location='cpu')\n",
    "checkpoint_hla = torch.load(checkpoints_dir/'hla_encoder_checkpoint.pth', map_location='cpu')\n",
    "\n",
    "# Step 2: Recreate the model architectures (same as during training)\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "    target_modules=[\"out_proj\", \"layernorm_qkv.1\"],\n",
    ")\n",
    "\n",
    "# Create models with same architecture as during training\n",
    "tcr_encoder = LoraModel(ESMC.from_pretrained(\"esmc_300m\"), lora_cfg, adapter_name=\"tcr\")\n",
    "peptide_encoder = LoraModel(ESMC.from_pretrained(\"esmc_300m\"), lora_cfg, adapter_name=\"pep\")\n",
    "hla_encoder = LoraModel(ESMC.from_pretrained(\"esmc_300m\"), lora_cfg, adapter_name=\"hla\")\n",
    "\n",
    "# Step 3: Load the model state dictionaries (this loads base weights + LoRA adapters)\n",
    "# NOTE: We're NOT loading optimizer_state_dict - not needed for inference/feature extraction\n",
    "tcr_encoder.load_state_dict(checkpoint_tcr['tcr_model_state_dict'])\n",
    "peptide_encoder.load_state_dict(checkpoint_pep['peptide_model_state_dict'])\n",
    "hla_encoder.load_state_dict(checkpoint_hla['hla_model_state_dict'])\n",
    "\n",
    "# Step 4: Set to evaluation mode and move to CPU\n",
    "# .eval() mode ensures: no dropout, batch norm uses running stats, etc.\n",
    "tcr_encoder.to('cpu').eval()\n",
    "peptide_encoder.to('cpu').eval()\n",
    "hla_encoder.to('cpu').eval()\n",
    "\n",
    "print(\"✓ Encoders loaded successfully as fixed feature extractors!\")\n",
    "print(\"  (Optimizer state skipped - not needed for this use case)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939b46f2",
   "metadata": {},
   "source": [
    "##### a) Factorisation/Projection for ESMC Encoders\n",
    "- N.B. From next cell onwards, need to reconfigure to do for loop for all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "561878ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pick a batch and get token tensors and masks\n",
    "\n",
    "tcr_batch = next(iter(tcr_loader))\n",
    "pep_batch = next(iter(pep_loader))\n",
    "hla_batch = next(iter(hla_loader))\n",
    "\n",
    "tcr_ids = tcr_batch['clean_input_ids']\n",
    "tcr_mask = tcr_batch['attention_mask']\n",
    "\n",
    "pep_ids = pep_batch['clean_input_ids']\n",
    "pep_mask = pep_batch['attention_mask']\n",
    "\n",
    "hla_ids = hla_batch['clean_input_ids']\n",
    "hla_mask = hla_batch['attention_mask']\n",
    "\n",
    "# Unsqueze to make the right dims\n",
    "mT = tcr_mask.unsqueeze(-1).float()   # (B, L_T_pad, 1)\n",
    "mP = pep_mask.unsqueeze(-1).float()   # (B, L_P_pad, 1)\n",
    "mH = hla_mask.unsqueeze(-1).float()   # (B, L_H_pad, 1)\n",
    "\n",
    "\n",
    "B = tcr_ids.size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "584ac9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the BASE model inside the LoRA wrapper\n",
    "with torch.no_grad():\n",
    "    out_T = tcr_encoder.model.forward(sequence_tokens=tcr_ids)\n",
    "    out_P = peptide_encoder.model.forward(sequence_tokens=pep_ids)\n",
    "    out_H = hla_encoder.model.forward(sequence_tokens=hla_ids)\n",
    "\n",
    "# Some ESMC builds expose .embeddings; otherwise take last hidden state\n",
    "emb_T = out_T.embeddings \n",
    "emb_P = out_P.embeddings \n",
    "emb_H = out_H.embeddings\n",
    "\n",
    "# it is true that the shape of the embeddings is (B, L_pad, D)\n",
    "# takes 18 seconds on CPU to do 24 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a921c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum true tcr length \n",
    "L_T_true = tcr_mask.sum(dim=1)\n",
    "L_T_max = L_T_true.max()\n",
    "\n",
    "# maximum true peptide length \n",
    "L_P_true = pep_mask.sum(dim=1)\n",
    "L_P_max = L_P_true.max()\n",
    "\n",
    "# maximum true HLA length \n",
    "L_H_true = hla_mask.sum(dim=1)\n",
    "L_H_max = L_H_true.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b2e111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorised Encoder to get z_T and Z_pMHC\n",
    "# z = vec(A^TXB)H\n",
    "# X - (B, L_pad, D)\n",
    "# B - (D, rD)\n",
    "# A - (L_pad, rL)\n",
    "# H - (rD * rL, d)\n",
    "\n",
    "\n",
    "eps = 1e-8\n",
    "\n",
    "class ESMFactorisedEncoder(nn.Module):\n",
    "    def __init__(self, D, rL, rD, d, L_max):\n",
    "        \"\"\"\n",
    "        D    : ESM embedding dim (e.g. 960)\n",
    "        rL   : positional rank\n",
    "        rD   : channel rank\n",
    "        d    : latent dim\n",
    "        L_max: max true length for this modality in the batch\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.D   = D\n",
    "        self.rL  = rL\n",
    "        self.rD  = rD\n",
    "        self.d   = d\n",
    "        self.L_max = L_max\n",
    "\n",
    "        # Channel mixing: D -> rD\n",
    "        self.B_c = nn.Parameter(torch.empty(D, rD))\n",
    "        nn.init.xavier_uniform_(self.B_c)\n",
    "\n",
    "        # Positional mixing: positions 0..L_max-1 -> rL\n",
    "        self.A_c = nn.Parameter(torch.empty(L_max, rL))\n",
    "        nn.init.xavier_uniform_(self.A_c)\n",
    "\n",
    "        # Final map: (rL * rD) -> d\n",
    "        self.H_c = nn.Parameter(torch.empty(rL * rD, d))\n",
    "        nn.init.xavier_uniform_(self.H_c)\n",
    "\n",
    "    def forward(self, emb, mask):\n",
    "        \"\"\"\n",
    "        emb  : (B, L_pad, D) token embeddings\n",
    "        mask : (B, L_pad)   1 = real token, 0 = pad\n",
    "        returns z : (B, d)\n",
    "        \"\"\"\n",
    "        device = emb.device\n",
    "        B, L_pad, D = emb.shape\n",
    "        assert D == self.D\n",
    "\n",
    "        # Compute true lengths\n",
    "        L_true = mask.sum(dim=1)            # (B,)\n",
    "        z_list = []\n",
    "\n",
    "        for b in range(B):\n",
    "            Lb = int(L_true[b].item())\n",
    "            if Lb == 0:\n",
    "                # Degenerate case: no tokens -> zero vector\n",
    "                z_b = torch.zeros(self.d, device=device)\n",
    "                z_list.append(z_b)\n",
    "                continue\n",
    "\n",
    "            Xb = emb[b, :Lb, :]                      # (Lb, D)\n",
    "            mb = mask[b, :Lb].unsqueeze(-1).float()  # (Lb, 1)\n",
    "            Xb = Xb * mb                             # (Lb, D)\n",
    "\n",
    "            # 1) Channel compression: D -> rD\n",
    "            Yb = Xb @ self.B_c                       # (Lb, rD)\n",
    "\n",
    "            # 2) Positional compression: Lb -> rL\n",
    "            A_pos = self.A_c[:Lb, :]                 # (Lb, rL)\n",
    "            Ub = A_pos.T @ Yb                        # (rL, rD)\n",
    "\n",
    "            # 3) Flatten and map to latent d\n",
    "            Ub_flat = Ub.reshape(-1)                 # (rL * rD,)\n",
    "            z_b = Ub_flat @ self.H_c                 # (d,)\n",
    "\n",
    "            # 4) Normalise (optional; you can drop this if you want magnitude to carry info)\n",
    "            #z_b = z_b / (z_b.norm() + eps)\n",
    "            #normalise after function because need to combine p and hla first\n",
    "\n",
    "            z_list.append(z_b)\n",
    "\n",
    "        z = torch.stack(z_list, dim=0)               # (B, d)\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bb7b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape\n",
    "B, L_T_pad, D = emb_T.shape\n",
    "\n",
    "# Latent ranks and final dimension (hyperparameters)\n",
    "rL = 8      # positional rank for TCR (tunable)\n",
    "rD = 16     # channel rank for TCR (tunable)\n",
    "d    = 128    # final latent dimension (same d as in Z*)\n",
    "\n",
    "# ratio of peptide to HLA (hyperparameter)\n",
    "#R = 0.7\n",
    "# Epsilon for numerical stability\n",
    "eps=1e-8\n",
    "\n",
    "# maximum true lenghts of the sequences\n",
    "# maximum true tcr length \n",
    "L_T_true = tcr_mask.sum(dim=1)\n",
    "L_T_max = L_T_true.max()\n",
    "# maximum true peptide length \n",
    "L_P_true = pep_mask.sum(dim=1)\n",
    "L_P_max = L_P_true.max()\n",
    "# maximum true HLA length \n",
    "L_H_true = hla_mask.sum(dim=1)\n",
    "L_H_max = L_H_true.max()\n",
    "\n",
    "\n",
    "tcr_encoder_new = ESMFactorisedEncoder(D, rL, rD, d, L_max=L_T_max).to(device)\n",
    "pep_encoder_new = ESMFactorisedEncoder(D, rL, rD, d, L_max=L_P_max).to(device)\n",
    "hla_encoder_new = ESMFactorisedEncoder(D, rL, rD, d, L_max=L_H_max).to(device)\n",
    "\n",
    "# when you call module(), you are calling the forward method (PyTorch convention)\n",
    "zT = tcr_encoder_new(emb_T, tcr_mask)\n",
    "zP = pep_encoder_new(emb_P, pep_mask)\n",
    "zH = hla_encoder_new(emb_H, hla_mask)\n",
    "\n",
    "# normalise T as we are not gating this\n",
    "zT = zT / (zT.norm(dim=-1, keepdim=True) + eps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e58c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to what Barbara suggested in meeting - scale within the projection learning\n",
    "\n",
    "R_PH = 0.7  # peptide gets more weight\n",
    "\n",
    "gP = (R_PH ** 0.5)          # scalar\n",
    "gH = ((1.0 - R_PH) ** 0.5)  # scalar\n",
    "\n",
    "gP_t = torch.tensor(gP, device=device)\n",
    "gH_t = torch.tensor(gH, device=device)\n",
    "\n",
    "# zP, zH: (B, d)\n",
    "zPH = gP_t * zP + gH_t * zH      # (B, d)\n",
    "# Optionally normalise:\n",
    "zPH = zPH / (zPH.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "# concatenate into e_hat\n",
    "e_hat = torch.cat([zT, zPH], dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "584682df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next step to apply non linear layers (potentially ReLU as middle layers, as long as the end is linear transformation and can learn negative values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf91b7",
   "metadata": {},
   "source": [
    "##### Get Boltz Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cca6b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Boltz embeddings\n",
    "file_path = '/home/natasha/multimodal_model/outputs/boltz_runs/positives/pair_000/boltz_results_pair_000/predictions/pair_000/embeddings_pair_000.npz'\n",
    "manifest_path = '/home/natasha/multimodal_model/data/manifests/boltz_100_manifest.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3c488fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/natasha/multimodal_model/scripts/train\n",
      "/home/natasha/multimodal_model/data/manifests/boltz_100_manifest.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "home = '/home/natasha/multimodal_model'\n",
    "manifest_path = os.path.join(home, 'data', 'manifests', 'boltz_100_manifest.csv')\n",
    "print(manifest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a64ea0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch z shapes: [torch.Size([604, 604, 128]), torch.Size([604, 604, 128]), torch.Size([604, 604, 128]), torch.Size([604, 604, 128]), torch.Size([604, 604, 128]), torch.Size([604, 604, 128]), torch.Size([604, 604, 128]), torch.Size([604, 604, 128])]\n",
      "Pep lengths: [ 9  9 10  9  9 10  9 10]\n"
     ]
    }
   ],
   "source": [
    "class BoltzDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for loading Boltz z-embeddings one by one,\n",
    "    with chain lengths from the manifest.\n",
    "    Each pair has its own .npz file.\n",
    "    ORIGINAL VERSION - returns numpy arrays\n",
    "    \"\"\"\n",
    "    def __init__(self, manifest_path, base_path):\n",
    "        self.manifest = pd.read_csv(manifest_path)\n",
    "        self.base_path = base_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.manifest)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        yaml_rel_path = self.manifest.iloc[idx]['yaml_path']\n",
    "        pair_id = os.path.splitext(os.path.basename(yaml_rel_path))[0]\n",
    "        emb_path = os.path.join(\n",
    "            self.base_path,\n",
    "            'outputs',\n",
    "            'boltz_runs',\n",
    "            'positives',\n",
    "            pair_id,\n",
    "            f'boltz_results_{pair_id}',\n",
    "            'predictions',\n",
    "            pair_id,\n",
    "            f'embeddings_{pair_id}.npz'\n",
    "        )\n",
    "        with np.load(emb_path) as arr:\n",
    "            z = arr['z']  # Returns numpy array as-is\n",
    "        pep_len = self.manifest.iloc[idx]['pep_len']\n",
    "        tcra_len = self.manifest.iloc[idx]['tcra_len']\n",
    "        tcrb_len = self.manifest.iloc[idx]['tcrb_len']\n",
    "        hla_len = self.manifest.iloc[idx]['hla_len']\n",
    "        return z, pep_len, tcra_len, tcrb_len, hla_len\n",
    "\n",
    "\n",
    "def boltz_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Uses numpy for padding, then converts to torch at the end\n",
    "    \"\"\"\n",
    "    zs, pep_lens, tcra_lens, tcrb_lens, hla_lens = zip(*batch)\n",
    "    # Each z has shape [sum_of_lengths, sum_of_lengths, dim] or [1, sum_of_lengths, sum_of_lengths, dim]\n",
    "    # Pad zs to max shape with zeros and stack into tensor\n",
    "    zs = [np.squeeze(z, axis=0) for z in zs]\n",
    "    max_len = max(z.shape[0] for z in zs)\n",
    "    # get channel dimension number from first z\n",
    "    dim = zs[0].shape[-1]\n",
    "    padded_zs = np.zeros((len(zs), max_len, max_len, dim), dtype=zs[0].dtype)\n",
    "    for i, z in enumerate(zs):\n",
    "        l = z.shape[0]\n",
    "        padded_zs[i, :l, :l, :] = z\n",
    "    zs = torch.from_numpy(padded_zs).float()  # or .to(device)\n",
    "    \n",
    "    return {\n",
    "        \"z\": zs,  # batch of z arrays, each possibly of different shape\n",
    "        \"pep_len\": np.array(pep_lens),\n",
    "        \"tcra_len\": np.array(tcra_lens),\n",
    "        \"tcrb_len\": np.array(tcrb_lens),\n",
    "        \"hla_len\": np.array(hla_lens),\n",
    "    }\n",
    "\n",
    "# Example usage with original version:\n",
    "dataset_original = BoltzDataset(manifest_path, home)\n",
    "dataloader_original = DataLoader(\n",
    "    dataset_original, batch_size=8, shuffle=True, collate_fn=boltz_collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "batch = next(iter(dataloader_original))\n",
    "print(\"Batch z shapes:\", [z.shape for z in batch[\"z\"]])\n",
    "print(\"Pep lengths:\", batch[\"pep_len\"])\n",
    "\n",
    "# 5.6s to load 100 pairs\n",
    "# 56 s to load 1000 pairs\n",
    "# 560 s to load 10000 pairs (9 mins)\n",
    "# 5600 s to load 100000 pairs (16 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9717bdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230 10 365 [112 114 112 110 113 112 113 112] [116 116 113 114 115 114 115 117] [ 9  9 10  9  9 10  9 10] [362 365 365 365 365 365 365 365]\n"
     ]
    }
   ],
   "source": [
    "L_T = batch[\"tcra_len\"] + batch[\"tcrb_len\"]\n",
    "L_P = batch[\"pep_len\"]\n",
    "L_H = batch[\"hla_len\"]\n",
    "\n",
    "L_T_max = max(L_T)\n",
    "L_P_max = max(L_P)\n",
    "L_H_max = max(L_H)\n",
    "\n",
    "print(L_T_max, L_P_max, L_H_max, batch['tcra_len'], batch['tcrb_len'], batch['pep_len'], batch['hla_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f9e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorisation Z\n",
    "\n",
    "for i in range(len(batch[\"z\"])):\n",
    "    L_T_max = batch[\"tcra_len\"][i]\n",
    "    L_P_max = batch[\"pep_len\"][i]\n",
    "    L_H_max = batch[\"hla_len\"][i]\n",
    "\n",
    "\n",
    "\n",
    "dB  = 128 # dimension of Boltz embeddings\n",
    "rB  = 16 # Boltz channel rank\n",
    "rT  = 8 # TCR positional rank\n",
    "rPH = 8 # pMHC positional rank\n",
    "\n",
    "B_Z = torch.nn.Parameter(torch.empty(dB, rB))\n",
    "nn.init.xavier_uniform_(B_Z)\n",
    "\n",
    "\n",
    "L_T_max = int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ccef78",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mBoltzFactorised\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Factorised Boltz embeddings for projection into latent shared space before NC loss\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    - Zstar_batch: (B, 2d, 2d) operator acting on e_hat_t, e_hat_pmc in R^2d\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dB, rB, rT, rPH, d, L_max, L_PH_max):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class BoltzFactorised(nn.Module):\n",
    "    \"\"\"\n",
    "    Factorised Boltz embeddings for projection into latent shared space before NC loss\n",
    "\n",
    "    Inputs:\n",
    "    - z_boltz: (B, L_pad, L_pad, d_boltz) full Boltz z for the batch\n",
    "    - L_alpha, L_beta, L_p, L_h: (B,) lengths of the TCR alpha, TCR beta, peptide, HLA\n",
    "    - gP, gH: (B,) scalar or (B,) peptide/HLA gates in [0,1], norm-preserving in quadrature???? As in, gP**2 + gH**2 = 1\n",
    "\n",
    "    Outputs:\n",
    "    - Zstar_batch: (B, 2d, 2d) operator acting on e_hat_t, e_hat_pmc in R^2d\n",
    "    \"\"\"\n",
    "    def __init__(self, dB, rB, rT, rPH, d, L_max, L_PH_max):\n",
    "        \"\"\"\n",
    "        dB      : channel dimension of Boltz embeddings\n",
    "        rB      : rank of Boltz channel factorisation\n",
    "        rT      : rank of TCR positional encoding\n",
    "        rPH     : rank of pMHC positional encoding\n",
    "        d       : latent dimension of shared space\n",
    "        L_T_max   : maximum length of any sequence in the batch\n",
    "        L_PH_max: maximum length of any pMHC sequence in the batch\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dB     = dB\n",
    "        self.rB     = rB\n",
    "        self.rT     = rT\n",
    "        self.rPH    = rPH\n",
    "        self.d      = d\n",
    "        self.L_T_max  = L_max\n",
    "        self.L_PH_max = L_PH_max\n",
    "\n",
    "        # ---- 1) Channel mixing: dB -> rB ---- \n",
    "        self.B_Z = torch.nn.Parameter(torch.empty(dB, rB))\n",
    "        nn.init.xavier_uniform_(self.B_Z)\n",
    "\n",
    "        # ---- 2)a) TCR positional encoding: rT -> rT ---- \n",
    "        self.A_T = torch.nn.Parameter(torch.empty(L_T_max, rT))\n",
    "        nn.init.xavier_uniform_(self.A_T)\n",
    "\n",
    "        # ---- 2)b) pMHC positional encoding: rPH -> rPH ---- \n",
    "        self.A_PH = torch.nn.Parameter(torch.empty(L_PH_max, rPH))\n",
    "        nn.init.xavier_uniform_(self.A_PH)\n",
    "\n",
    "        # ---- 3) Learnable maps from factorised z (r* x r* x rB) -> d x d ---- \n",
    "        # flatten sizes for each block\n",
    "        n_TT   = rT  * rT  * rB\n",
    "        n_TPH  = rT  * rPH * rB\n",
    "        n_PHT  = rPH * rT  * rB\n",
    "        n_PHPH = rPH * rPH * rB\n",
    "        dd     = d * d\n",
    "\n",
    "        self.H_TT   = nn.Parameter(torch.empty(n_TT,   dd))\n",
    "        self.H_TPH  = nn.Parameter(torch.empty(n_TPH,  dd))\n",
    "        self.H_PHT  = nn.Parameter(torch.empty(n_PHT,  dd))\n",
    "        self.H_PHPH = nn.Parameter(torch.empty(n_PHPH, dd))\n",
    "\n",
    "        nn.init.xavier_uniform_(self.H_TT)\n",
    "        nn.init.xavier_uniform_(self.H_TPH)\n",
    "        nn.init.xavier_uniform_(self.H_PHT)\n",
    "        nn.init.xavier_uniform_(self.H_PHPH)\n",
    "\n",
    "        # ---- 4) Final linear layer: d -> d ---- \n",
    "        self.W_out = nn.Parameter(torch.empty(d, d))\n",
    "        nn.init.xavier_uniform_(self.W_out)\n",
    "    \n",
    "    def _get_gate_scalar(self, g, b):\n",
    "        \"\"\"\n",
    "        Helper: allow g to be a scalar tensor () or per-sample tensor (B,).\n",
    "        Returns a Python float for sample b.\n",
    "        \"\"\"\n",
    "        if g.dim() == 0:\n",
    "            return float(g.item())\n",
    "        else:\n",
    "            return float(g[b].item())\n",
    "\n",
    "    def forward(self, z_boltz, L_alpha, L_beta, L_p, L_h, gP, gH):\n",
    "        \"\"\"\n",
    "        Z_boltz : (B, L_pad, L_pad, dB)\n",
    "        L_alpha : (B,) true alpha lengths\n",
    "        L_beta  : (B,) true beta lengths\n",
    "        L_p     : (B,) true peptide lengths\n",
    "        L_h     : (B,) true HLA lengths\n",
    "        gP      : scalar () or (B,) peptide gate (already sqrt(R_PH))\n",
    "        gH      : scalar () or (B,) HLA gate (already sqrt(1-R_PH))\n",
    "\n",
    "        Returns:\n",
    "          Zstar_batch: (B, 2d, 2d)\n",
    "        \"\"\"\n",
    "\n",
    "        device = z_boltz.device\n",
    "        B, L_pad, _, dB = z_boltz.shape\n",
    "        assert dB == self.dB\n",
    "\n",
    "        Zstar_list = []\n",
    "\n",
    "        for b in range(B):\n",
    "            La  = int(L_alpha[b].item())\n",
    "            Lb  = int(L_beta[b].item())\n",
    "            Lp_ = int(L_p[b].item())\n",
    "            Lh_ = int(L_h[b].item())\n",
    "\n",
    "            L_T     = La + Lb\n",
    "            L_PH    = Lp_ + Lh_\n",
    "            L       = L_T + L_PH\n",
    "\n",
    "            # if we have missing z it just returns identity\n",
    "            if L == 0:\n",
    "                I_2d = torch.eye(2* self.d, device=device)\n",
    "                Zstar_list.append(I_2d)\n",
    "                continue\n",
    "            \n",
    "\n",
    "            # restrict to true tokens for the sample\n",
    "            Z = z_boltz[b, :L, :L, :] # (L, L, dB)\n",
    "\n",
    "            # ---- 2) Per channel normalisation (right now omit this step) ----\n",
    "            # another potential here is to use Adam optimiser to learn the normalisation\n",
    "            # mu = Z.mean(dim=(0,1), keepdim=True) # (1, 1, dB)\n",
    "            # std = Z.std(dim=(0,1), keepdim=True) # (1, 1, dB)\n",
    "            # Zc = (Z - mu) / std            # (L, L, dB)\n",
    "            # Zc = Zc / (math.sqrt(L) + eps) # scale by sqrt(L)\n",
    "            Zc = Z.clone()\n",
    "\n",
    "            # ----- 3) Gating ----\n",
    "            # TCR gates\n",
    "            if La > 0 and Lb > 0:\n",
    "                gA_b = 2**-0.5\n",
    "                gB_b = 2**-0.5\n",
    "            elif La > 0 and Lb == 0:\n",
    "                gA_b = 1\n",
    "                gB_b = 0\n",
    "            elif La == 0 and Lb > 0:\n",
    "                gA_b = 0\n",
    "                gB_b = 1\n",
    "            else:\n",
    "                gA_b = 0\n",
    "                gB_b = 0\n",
    "\n",
    "            # Peptide/HLA gates (from R set in encoder part)\n",
    "            gP_b = self._get_gate_scalar(gP, b)\n",
    "            gH_b = self._get_gate_scalar(gH, b)\n",
    "\n",
    "            # ---- 4) Build token-level gate vector over [alpha | beta | p | h] ----\n",
    "            gate = torch.zeros(L, device=device) # (L,)\n",
    "\n",
    "            idx0 = 0\n",
    "            idx1 = idx0 + La\n",
    "            idx2 = idx1 + Lb\n",
    "            idx3 = idx2 + Lp_\n",
    "            idx4 = idx3 + Lh_\n",
    "\n",
    "            if La  > 0: gate[idx0:idx1] = gA_b\n",
    "            if Lb  > 0: gate[idx1:idx2] = gB_b\n",
    "            if Lp_ > 0: gate[idx2:idx3] = gP_b\n",
    "            if Lh_ > 0: gate[idx3:idx4] = gH_b\n",
    "\n",
    "            gate_row = gate.view(L, 1, 1) # (L, 1, 1)\n",
    "            gate_col = gate.view(1, L, 1) # (1, L, 1)\n",
    "\n",
    "            Zg = Zc * gate_row * gate_col # (L, L, dB)\n",
    "\n",
    "            # ---- 5) Get TCR/pMHC blocks ----\n",
    "            sT = slice(0, L_T)           # [0, L_T) -> TCR (alpha + beta)\n",
    "            sPH = slice(L_T, L_T + L_PH) # [L_T, L] -> pMHC (P+H)\n",
    "\n",
    "            Z_TT  = Zg[sT, sT, :]    # (L_T, L_T, dB)\n",
    "            Z_TPH = Zg[sT, sPH, :]   # (L_T, L_PH, dB)\n",
    "            Z_PHT = Zg[sPH, sT, :]   # (L_PH, L_T, dB)\n",
    "            Z_PHPH = Zg[sPH, sPH, :] # (L_PH, L_PH, dB)\n",
    "\n",
    "            # ---- 6) channel/dimension compression ----\n",
    "            B_Z = self.B_Z # (dB, rB) operator across channels\n",
    "            Y_TT   = torch.einsum('ijc,cr->ijr', Z_TT,   B_Z)   # (L_T,  L_T,  rB)\n",
    "            Y_TPH  = torch.einsum('ijc,cr->ijr', Z_TPH,  B_Z)   # (L_T,  L_PH, rB)\n",
    "            Y_PHT  = torch.einsum('ijc,cr->ijr', Z_PHT,  B_Z)   # (L_PH, L_T,  rB)\n",
    "            Y_PHPH = torch.einsum('ijc,cr->ijr', Z_PHPH, B_Z)   # (L_PH, L_PH, rB)\n",
    "\n",
    "            # ---- 7) TCR positional compression with A_T / A_PH ----\n",
    "            # Sample-specific rows for the correct lengths for the per-sample positional tensors\n",
    "            if L_T > 0:\n",
    "                A_T_b = self.A_T[:L_T, :] # (L_T, rT)\n",
    "            else: \n",
    "                # no TCRs, so treat as zero contribution (or could be noise, decide later)\n",
    "                A_T_b = self.A_T[:1, :] * 0.0 # (1, rT) dummy\n",
    "\n",
    "            if L_PH > 0:\n",
    "                A_PH_b = self.A_PH[:L_PH, :] # (L_PH, rPH)\n",
    "            else:\n",
    "                # no pMHCs, so treat as zero contribution (or could be noise, decide later)\n",
    "                A_PH_b = self.A_PH[:1, :] * 0.0 # (1, rPH) dummy\n",
    "            \n",
    "            rT  = self.rT\n",
    "            rPH = self.rPH\n",
    "            rB  = self.rB\n",
    "            d   = self.d\n",
    "\n",
    "            # N.B. U tensors are not learned, they are discarded as intermediary steps for compression\n",
    "            # TCR-TCR (L_T, L_T, rB) -> (rT, rT, rB)\n",
    "            if L_T > 0:\n",
    "                U_TT = torch.einsum('ip,ijr->pjr', A_T_b, Y_TT) # (rT, L_T, rB)\n",
    "                V_TT = torch.einsum('pjr,jq->pqr', U_TT, A_T_b) # (rT, rT, rB)\n",
    "            else:\n",
    "                V_TT = torch.zeros(rT, rT, rB, device=device) # (rT, rT, rB)\n",
    "\n",
    "            # TCR–pMHC: (L_T, L_PH, rB) -> (rT, rPH, rB)\n",
    "            if L_T > 0 and L_PH > 0:\n",
    "                U_TPH = torch.einsum('ip,ijr->pjr', A_T_b,  Y_TPH)   # (rT,  L_PH, rB)\n",
    "                V_TPH = torch.einsum('pjr,jq->pqr', U_TPH, A_PH_b)   # (rT,  rPH, rB)\n",
    "            else:\n",
    "                V_TPH = torch.zeros(rT, rPH, rB, device=device)\n",
    "\n",
    "            # pMHC–TCR: (L_PH, L_T, rB) -> (rPH, rT, rB)\n",
    "            if L_T > 0 and L_PH > 0:\n",
    "                U_PHT = torch.einsum('ip,ijr->pjr', A_PH_b, Y_PHT)   # (rPH, L_T,  rB)\n",
    "                V_PHT = torch.einsum('pjr,jq->pqr', U_PHT, A_T_b)    # (rPH, rT,  rB)\n",
    "            else:\n",
    "                V_PHT = torch.zeros(rPH, rT, rB, device=device)\n",
    "\n",
    "            # pMHC–pMHC: (L_PH, L_PH, rB) -> (rPH, rPH, rB)\n",
    "            if L_PH > 0:\n",
    "                U_PHPH = torch.einsum('ip,ijr->pjr', A_PH_b, Y_PHPH) # (rPH, L_PH, rB)\n",
    "                V_PHPH = torch.einsum('pjr,jq->pqr', U_PHPH, A_PH_b) # (rPH, rPH, rB)\n",
    "            else:\n",
    "                V_PHPH = torch.zeros(rPH, rPH, rB, device=device)\n",
    "\n",
    "            # ---- 8) Flatten factorised blocks and map to d×d via H_* ----\n",
    "            v_TT_flat   = V_TT.reshape(-1)    # (rT*rT*rB,)\n",
    "            v_TPH_flat  = V_TPH.reshape(-1)   # (rT*rPH*rB,)\n",
    "            v_PHT_flat  = V_PHT.reshape(-1)   # (rPH*rT*rB,)\n",
    "            v_PHPH_flat = V_PHPH.reshape(-1)  # (rPH*rPH*rB,)\n",
    "\n",
    "            k_TT_flat   = v_TT_flat   @ self.H_TT   # (d*d,)\n",
    "            k_TPH_flat  = v_TPH_flat  @ self.H_TPH  # (d*d,)\n",
    "            k_PHT_flat  = v_PHT_flat  @ self.H_PHT  # (d*d,)\n",
    "            k_PHPH_flat = v_PHPH_flat @ self.H_PHPH # (d*d,)\n",
    "\n",
    "            K_TT   = k_TT_flat.view(d, d)\n",
    "            K_TPH  = k_TPH_flat.view(d, d)\n",
    "            K_PHT  = k_PHT_flat.view(d, d)\n",
    "            K_PHPH = k_PHPH_flat.view(d, d)\n",
    "\n",
    "            # Optional: enforce symmetry on diagonal blocks\n",
    "            # Enforce symmetry on all blocks?\n",
    "            K_TT   = 0.5 * (K_TT   + K_TT.t())\n",
    "            K_PHPH = 0.5 * (K_PHPH + K_PHPH.t())\n",
    "\n",
    "            # ---- 9) Assemble 2d x 2d operator for this sample ----\n",
    "            I_d = torch.eye(d, device=device)\n",
    "            Zstar_b = torch.zeros(2*d, 2*d, device=device)\n",
    "\n",
    "            Zstar_b[:d,  :d]  = I_d + K_TT\n",
    "            Zstar_b[:d,  d:]  = I_d + K_TPH\n",
    "            Zstar_b[d:,  :d]  = I_d + K_PHT\n",
    "            Zstar_b[d:,  d:]  = I_d + K_PHPH\n",
    "\n",
    "            Zstar_list.append(Zstar_b)\n",
    "\n",
    "        Zstar_batch = torch.stack(Zstar_list, dim=0)  # (B, 2d, 2d)\n",
    "        return Zstar_batch \n",
    "\n",
    "\n",
    "# index convention for the einsum\n",
    "# i = row token position, “i” is historically “index” or “first axis”\n",
    "# j\t= column token position, second positional axis (matrix-like)\n",
    "# c\t= channel\n",
    "# p, q\t= latent positional modes, P = projection / latent position\n",
    "# r = latent channel modes, R = rank / channel rank\n",
    "# b\t= batch index, B = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d7b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting it all together\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafcf2cf",
   "metadata": {},
   "source": [
    "#### Old/Unused Code + Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337c46c0",
   "metadata": {},
   "source": [
    "##### Masking/Encoder Fine-tuning Notes\n",
    "1. Model selection & freeze policy\n",
    "- Load ESM-C base.\n",
    "- Freeze embeddings + lower N blocks; leave top K blocks trainable (or use adapters/LoRA). Record N, K.\n",
    "2. Two forwards you will run\n",
    "- Masked forward (for loss): feed masked_sequences → get logits over vocab (B, L, V).\n",
    "- Clean forward (for caching embeddings): feed sequences → get token_hidden_states (B, L, d_model).\n",
    "3. Loss and metrics\n",
    "- Compute token-level cross-entropy on logits vs labels (ignore_index = −100).\n",
    "- Track masked-token accuracy and perplexity per modality.\n",
    "5. Optimisation\n",
    "- AdamW; no weight decay on LayerNorm/bias; gradient clip ~1.0; fp16/bf16 if available.\n",
    "- LR schedule: warm-up then cosine/linear decay.\n",
    "- Validation sets per modality with identical masking policy.\n",
    "6. Checkpoints\n",
    "- Save model weights + adapter/LoRA if used, optimizer, scheduler, tokenizer hash, mask rate, freeze policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f0263053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before model loading:\n",
      "\n",
      "GPU Memory Usage:\n",
      "Allocated: 1168.29 MB\n",
      "Cached: 1272.00 MB\n",
      "Total GPU Memory: 16376.00 MB\n",
      "Available: 15104.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Check GPU memory\n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\nGPU Memory Usage:\")\n",
    "        print(f\"Allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "        print(f\"Cached: {torch.cuda.memory_reserved()/1024**2:.2f} MB\")\n",
    "        \n",
    "        # Get total GPU memory\n",
    "        import subprocess\n",
    "        result = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.total', '--format=csv,nounits,noheader'])\n",
    "        total_memory = float(result.decode('utf-8').strip())\n",
    "        print(f\"Total GPU Memory: {total_memory:.2f} MB\")\n",
    "        print(f\"Available: {total_memory - torch.cuda.memory_reserved()/1024**2:.2f} MB\")\n",
    "\n",
    "# Check memory before model loading\n",
    "print(\"Before model loading:\")\n",
    "print_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1754784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not necessary, ESM C has their own much more complex attention layers\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, num_heads=4):\n",
    "        \"\"\"\n",
    "        hidden_dim: Dimensionality of the input\n",
    "        num_heads: Number of attention heads to split the input into\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n",
    "        self.Wv = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Value part\n",
    "        self.Wk = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Key part\n",
    "        self.Wq = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Query part\n",
    "        self.Wo = nn.Linear(hidden_dim, hidden_dim, bias=False) # the output part\n",
    "\n",
    "    def check_sdpa_inputs(self, x):\n",
    "        assert x.size(1) == self.num_heads, f\"Expected shape (-1, {self.num_heads}, -1, {self.hidden_dim // self.num_heads}), got {tuple(x.size())}\"\n",
    "        assert x.size(3) == self.hidden_dim // self.num_heads\n",
    "\n",
    "    def scaled_dot_product_attention(self,\n",
    "            query,\n",
    "            key,\n",
    "            value,\n",
    "            attention_mask=None,\n",
    "            key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        query: tensor of shape (batch_size, num_heads, query_sequence_length, hidden_dim // num_heads)\n",
    "        key: tensor of shape (batch_size, num_heads, key_sequence_length, hidden_dim // num_heads)\n",
    "        value: tensor of shape (batch_size, num_heads, value_sequence_length, hidden_dim // num_heads)\n",
    "        attention_mask: tensor of shape (query_sequence_length, key_sequence_length)\n",
    "        key_padding_mask: tensor of shape (sequence_length, key_sequence_length)\n",
    "        \"\"\"\n",
    "        self.check_sdpa_inputs(query)\n",
    "        self.check_sdpa_inputs(key)\n",
    "        self.check_sdpa_inputs(value)\n",
    "\n",
    "        d_k = query.size(-1) #size of the last dimension of the query tensor\n",
    "        tgt_len, src_len = query.size(-2), key.size(-2)\n",
    "        #logits = (B, H, tgt_len, E) * (B, H, E, src_len) = (B, H, tgt_len, src_len)\n",
    "        logits = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        \n",
    "        # Attention mask\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.dim() == 2:\n",
    "                assert attention_mask.size() == (tgt_len, src_len)\n",
    "                attention_mask = attention_mask.unsqueeze(0)\n",
    "                logits = logits + attention_mask\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid size of attention_mask: {attention_mask.size()}\")\n",
    "\n",
    "        # Key padding mask\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2) # Broadcast over batch size, num heads\n",
    "            logits = logits + key_padding_mask\n",
    "\n",
    "        attention = torch.softmax(logits, dim=-1)\n",
    "        output = torch.matmul(attention, value) # (batch_size, num_heads, sequence_length, hidden_dim)\n",
    "\n",
    "        return output, attention\n",
    "\n",
    "    def split_into_heads(self, x, num_heads):\n",
    "        batch_size, seq_length, hidden_dim = x.size()\n",
    "        x = x.view(batch_size, seq_length, num_heads, hidden_dim // num_heads)\n",
    "\n",
    "        return x.transpose(1, 2) # (batch_size, num_heads, seq_length, hidden_dim // num_heads)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, num_heads, seq_length, head_hidden_dim = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, num_heads * head_hidden_dim)\n",
    "    \n",
    "    def forward(\n",
    "            self,\n",
    "            q,\n",
    "            k,\n",
    "            v,\n",
    "            attention_mask=None,\n",
    "            key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        q: tensor of shape (batch_size, query_sequence_length, hidden_dim)\n",
    "        k: tensor of shape (batch_size, key_sequence_length, hidden_dim)\n",
    "        v: tensor of shape (batch_size, value_sequence_length, hidden_dim)\n",
    "        attention_mask: tensor of shape (query_sequence_length, key_sequence_length)\n",
    "        key_padding_mask: tensor of shape (sequence_length, key_sequence_length)\n",
    "        \"\"\"\n",
    "        q = self.Wq(q)\n",
    "        k = self.Wk(k)\n",
    "        v = self.Wv(v)\n",
    "\n",
    "        q = self.split_into_heads(q, self.num_heads) # (batch_size, num_heads, query_sequence_length, hidden_dim // num_heads)\n",
    "        k = self.split_into_heads(k, self.num_heads) # (batch_size, num_heads, key_sequence_length, hidden_dim // num_heads)\n",
    "        v = self.split_into_heads(v, self.num_heads) # (batch_size, num_heads, value_sequence_length, hidden_dim // num_heads)\n",
    "        \n",
    "        attn_values, attn_weights = self.scaled_dot_product_attention(\n",
    "            query=q,\n",
    "            key=k,\n",
    "            value=v,\n",
    "            attention_mask=attention_mask,\n",
    "            key_padding_mask=key_padding_mask\n",
    "        )\n",
    "        grouped = self.combine_heads(attn_values)\n",
    "        output = self.Wo(grouped)\n",
    "\n",
    "        self.attention_weights = attn_weights\n",
    "\n",
    "        return output\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49d4881",
   "metadata": {},
   "outputs": [],
   "source": [
    "### masking for one sequence\n",
    "\n",
    "# # tokenize sequences\n",
    "# clean_tcrs_tokenized = model.tokenizer(tcrs_data, return_tensors='pt', padding=True)\n",
    "# clean_peptides_tokenized = model.tokenizer(peptides_data, return_tensors='pt', padding=True)\n",
    "# clean_hlas_tokenized = model.tokenizer(hlas_data, return_tensors='pt', padding=True)\n",
    "\n",
    "# # clean_tcrs_tokenized.keys()\n",
    "# # ids = clean_tcrs_tokenized['input_ids']\n",
    "# # attention_mask = clean_tcrs_tokenized['attention_mask']\n",
    "\n",
    "# #copies for masking\n",
    "# masked_tcrs_tokenized = clean_tcrs_tokenized.copy()\n",
    "# masked_peptides_tokenized = clean_peptides_tokenized.copy()\n",
    "# masked_hlas_tokenized = clean_hlas_tokenized.copy()\n",
    "\n",
    "# # special tokens and amino acids\n",
    "# CLS = model.tokenizer.cls_token_id\n",
    "# EOS = model.tokenizer.eos_token_id\n",
    "# PAD = model.tokenizer.pad_token_id\n",
    "# amino_acids = torch.tensor([5, 10, 17, 13, 23, 16, 9, 6, 21, 12, 4, 15, 20, 18, 14, 8, 11, 22, 19, 7], device=ids.device)\n",
    "# mask_token = 32\n",
    "\n",
    "# # beginning of per sequence masking\n",
    "# # IDS\n",
    "# ids = masked_tcrs_tokenized['input_ids'][0]\n",
    "# valid_mask = (ids != CLS) & (ids != EOS) & (ids != PAD)\n",
    "# valid_idx = valid_mask.nonzero(as_tuple=False).squeeze(1)\n",
    "\n",
    "# #L_valid = valid_mask.sum()\n",
    "# L_valid = valid_idx.numel()\n",
    "# if L_valid == 0:\n",
    "#     # nothing to mask, make labels -100 to be ignored in the loss\n",
    "#     masked_ids = ids.clone()\n",
    "#     labels = torch.full_like(ids, -100)\n",
    "# else:\n",
    "#     # L_valid = valid_mask.sum(dim=1) # for batch, but currently only 1 dimension\n",
    "#     n = torch.floor(0.15 * torch.tensor(L_valid, device=ids.device, dtype=torch.float32)).to(torch.int64)\n",
    "#     n = n.clamp(min=2, max=min(45, L_valid))\n",
    "#     # n = torch.floor(\n",
    "#     #         0.15 * torch.tensor(L_valid, device=ids.device, dtype=torch.float32)\n",
    "#     #         ).to(torch.int64).clamp(min=2, max=45)\n",
    "    \n",
    "#     # choose n distinct valid positions \n",
    "#     # reshuffle\n",
    "#     perm = torch.randperm(L_valid, device=ids.device)\n",
    "#     # choose first n\n",
    "#     chosen_local = perm[:n]\n",
    "#     # get correct indices based off of valid indices\n",
    "#     chosen = valid_idx[chosen_local]\n",
    "\n",
    "#     # 4) split into amino acids and mask tokens\n",
    "#     n_amino = torch.floor(0.2 * n).to(torch.int64).clamp(min=1)\n",
    "#     n_mask = n - n_amino\n",
    "\n",
    "#     # 5) shuffle and create positions\n",
    "#     # need to shuffle as otherwise will always have the first part being masked with the mask token\n",
    "#     # and the same goes for the amino acids, always masked at the second part of the sequence\n",
    "#     shuffle   = torch.randperm(n.item(), device=ids.device)\n",
    "#     mask_pos  = chosen[shuffle[:n_mask]]\n",
    "#     amino_pos = chosen[shuffle[n_mask:]]\n",
    "\n",
    "#     # 6) build labels - originals at selected positions, -100 for non-masked positions\n",
    "#     masked_ids = ids.clone()\n",
    "#     labels     = torch.full_like(ids, -100)\n",
    "#     labels[chosen] = ids[chosen]\n",
    "\n",
    "#     # 7) apply mask\n",
    "#     masked_ids[mask_pos] = mask_token\n",
    "\n",
    "#     # 8) assign amino acids masks from random amino acids\n",
    "#     if n_amino > 0:\n",
    "#         rand_idx = torch.randint(high=amino_acids.numel(), size=(n_amino.item(),), device=ids.device)\n",
    "#         masked_ids[amino_pos] = amino_acids[rand_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975843ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "\n",
    "# for batch in tcr_loader:\n",
    "#     batched = batch[\"masked_input_ids_ESMprotein_batched\"]\n",
    "#     batched = batched.to(device)\n",
    "#     labels = batch['labels'].to(device)\n",
    "\n",
    "#     logits_output = model_tcr.logits(\n",
    "#         batched, LogitsConfig(sequence=True, return_embeddings=True)\n",
    "#         )\n",
    "    \n",
    "#     logits_s = logits_output.logits.sequence\n",
    "#     loss = F.cross_entropy(\n",
    "#         logits_s.view(-1, logits_s.size(-1)), # [B*L, V]\n",
    "#         labels.view(-1),                      # [B*L]\n",
    "#         ignore_index=-100\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f28783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to print the modules within linear layers to find what to add Lora to\n",
    "\n",
    "for name, mod in base.named_modules():\n",
    "    if isinstance(mod, torch.nn.Linear):\n",
    "        print(name, mod.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794e2e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "num tokens: 33\n",
      "sample tokens: ['O', 'W', '-', 'S', 'T', '<eos>', 'G', 'B', 'K', '<pad>', 'H', 'M', 'L', 'R', '<mask>', 'A', 'F', 'U', 'Z', '<unk>', 'P', 'X', 'N', 'Q', 'C', 'I', 'V', 'E', '<cls>', 'D', '|', 'Y', '.']\n",
      "bos_token <cls>\n",
      "eos_token <eos>\n",
      "unk_token <unk>\n",
      "pad_token <pad>\n",
      "mask_token <mask>\n",
      "ids: [0, 5, 10, 17, 13, 23, 16, 9, 6, 21, 12, 4, 15, 20, 18, 14, 8, 11, 22, 19, 7, 1, 32, 3, 2]\n",
      "decoded: <cls> A R N D C Q E G H I L K M F P S T W Y V <pad> <mask> <unk> <eos>\n",
      "ForwardTrackData(sequence=tensor([[[-36.7500, -36.7500, -36.7500,  11.6250,  18.7500,  19.1250,  19.1250,\n",
      "           18.7500,  18.7500,  18.7500,  18.8750,  18.7500,  18.1250,  18.3750,\n",
      "           18.7500,  18.2500,  18.1250,  18.0000,  17.6250,  17.8750,  18.5000,\n",
      "           17.6250,  17.3750,  17.1250,  18.1250,  -0.1055,  -3.7031,  -3.7656,\n",
      "          -20.0000, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
      "          -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
      "          -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
      "          -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
      "          -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
      "          -36.7500],\n",
      "         [-34.7500, -34.7500, -34.7500,  16.8750,  23.5000,  24.0000,  23.2500,\n",
      "           23.6250,  23.8750,  22.0000,  23.5000,  23.6250,  22.6250,  22.1250,\n",
      "           23.5000,  22.0000,  22.1250,  22.1250,  22.2500,  21.7500,  24.6250,\n",
      "           21.8750,  21.8750,  22.5000,  16.0000,   0.3145,  -0.2715,  -1.5781,\n",
      "          -20.0000, -35.0000, -34.7500, -34.7500, -35.0000, -35.0000, -35.0000,\n",
      "          -34.7500, -35.0000, -34.7500, -34.7500, -34.7500, -34.7500, -35.0000,\n",
      "          -35.0000, -34.7500, -34.7500, -35.0000, -35.0000, -34.7500, -35.0000,\n",
      "          -35.0000, -35.0000, -34.7500, -35.0000, -35.0000, -35.0000, -34.7500,\n",
      "          -34.7500, -34.7500, -35.0000, -34.7500, -35.0000, -35.0000, -35.0000,\n",
      "          -35.0000],\n",
      "         [-32.5000, -32.5000, -32.5000,  20.1250,  26.5000,  27.2500,  26.7500,\n",
      "           26.5000,  27.1250,  25.7500,  27.0000,  27.0000,  26.0000,  25.8750,\n",
      "           26.5000,  25.3750,  25.5000,  25.6250,  25.5000,  25.2500,  25.2500,\n",
      "           25.3750,  25.3750,  25.8750,  19.0000,  -0.1904,  -0.1445,  -0.6562,\n",
      "          -17.1250, -32.5000, -32.5000, -32.2500, -32.5000, -32.5000, -32.5000,\n",
      "          -32.5000, -32.5000, -32.2500, -32.2500, -32.5000, -32.5000, -32.5000,\n",
      "          -32.5000, -32.2500, -32.5000, -32.5000, -32.5000, -32.5000, -32.5000,\n",
      "          -32.5000, -32.5000, -32.2500, -32.5000, -32.5000, -32.5000, -32.5000,\n",
      "          -32.5000, -32.5000, -32.5000, -32.2500, -32.5000, -32.5000, -32.5000,\n",
      "          -32.5000],\n",
      "         [-33.2500, -33.2500, -33.2500,  19.0000,  23.7500,  25.1250,  24.2500,\n",
      "           23.8750,  24.8750,  22.8750,  23.8750,  24.8750,  23.5000,  23.2500,\n",
      "           24.2500,  22.5000,  22.5000,  23.1250,  22.7500,  22.5000,  22.7500,\n",
      "           22.2500,  22.0000,  22.8750,  17.0000,   3.0625,   3.4688,   3.6875,\n",
      "          -17.7500, -33.2500, -33.2500, -33.2500, -33.2500, -33.2500, -33.2500,\n",
      "          -33.2500, -33.2500, -33.2500, -33.2500, -33.2500, -33.2500, -33.2500,\n",
      "          -33.2500, -33.2500, -33.2500, -33.2500, -33.2500, -33.2500, -33.2500,\n",
      "          -33.2500, -33.2500, -33.2500, -33.2500, -33.2500, -33.2500, -33.2500,\n",
      "          -33.2500, -33.2500, -33.2500, -33.0000, -33.2500, -33.2500, -33.2500,\n",
      "          -33.2500],\n",
      "         [-31.3750, -31.3750, -31.3750,  20.8750,  27.5000,  28.6250,  27.7500,\n",
      "           27.3750,  28.3750,  26.7500,  27.8750,  28.1250,  26.7500,  26.5000,\n",
      "           27.7500,  26.3750,  26.6250,  26.2500,  26.3750,  25.7500,  26.1250,\n",
      "           26.1250,  26.2500,  26.6250,  20.6250,   1.6016,  -0.9922,   2.4062,\n",
      "          -15.5000, -31.3750, -31.3750, -31.3750, -31.3750, -31.5000, -31.3750,\n",
      "          -31.3750, -31.3750, -31.3750, -31.3750, -31.3750, -31.3750, -31.3750,\n",
      "          -31.3750, -31.3750, -31.3750, -31.3750, -31.3750, -31.3750, -31.3750,\n",
      "          -31.3750, -31.5000, -31.3750, -31.3750, -31.5000, -31.3750, -31.3750,\n",
      "          -31.3750, -31.3750, -31.3750, -31.3750, -31.3750, -31.3750, -31.3750,\n",
      "          -31.3750],\n",
      "         [-32.2500, -32.2500, -32.2500,  18.6250,  24.5000,  25.1250,  24.5000,\n",
      "           24.3750,  25.1250,  23.5000,  24.7500,  24.7500,  24.1250,  23.6250,\n",
      "           24.2500,  23.6250,  23.5000,  23.7500,  23.7500,  23.2500,  23.1250,\n",
      "           23.3750,  23.1250,  24.0000,  16.7500,   1.6172,   2.1719,   0.9414,\n",
      "          -17.2500, -32.2500, -32.2500, -32.2500, -32.2500, -32.2500, -32.2500,\n",
      "          -32.2500, -32.2500, -32.2500, -32.2500, -32.2500, -32.2500, -32.2500,\n",
      "          -32.2500, -32.2500, -32.2500, -32.2500, -32.2500, -32.2500, -32.2500,\n",
      "          -32.2500, -32.2500, -32.2500, -32.2500, -32.2500, -32.2500, -32.2500,\n",
      "          -32.2500, -32.2500, -32.2500, -32.2500, -32.2500, -32.2500, -32.2500,\n",
      "          -32.2500],\n",
      "         [-31.8750, -31.8750, -31.8750,  21.0000,  26.1250,  26.8750,  26.1250,\n",
      "           26.0000,  26.6250,  25.5000,  26.5000,  26.6250,  25.3750,  25.3750,\n",
      "           26.1250,  25.1250,  25.1250,  25.0000,  25.1250,  24.6250,  24.7500,\n",
      "           24.8750,  24.7500,  25.1250,  18.8750,   3.1875,  -0.7930,   1.8594,\n",
      "          -16.8750, -32.0000, -31.8750, -31.8750, -31.8750, -32.0000, -31.8750,\n",
      "          -31.8750, -32.0000, -31.8750, -31.8750, -31.8750, -31.8750, -31.8750,\n",
      "          -31.8750, -31.8750, -31.8750, -31.8750, -32.0000, -31.8750, -31.8750,\n",
      "          -31.8750, -32.0000, -31.8750, -31.8750, -32.0000, -31.8750, -31.8750,\n",
      "          -31.8750, -31.8750, -31.8750, -31.8750, -32.0000, -32.0000, -32.0000,\n",
      "          -31.8750],\n",
      "         [-32.0000, -32.0000, -32.0000,  20.0000,  25.5000,  26.5000,  25.7500,\n",
      "           25.5000,  26.2500,  24.7500,  26.1250,  26.0000,  25.0000,  24.7500,\n",
      "           25.8750,  24.6250,  24.6250,  24.5000,  24.5000,  24.1250,  24.2500,\n",
      "           24.2500,  24.2500,  24.6250,  17.0000,  -0.2520,  -0.7344,  -0.1436,\n",
      "          -17.0000, -32.0000, -32.0000, -32.0000, -32.0000, -32.0000, -32.0000,\n",
      "          -32.0000, -32.0000, -32.0000, -32.0000, -32.0000, -32.0000, -32.0000,\n",
      "          -32.0000, -32.0000, -32.0000, -32.0000, -32.0000, -32.0000, -32.0000,\n",
      "          -32.0000, -32.0000, -32.0000, -32.0000, -32.0000, -32.0000, -32.0000,\n",
      "          -32.0000, -32.0000, -32.0000, -32.0000, -32.0000, -32.0000, -32.0000,\n",
      "          -32.0000],\n",
      "         [-32.5000, -32.5000, -32.5000,  19.3750,  25.0000,  25.7500,  25.1250,\n",
      "           24.8750,  25.2500,  24.1250,  25.2500,  25.0000,  24.1250,  24.1250,\n",
      "           25.1250,  23.7500,  24.0000,  23.5000,  23.7500,  23.1250,  23.5000,\n",
      "           23.6250,  23.3750,  23.7500,  17.2500,   2.1719,   1.1719,   0.8320,\n",
      "          -17.3750, -32.5000, -32.5000, -32.5000, -32.5000, -32.5000, -32.5000,\n",
      "          -32.5000, -32.5000, -32.5000, -32.5000, -32.5000, -32.5000, -32.5000,\n",
      "          -32.5000, -32.5000, -32.5000, -32.5000, -32.5000, -32.5000, -32.5000,\n",
      "          -32.5000, -32.5000, -32.5000, -32.5000, -32.5000, -32.5000, -32.5000,\n",
      "          -32.5000, -32.5000, -32.5000, -32.5000, -32.5000, -32.5000, -32.5000,\n",
      "          -32.5000],\n",
      "         [-33.2500, -33.2500, -33.2500,  18.7500,  24.8750,  25.7500,  25.1250,\n",
      "           24.6250,  25.5000,  24.0000,  25.3750,  25.2500,  24.1250,  24.0000,\n",
      "           25.1250,  24.0000,  23.8750,  23.7500,  23.7500,  23.1250,  23.6250,\n",
      "           23.5000,  23.6250,  24.0000,  16.1250,  -0.4785,   0.4727,  -1.9141,\n",
      "          -18.1250, -33.2500, -33.2500, -33.2500, -33.2500, -33.5000, -33.2500,\n",
      "          -33.2500, -33.2500, -33.2500, -33.2500, -33.2500, -33.2500, -33.2500,\n",
      "          -33.2500, -33.2500, -33.2500, -33.2500, -33.2500, -33.2500, -33.2500,\n",
      "          -33.2500, -33.5000, -33.2500, -33.2500, -33.5000, -33.2500, -33.2500,\n",
      "          -33.2500, -33.2500, -33.2500, -33.2500, -33.2500, -33.2500, -33.2500,\n",
      "          -33.2500],\n",
      "         [-33.5000, -33.5000, -33.5000,  19.0000,  26.6250,  27.5000,  26.8750,\n",
      "           26.5000,  27.2500,  25.7500,  27.2500,  27.1250,  26.1250,  26.0000,\n",
      "           27.0000,  25.6250,  25.6250,  25.8750,  25.7500,  25.2500,  25.2500,\n",
      "           25.6250,  25.5000,  25.8750,  18.7500,  -2.2500,  -1.6328,  -2.8906,\n",
      "          -18.0000, -33.5000, -33.5000, -33.5000, -33.5000, -33.5000, -33.5000,\n",
      "          -33.5000, -33.5000, -33.5000, -33.5000, -33.5000, -33.5000, -33.5000,\n",
      "          -33.5000, -33.5000, -33.5000, -33.5000, -33.5000, -33.5000, -33.5000,\n",
      "          -33.5000, -33.5000, -33.5000, -33.5000, -33.5000, -33.5000, -33.5000,\n",
      "          -33.5000, -33.5000, -33.5000, -33.5000, -33.5000, -33.5000, -33.5000,\n",
      "          -33.5000],\n",
      "         [-36.2500, -36.2500, -36.2500,  22.8750,  19.2500,  20.1250,  19.3750,\n",
      "           19.1250,  19.5000,  18.1250,  19.3750,  19.5000,  18.1250,  18.0000,\n",
      "           19.3750,  17.8750,  17.7500,  17.6250,  18.0000,  17.5000,  18.0000,\n",
      "           17.6250,  17.6250,  18.6250,  13.0625,   4.9688,   5.0312,  -0.1738,\n",
      "          -24.8750, -36.5000, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500,\n",
      "          -36.2500, -36.5000, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500,\n",
      "          -36.5000, -36.2500, -36.2500, -36.2500, -36.5000, -36.2500, -36.2500,\n",
      "          -36.5000, -36.5000, -36.2500, -36.2500, -36.5000, -36.2500, -36.5000,\n",
      "          -36.2500, -36.2500, -36.5000, -36.2500, -36.5000, -36.2500, -36.5000,\n",
      "          -36.2500],\n",
      "         [-33.5000, -33.5000, -33.5000,  23.0000,  23.2500,  24.1250,  23.6250,\n",
      "           23.1250,  23.8750,  22.7500,  23.8750,  23.6250,  22.6250,  22.5000,\n",
      "           23.7500,  22.6250,  22.5000,  22.5000,  22.0000,  22.1250,  21.8750,\n",
      "           22.2500,  21.8750,  22.5000,  17.8750,   2.5000,   3.1406,   1.2500,\n",
      "          -19.8750, -33.5000, -33.5000, -33.5000, -33.5000, -33.5000, -33.5000,\n",
      "          -33.5000, -33.5000, -33.5000, -33.5000, -33.5000, -33.5000, -33.5000,\n",
      "          -33.5000, -33.5000, -33.5000, -33.5000, -33.5000, -33.5000, -33.5000,\n",
      "          -33.5000, -33.5000, -33.5000, -33.5000, -33.5000, -33.5000, -33.5000,\n",
      "          -33.5000, -33.5000, -33.5000, -33.5000, -33.5000, -33.5000, -33.5000,\n",
      "          -33.5000]]], device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None) torch.Size([1, 13, 960])\n"
     ]
    }
   ],
   "source": [
    "print(getattr(tok, \"vocab_size\", None))\n",
    "\n",
    "get_vocab = getattr(tok, \"get_vocab\", None)\n",
    "if callable(get_vocab):\n",
    "    vocab = get_vocab()\n",
    "    print(\"num tokens:\", len(vocab))\n",
    "    print(\"sample tokens:\", list(vocab.keys())[:50])\n",
    "\n",
    "# Special tokens (common names; guarded)\n",
    "for name in [\"bos_token\", \"eos_token\", \"unk_token\", \"pad_token\", \"mask_token\"]:\n",
    "    #print(name, getattr(tok, name, None))\n",
    "\n",
    "\n",
    "seq = \"ARNDCQEGHILKMFPSTWYV<pad><mask><unk>\"\n",
    "ids = tok.encode(seq) if hasattr(tok, \"encode\") else None\n",
    "#print(\"ids:\", ids)\n",
    "if ids is not None and hasattr(tok, \"decode\"):\n",
    "    #print(\"decoded:\", tok.decode(ids))\n",
    "\n",
    "\n",
    "# load base protein language model (esm2 8M params, 6 layers) with specified PEFT methods. See the esm2 repo for more model size options\n",
    "\n",
    "protein = ESMProtein(sequence=\"AACGTATTTA<unk>\")\n",
    "model = ESMC.from_pretrained(\"esmc_300m\").to(\"cuda\") # or \"cpu\"\n",
    "protein_tensor = model.encode(protein)\n",
    "logits_output = model.logits(\n",
    "   protein_tensor, LogitsConfig(sequence=True, return_embeddings=True)\n",
    ")\n",
    "print(logits_output.logits, logits_output.embeddings.size())\n",
    "#protein_batch_converter = protein_LM_alphabet.get_batch_converter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1818dc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_detokenize', '_tokenize', 'tokenizer']\n",
      "<class 'esm.tokenization.sequence_tokenizer.EsmSequenceTokenizer'>\n",
      "['SPECIAL_TOKENS_ATTRIBUTES', 'add_prefix_space', 'add_special_tokens', 'add_tokens', 'added_tokens_decoder', 'added_tokens_encoder', 'all_special_ids', 'all_special_tokens', 'all_special_tokens_extended', 'all_token_ids', 'apply_chat_template', 'as_target_tokenizer', 'backend_tokenizer', 'batch_decode', 'batch_encode_plus', 'bos_token', 'bos_token_id', 'build_inputs_with_special_tokens', 'can_save_slow_tokenizer', 'cb_token', 'chain_break_token', 'chain_break_token_id', 'chat_template', 'clean_up_tokenization', 'clean_up_tokenization_spaces', 'cls_token', 'cls_token_id', 'convert_added_tokens', 'convert_ids_to_tokens', 'convert_tokens_to_ids', 'convert_tokens_to_string', 'create_token_type_ids_from_sequences', 'decode', 'decoder', 'deprecation_warnings', 'encode', 'encode_plus', 'eos_token', 'eos_token_id', 'extra_special_tokens', 'from_pretrained', 'get_added_vocab', 'get_chat_template', 'get_special_tokens_mask', 'get_vocab', 'init_inputs', 'init_kwargs', 'is_fast', 'mask_token', 'mask_token_id', 'max_len_sentences_pair', 'max_len_single_sentence', 'model_input_names', 'model_max_length', 'name_or_path', 'num_special_tokens_to_add', 'pad', 'pad_token', 'pad_token_id', 'pad_token_type_id', 'padding_side', 'prepare_for_model', 'prepare_seq2seq_batch', 'pretrained_vocab_files_map', 'push_to_hub', 'register_for_auto_class', 'sanitize_special_tokens', 'save_pretrained', 'save_vocabulary', 'set_truncation_and_padding', 'slow_tokenizer_class', 'special_token_ids', 'special_tokens_map', 'special_tokens_map_extended', 'split_special_tokens', 'tokenize', 'train_new_from_iterator', 'truncate_sequences', 'truncation_side', 'verbose', 'vocab', 'vocab_files_names', 'vocab_size']\n",
      "<bound method Module.named_parameters of ESMC(\n",
      "  (embed): Embedding(64, 960)\n",
      "  (transformer): TransformerStack(\n",
      "    (blocks): ModuleList(\n",
      "      (0-29): 30 x UnifiedTransformerBlock(\n",
      "        (attn): MultiHeadAttention(\n",
      "          (layernorm_qkv): Sequential(\n",
      "            (0): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=960, out_features=2880, bias=False)\n",
      "          )\n",
      "          (out_proj): Linear(in_features=960, out_features=960, bias=False)\n",
      "          (q_ln): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
      "          (k_ln): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
      "          (rotary): RotaryEmbedding()\n",
      "        )\n",
      "        (ffn): Sequential(\n",
      "          (0): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=960, out_features=5120, bias=False)\n",
      "          (2): SwiGLU()\n",
      "          (3): Linear(in_features=2560, out_features=960, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (sequence_head): Sequential(\n",
      "    (0): Linear(in_features=960, out_features=960, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Linear(in_features=960, out_features=64, bias=True)\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# from esm.models.esmc import ESMC\n",
    "# model = ESMC.from_pretrained(\"esmc_300m\").eval()\n",
    "\n",
    "# print([a for a in dir(model) if \"token\" in a.lower()])\n",
    "# # Common: 'tokenizer' shows up; then introspect it:\n",
    "# tok = getattr(model, \"tokenizer\", None)\n",
    "# print(type(tok))\n",
    "# print([a for a in dir(tok) if not a.startswith(\"_\")])\n",
    "\n",
    "# # use model's input embeddings to get the vocab \n",
    "# print(model.named_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d975d6b2",
   "metadata": {},
   "source": [
    "###### Raw code before making into a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf8e4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and defining dimensions + projection heads\n",
    "# N.B. Turned this into a class \n",
    "\n",
    "# Shape\n",
    "B, L_T_pad, D = emb_T.shape\n",
    "\n",
    "# Latent ranks and final dimension (hyperparameters)\n",
    "rL_T = 8      # positional rank for TCR (tunable)\n",
    "rD_T = 16     # channel rank for TCR (tunable)\n",
    "d    = 128    # final latent dimension (same d as in Z*)\n",
    "\n",
    "# ratio of peptide to HLA (hyperparameter)\n",
    "R = 0.7\n",
    "# Epsilon for numerical stability\n",
    "eps=1e-8\n",
    "\n",
    "# maximum true lenghts of the sequences\n",
    "# maximum true tcr length \n",
    "L_T_true = tcr_mask.sum(dim=1)\n",
    "L_T_max = L_T_true.max()\n",
    "# maximum true peptide length \n",
    "L_P_true = pep_mask.sum(dim=1)\n",
    "L_P_max = L_P_true.max()\n",
    "# maximum true HLA length \n",
    "L_H_true = hla_mask.sum(dim=1)\n",
    "L_H_max = L_H_true.max()\n",
    "\n",
    "# Factorisation Matrices\n",
    "\n",
    "# Channel mixing D -> rD_T\n",
    "B_Tc = torch.nn.Parameter(torch.empty(D, rD_T, device=device))\n",
    "torch.nn.init.xavier_uniform_(B_Tc)\n",
    "\n",
    "# Positional mixing: positions 0..L_T_max-1 -> rL_T\n",
    "A_Tc = torch.nn.Parameter(torch.empty(L_T_max, rL_T, device=device))\n",
    "torch.nn.init.xavier_uniform_(A_Tc)\n",
    "\n",
    "# Final map: flattened projections (rD_T * rL_T, d)\n",
    "H_Tc = torch.nn.Parameter(torch.empty(rD_T * rL_T, d, device=device))\n",
    "torch.nn.init.xavier_uniform_(H_Tc)\n",
    "\n",
    "\n",
    "# loop over the batch to get zT (the factorised TCR embedding)\n",
    "# z = vec(A^TXB)H\n",
    "# X - (B, L_pad, D)\n",
    "# B - (D, rD_T)\n",
    "# A - (L_)\n",
    "\n",
    "zT_list = []\n",
    "\n",
    "for b in range(B):\n",
    "    ### 1. Get true length and slice\n",
    "    Lb = int(L_T_true[b].item())          # scalar length for sample b\n",
    "\n",
    "    # Slice to the true length (drop padded positions)\n",
    "    # Encoder input X_T\n",
    "    X = emb_T[b, :Lb, :]                  # (Lb, D)\n",
    "    m = tcr_mask[b, :Lb].unsqueeze(-1)    # (Lb, 1), 1 = real token, 0 = pad\n",
    "\n",
    "    # Apply mask to zero out padded positions (it's not necessary to do this explicitly, but clean)\n",
    "    X = X * m \n",
    "\n",
    "    # ---- 2. Channel compression: D -> rD_T ----\n",
    "    # Mix the D channels using B_Tc\n",
    "    Y = X @ B_Tc                          # (Lb, rD_T)\n",
    "    # map the D dim to a latent rank rD_T, so the same for each channel\n",
    "\n",
    "    A_pos = A_Tc[:Lb, :]\n",
    "    U = A_pos.T @ Y \n",
    "\n",
    "    # ---- 4. Flatten and map to the final d-dimensional vector ----\n",
    "    U_flat = U.reshape(-1)                # (rL_T * rD_T,)\n",
    "    z_b = U_flat @ H_Tc                   # (d,)\n",
    "\n",
    "    # ---- 5. Normalise the resulting vector ----\n",
    "    z_b = z_b / (z_b.norm() + eps)        # (d,)\n",
    "\n",
    "    zT_list.append(z_b)\n",
    "\n",
    "# Stack into a single tensor of shape (B, d)\n",
    "zT = torch.stack(zT_list, dim=0)          # (B, d)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f5d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading invidiual z embeddings\n",
    "file_path = '/home/natasha/multimodal_model/outputs/boltz_runs/positives/pair_000/boltz_results_pair_000/predictions/pair_000/embeddings_pair_000.npz'\n",
    "\n",
    "\n",
    "# shape of s is the sum of the lengths of the TCR, HLA and peptide [batch, sum_of_len, dim (384)]\n",
    "# shape of z is the length of the TCR, HLA and peptide twice [batch, sum_of_len, sum_of_len, dim (128)]\n",
    "\n",
    "with np.load(file_path) as data:\n",
    "    #print(list(data.keys()))\n",
    "    #print(data['s'].shape)\n",
    "    print(data['z'].shape)\n",
    "    s = data['s']\n",
    "    z = data['z']\n",
    "\n",
    "print(z)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcr-multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
