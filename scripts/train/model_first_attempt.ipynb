{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e2b02a3",
   "metadata": {},
   "source": [
    "##### This script defines the encoders for the multimodal model\n",
    "##### Overview:\n",
    "- The inputs are the ESMC and Boltz embeddings \n",
    "- 3 encoders from PEFT of ESMC\n",
    "- Two projection heads: one for ESM embeddings and one for Boltz embeddings\n",
    "- Non contrastive learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91aae1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/natasha/miniconda3/envs/tcr-multimodal/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "# Streamlined imports - removing duplicates\n",
    "import time\n",
    "import esm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import subprocess\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, update_display\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ESM imports\n",
    "from esm.models.esmc import ESMC\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig, ESMProteinTensor\n",
    "from esm.models.esmc import _BatchedESMProteinTensor\n",
    "\n",
    "# Tokenizer imports\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import decoders\n",
    "\n",
    "# PEFT imports\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from peft.tuners.lora import LoraConfig, LoraModel\n",
    "\n",
    "# Set environment variable\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31bb77ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/natasha/multimodal_model/scripts/train\n",
      "Project root: /home/natasha/multimodal_model\n",
      "Models directory at: /home/natasha/multimodal_model/models\n",
      "Checkpoints directory at: /home/natasha/multimodal_model/models/checkpoints\n"
     ]
    }
   ],
   "source": [
    "# Get current working directory and create models folder\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "# Get the project root (go up one level from scripts/train)\n",
    "project_root = Path(current_dir).parent.parent\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Create models directory in project root\n",
    "models_dir = project_root / \"models\"\n",
    "if not models_dir.exists():\n",
    "    print(f\"Models directory does not exist, creating it at: {models_dir}\")\n",
    "    models_dir.mkdir(exist_ok=True)\n",
    "print(f\"Models directory at: {models_dir}\")\n",
    "\n",
    "# Also create a checkpoints subdirectory for saving model checkpoints\n",
    "checkpoints_dir = models_dir / \"checkpoints\"\n",
    "if not checkpoints_dir.exists():\n",
    "    print(f\"Checkpoints directory does not exist, creating it at: {checkpoints_dir}\")\n",
    "    checkpoints_dir.mkdir(exist_ok=True)\n",
    "print(f\"Checkpoints directory at: {checkpoints_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07943542",
   "metadata": {},
   "source": [
    "##### Get ESM Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d270e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to load ESM C with LM head enabled \n",
    "# expose final token embeddings before the logits head (is logits head the LM head, LM head=language modelling head)?\n",
    "# collator returns: input_ids, attention_mask\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# load model and allow lora (rather than eval mode?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17ffa245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size = [1,7,960], always the case?\n",
    "# size = [1, 12, 960]\n",
    "# size is I think batch number, sequence length, embedding dimension\n",
    "\n",
    "df = pd.read_csv('/home/natasha/multimodal_model/data/raw/HLA/boltz_100_runs.csv')\n",
    "# Fill empty/nan values with <unk> token\n",
    "df['TCRa'] = df['TCRa'].fillna('X')\n",
    "df['TCRb'] = df['TCRb'].fillna('X')\n",
    "\n",
    "# Replace empty strings with <unk>\n",
    "df.loc[df['TCRa'] == '', 'TCRa'] = 'X'\n",
    "df.loc[df['TCRb'] == '', 'TCRb'] = 'X'\n",
    "\n",
    "df['TCR_full'] = df['TCRa'] + df['TCRb']\n",
    "df['m_alpha'] = 1\n",
    "df['m_beta'] = 1\n",
    "df.loc[df['TCRa'] == 'X', 'm_alpha'] = 0\n",
    "df.loc[df['TCRb'] == 'X', 'm_beta'] = 0\n",
    "#df.to_csv('/home/natasha/multimodal_model/data/raw/HLA/boltz_100_runs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9631825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read full train dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ca97902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hugging face dataset?\n",
    "class TCR_dataset(Dataset):\n",
    "    \"\"\"Dataset for TCR data, for use in encoder training to propagate through to NC model\"\"\"\n",
    "    def __init__(self, data_path, column_name='TCR_full', include_label=False):\n",
    "        self.data_path = data_path\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.column_name = column_name  # Store column name here\n",
    "        self.include_label = include_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):  # Remove column_name parameter\n",
    "        row = self.data.iloc[idx]  # Fix: self.data, not self.csv\n",
    "        protein = row[self.column_name]  # Use stored column name\n",
    "        protein_idx = f'TCR_{idx}'\n",
    "        if self.include_label:\n",
    "            return protein_idx, protein, row.get('Binding', -1)\n",
    "        #return protein_idx, protein\n",
    "        return protein\n",
    "\n",
    "class peptide_dataset(Dataset):\n",
    "    def __init__(self, data_path, column_name='Peptide', include_label=False):\n",
    "        self.data_path = data_path\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.column_name = column_name\n",
    "        self.include_label = include_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        peptide = row[self.column_name]\n",
    "        peptide_idx = f'peptide_{idx}'\n",
    "        if self.include_label:\n",
    "            return peptide_idx, peptide, row.get('Binding', -1)\n",
    "        #return peptide_idx, peptide\n",
    "        return peptide\n",
    "\n",
    "class HLA_dataset(Dataset):\n",
    "    def __init__(self, data_path, column_name='HLA', include_label=False):\n",
    "        self.data_path = data_path\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.column_name = column_name\n",
    "        self.include_label = include_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        hla = row[self.column_name]\n",
    "        hla_idx = f'hla_{idx}'\n",
    "        if self.include_label:\n",
    "            return hla_idx, hla, row.get('Binding', -1)\n",
    "        #return hla_idx, hla\n",
    "        return hla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b671bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcr = TCR_dataset(data_path='/home/natasha/multimodal_model/data/raw/HLA/boltz_100_runs.csv', column_name='TCR_full')\n",
    "peptide = peptide_dataset(data_path='/home/natasha/multimodal_model/data/raw/HLA/boltz_100_runs.csv', column_name='Peptide')\n",
    "hla = HLA_dataset(data_path='/home/natasha/multimodal_model/data/raw/HLA/boltz_100_runs.csv', column_name='HLA_sequence')\n",
    "\n",
    "# not sure I reallt need these classes??? Hmmmmmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94e53763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c745507b1ecf49188725e6a016134354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tcrs = [ESMProtein(sequence=s) for s in tcr.data['TCR_full']]\n",
    "peptides = [ESMProtein(sequence=s) for s in peptide.data['Peptide']]\n",
    "hlas = [ESMProtein(sequence=s) for s in hla.data['HLA_sequence']]\n",
    "\n",
    "# can batch at the forward step, not the encoding step\n",
    "\n",
    "#model = ESMC.from_pretrained(\"esmc_300m\").to(device).eval()\n",
    "model = ESMC.from_pretrained(\"esmc_300m\").eval()\n",
    "\n",
    "tcrs_data = [seq for seq in tcr]\n",
    "peptides_data = [seq for seq in peptide]\n",
    "hlas_data = [seq for seq in hla]\n",
    "\n",
    "encoded_tcrs = [model.encode(p) for p in tcrs]\n",
    "encoded_peptides = [model.encode(p) for p in peptides]\n",
    "encoded_hlas = [model.encode(p) for p in hlas]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b45d13e",
   "metadata": {},
   "source": [
    "##### Mask Data and Collate for MLM for Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eaf8da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do for entire dataset\n",
    "# do we also want to output attention_mask from the tokenizer?\n",
    "\n",
    "tok = model.tokenizer\n",
    "CLS_ID = tok.cls_token_id\n",
    "EOS_ID = tok.eos_token_id\n",
    "PAD_ID = tok.pad_token_id\n",
    "MASK_ID = tok.mask_token_id\n",
    "\n",
    "AA_IDS =  [5,10,17,13,23,16,9,6,21,12,4,15,20,18,14,8,11,22,19,7]\n",
    "\n",
    "\n",
    "class EncodedSeqDataset(Dataset):\n",
    "    def __init__(self, sequences, enc):     # ← now takes two arguments\n",
    "        self.sequences = sequences          # list[str]\n",
    "        self.input_ids = enc['input_ids']\n",
    "        self.attention_mask = enc['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"sequence\": self.sequences[idx],  # raw sequence string\n",
    "            \"input_ids\": torch.as_tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            \"attention_mask\": torch.as_tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0865bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMProteinCollator:\n",
    "    def __init__(self, *, cls_id, eos_id, pad_id, mask_id, amino_acids,\n",
    "                 p=0.15, min_per_seq=2, max_per_seq=45, aa_frac=0.20):\n",
    "        self.CLS = cls_id\n",
    "        self.EOS = eos_id\n",
    "        self.PAD = pad_id\n",
    "        self.MASK = mask_id\n",
    "        self.aa = torch.as_tensor(amino_acids, dtype=torch.long)\n",
    "        self.p = p\n",
    "        self.min_per_seq = min_per_seq\n",
    "        self.max_per_seq = max_per_seq\n",
    "        self.aa_frac = aa_frac\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def mask_batch(self, input_ids, attention_mask):\n",
    "        device = input_ids.device\n",
    "        aa = self.aa.to(device)\n",
    "\n",
    "        B, L = input_ids.shape\n",
    "        valid_mask = attention_mask.bool() \\\n",
    "                   & (input_ids != self.PAD) \\\n",
    "                   & (input_ids != self.CLS) \\\n",
    "                   & (input_ids != self.EOS)\n",
    "\n",
    "        masked_input_ids = input_ids.clone()\n",
    "        labels = torch.full_like(input_ids, -100)\n",
    "\n",
    "        for i in range(B):\n",
    "            vmask = valid_mask[i]\n",
    "            if not vmask.any():\n",
    "                continue\n",
    "\n",
    "            valid_idx = vmask.nonzero(as_tuple=False).squeeze(1)  # (L_valid,)\n",
    "            L_valid = valid_idx.numel()\n",
    "\n",
    "            # how many to mask: floor(p*L_valid), clamped to [2, 45] but never > L_valid\n",
    "            n = torch.floor(self.p * torch.tensor(L_valid, device=device, dtype=torch.float32)).to(torch.int64)\n",
    "            n = torch.clamp(n, min=self.min_per_seq, max=min(self.max_per_seq, L_valid))\n",
    "            if n.item() == 0:\n",
    "                continue\n",
    "\n",
    "            # choose n distinct valid positions\n",
    "            chosen = valid_idx[torch.randperm(L_valid, device=device)[:n]]\n",
    "\n",
    "            # split into AA vs MASK; ensure >=1 AA if n>=2\n",
    "            n_amino = torch.floor(self.aa_frac * n).to(torch.int64)\n",
    "            if n.item() >= 2:\n",
    "                n_amino = torch.clamp(n_amino, min=1)\n",
    "            n_mask = n - n_amino\n",
    "\n",
    "            order = torch.randperm(n.item(), device=device)\n",
    "            mask_pos  = chosen[order[:n_mask]]\n",
    "            amino_pos = chosen[order[n_mask:]]\n",
    "\n",
    "            # labels only at supervised positions\n",
    "            labels[i, chosen] = input_ids[i, chosen]\n",
    "\n",
    "            # apply edits\n",
    "            if n_mask.item() > 0:\n",
    "                masked_input_ids[i, mask_pos] = self.MASK\n",
    "            if n_amino.item() > 0:\n",
    "                r_idx = torch.randint(high=aa.numel(), size=(n_amino.item(),), device=device)\n",
    "                masked_input_ids[i, amino_pos] = aa[r_idx]\n",
    "\n",
    "        return masked_input_ids, labels\n",
    "\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_ids = torch.stack([f[\"input_ids\"] for f in features], dim=0)\n",
    "        attention_mask = torch.stack([f[\"attention_mask\"] for f in features], dim=0)\n",
    "        sequences = [f[\"sequence\"] for f in features]\n",
    "        proteins = [ESMProtein(sequence=f[\"sequence\"]) for f in features]\n",
    "        batched_clean = _BatchedESMProteinTensor(sequence=input_ids)\n",
    "\n",
    "\n",
    "        masked_input_ids, labels = self.mask_batch(input_ids, attention_mask)\n",
    "\n",
    "        # build masked sequences as strings (keep <mask>, drop CLS/EOS/PAD)\n",
    "        masked_sequences = []\n",
    "        for row in masked_input_ids.tolist():\n",
    "            toks = collator.tokenizer.convert_ids_to_tokens(row, skip_special_tokens=False)\n",
    "            aa = []\n",
    "            for t in toks:\n",
    "                if t in (collator.tokenizer.cls_token, collator.tokenizer.eos_token, collator.tokenizer.pad_token):\n",
    "                    continue\n",
    "                aa.append(t)  # AA tokens are single letters; keep \"<mask>\" as is\n",
    "            masked_sequences.append(\"\".join(aa))\n",
    "\n",
    "        proteins_masked = [ESMProtein(sequence=s) for s in masked_sequences]\n",
    "        batched_masked = _BatchedESMProteinTensor(sequence=masked_input_ids)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"masked_input_ids\": masked_input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"clean_input_ids\": input_ids.clone(),\n",
    "            \"clean_sequences\": sequences,                 # clean strings\n",
    "            \"masked_sequences\": masked_sequences,   # masked strings  ← NEW\n",
    "            \"clean_sequences_ESMprotein\": proteins,\n",
    "            \"masked_sequences_ESMprotein\": proteins_masked,\n",
    "            \"masked_input_ids_ESMprotein_batched\": batched_masked,\n",
    "            \"clean_input_ids_ESMprotein_batched\": batched_clean,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "146109ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your existing BatchEncodings:\n",
    "# clean_tcrs_tokenized, clean_peptides_tokenized, clean_hlas_tokenized\n",
    "clean_tcrs_tokenized = model.tokenizer(tcrs_data, return_tensors='pt', padding=True)\n",
    "clean_peptides_tokenized = model.tokenizer(peptides_data, return_tensors='pt', padding=True)\n",
    "clean_hlas_tokenized = model.tokenizer(hlas_data, return_tensors='pt', padding=True)\n",
    "\n",
    "tcr_ds = EncodedSeqDataset(tcrs_data,clean_tcrs_tokenized)\n",
    "pep_ds = EncodedSeqDataset(peptides_data, clean_peptides_tokenized)\n",
    "hla_ds = EncodedSeqDataset(hlas_data, clean_hlas_tokenized)\n",
    "\n",
    "collator = MLMProteinCollator(\n",
    "    cls_id=CLS_ID, eos_id=EOS_ID, pad_id=PAD_ID, mask_id=MASK_ID,\n",
    "    amino_acids=AA_IDS, p=0.15, min_per_seq=2, max_per_seq=45, aa_frac=0.20\n",
    ")\n",
    "collator.tokenizer = model.tokenizer\n",
    "\n",
    "\n",
    "tcr_loader = DataLoader(tcr_ds, batch_size=8, shuffle=True, num_workers=4, collate_fn=collator)\n",
    "pep_loader = DataLoader(pep_ds, batch_size=8, shuffle=True, num_workers=4, collate_fn=collator)\n",
    "hla_loader = DataLoader(hla_ds, batch_size=8, shuffle=True, num_workers=4, collate_fn=collator)\n",
    "\n",
    "\n",
    "# gives a batch dict from the collator with 4 keys\n",
    "# input_ids, labels (original tokens only at masked positions, -100 everywhere else)\n",
    "# attention_mask (0,1 for padding), clean_input_ids (clean copy of the input for clean forward pass using boltz for NC loss)\n",
    "\n",
    "# model.to(\"cpu\")\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "def optimizer_to_cpu(optim):\n",
    "    for st in optim.state.values():\n",
    "        for k, v in list(st.items()):\n",
    "            if torch.is_tensor(v):\n",
    "                st[k] = v.detach().to(\"cpu\")\n",
    "\n",
    "# move model to CPU and delete, as now we have the correct inputs \n",
    "model.to(\"cpu\")\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d9497b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_882739/627727058.py:33: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler  = GradScaler(enabled=False)                     # <-- no scaler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to tcr_encoder_checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "use_amp = torch.cuda.is_available()\n",
    "\n",
    "#base = ESMC.from_pretrained(\"esmc_300m\").to(device)\n",
    "#base = ESMC.from_pretrained(\"esmc_300m\")\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "    target_modules=[\"out_proj\", \"layernorm_qkv.1\"],  # inner Linear inside the Sequential\n",
    ")\n",
    "\n",
    "model_tcr = LoraModel(ESMC.from_pretrained(\"esmc_300m\"), lora_cfg, adapter_name=\"tcr\")\n",
    "\n",
    "\n",
    "#model_tcr = LoraModel(base, lora_cfg, adapter_name=\"tcr\")\n",
    "\n",
    "# freeze everything; unfreeze only LoRA params\n",
    "for p in model_tcr.parameters():\n",
    "    p.requires_grad = False\n",
    "for name, p in model_tcr.named_parameters():\n",
    "    if \"lora_A\" in name or \"lora_B\" in name:\n",
    "        p.requires_grad = True\n",
    "\n",
    "model_tcr.to(\"cuda\")\n",
    "model_tcr.train()\n",
    "\n",
    "optim_tcr = torch.optim.AdamW(\n",
    "    (p for p in model_tcr.parameters() if p.requires_grad),\n",
    "    lr=1e-3, weight_decay=0.01\n",
    ")\n",
    "\n",
    "\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler  = GradScaler(enabled=False)                     # <-- no scaler\n",
    "\n",
    "for batch in tcr_loader:\n",
    "    input_ids = batch[\"masked_input_ids\"].to(device, dtype=torch.long)\n",
    "    labels    = batch[\"labels\"].to(device)\n",
    "\n",
    "    with autocast(\"cuda\", enabled=use_amp, dtype=torch.bfloat16):  # <-- bf16\n",
    "        out    = model_tcr(input_ids)\n",
    "        logits = out.sequence_logits\n",
    "        #print(type(logits), logits.shape, logits.requires_grad)  # expect: Tensor, [B,L,V], True\n",
    "\n",
    "        loss   = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            labels.view(-1),\n",
    "            ignore_index=-100\n",
    "        )\n",
    "\n",
    "    loss.backward()                                     # <-- standard backward\n",
    "    optim_tcr.step(); optim_tcr.zero_grad(set_to_none=True)\n",
    "\n",
    "    del out, logits, loss, input_ids, labels, batch\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "# 1.1 seconds for 100\n",
    "# 11 for 1000\n",
    "# 110 for 10000\n",
    "# 350 for 35000 - 5.8 minutes to train one encoder on full dataset\n",
    "\n",
    "optimizer_to_cpu(optim_tcr)\n",
    "\n",
    "# save tcr model\n",
    "\n",
    "checkpoint_filename = 'tcr_encoder_checkpoint.pth'\n",
    "\n",
    "checkpoint_dict = {\n",
    "    #'epoch': num_epochs,\n",
    "    'tcr_model_state_dict': model_tcr.state_dict(),\n",
    "    #'pep_model_state_dict': peptide_model_nc.state_dict(), \n",
    "    'optimizer_state_dict': optim_tcr.state_dict(),\n",
    "    #'final_loss': avg_epoch_loss,\n",
    "    #'binding_threshold': threshold  # Add threshold to saved state\n",
    "}\n",
    "\n",
    "torch.save(checkpoint_dict, checkpoints_dir/checkpoint_filename)\n",
    "\n",
    "print(f\"Checkpoint saved to {checkpoint_filename}\")\n",
    "\n",
    "model_tcr.to(\"cpu\")\n",
    "\n",
    "del optim_tcr, model_tcr, checkpoint_dict, checkpoint_filename\n",
    "\n",
    "torch.cuda.empty_cache()  # Free up GPU memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bd0beba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_882739/363416641.py:32: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler  = GradScaler(enabled=False)                     # <-- no scaler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to peptide_encoder_checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "# peptide encoder\n",
    "\n",
    "use_amp = torch.cuda.is_available()\n",
    "\n",
    "#base = ESMC.from_pretrained(\"esmc_300m\").to(device)\n",
    "\n",
    "## use the same lora config for peptide and HLA - modify later\n",
    "# lora_cfg = LoraConfig(\n",
    "#     r=8, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "#     target_modules=[\"out_proj\", \"layernorm_qkv.1\"],  # inner Linear inside the Sequential\n",
    "# )\n",
    "\n",
    "model_pep = LoraModel(ESMC.from_pretrained(\"esmc_300m\"), lora_cfg, adapter_name=\"pep\")\n",
    "#model_pep = LoraModel(base, lora_cfg, adapter_name=\"peptide\")\n",
    "\n",
    "# freeze everything; unfreeze only LoRA params\n",
    "for p in model_pep.parameters():\n",
    "    p.requires_grad = False\n",
    "for name, p in model_pep.named_parameters():\n",
    "    if \"lora_A\" in name or \"lora_B\" in name:\n",
    "        p.requires_grad = True\n",
    "\n",
    "model_pep.to(\"cuda\")\n",
    "model_pep.train()\n",
    "\n",
    "optim_pep = torch.optim.AdamW(\n",
    "    (p for p in model_pep.parameters() if p.requires_grad),\n",
    "    lr=1e-3, weight_decay=0.01\n",
    ")\n",
    "\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler  = GradScaler(enabled=False)                     # <-- no scaler\n",
    "\n",
    "for batch in pep_loader:\n",
    "    input_ids = batch[\"masked_input_ids\"].to(device, dtype=torch.long)\n",
    "    labels    = batch[\"labels\"].to(device)\n",
    "\n",
    "    with autocast(\"cuda\", enabled=use_amp, dtype=torch.bfloat16):  # <-- bf16\n",
    "        out    = model_pep(input_ids)\n",
    "        logits = out.sequence_logits\n",
    "        #print(type(logits), logits.shape, logits.requires_grad)  # expect: Tensor, [B,L,V], True\n",
    "\n",
    "        loss   = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            labels.view(-1),\n",
    "            ignore_index=-100\n",
    "        )\n",
    "\n",
    "    loss.backward()                                     # <-- standard backward\n",
    "    optim_pep.step(); optim_pep.zero_grad(set_to_none=True)\n",
    "\n",
    "    del out, logits, loss, input_ids, labels, batch\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "optimizer_to_cpu(optim_pep)\n",
    "\n",
    "# save peptide model\n",
    "\n",
    "checkpoint_filename = 'peptide_encoder_checkpoint.pth'\n",
    "\n",
    "checkpoint_dict = {\n",
    "    #'epoch': num_epochs,\n",
    "    'peptide_model_state_dict': model_pep.state_dict(),\n",
    "    'optimizer_state_dict': optim_pep.state_dict(),\n",
    "    #'final_loss': avg_epoch_loss,\n",
    "    #'binding_threshold': threshold  # Add threshold to saved state\n",
    "}\n",
    "\n",
    "torch.save(checkpoint_dict, checkpoints_dir/checkpoint_filename)\n",
    "\n",
    "print(f\"Checkpoint saved to {checkpoint_filename}\")\n",
    "\n",
    "model_pep.to(\"cpu\")\n",
    "del optim_pep, model_pep, checkpoint_dict, checkpoint_filename\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "#print(torch.cuda.memory_summary())           # “Active” bytes should drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b319d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_882739/2523016605.py:33: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler  = GradScaler(enabled=False)                     # <-- no scaler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to hla_encoder_checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "# HLA encoder\n",
    "\n",
    "use_amp = torch.cuda.is_available()\n",
    "\n",
    "#base = ESMC.from_pretrained(\"esmc_300m\").to(device)\n",
    "\n",
    "## use the same lora config for peptide and HLA - modify later\n",
    "# lora_cfg = LoraConfig(\n",
    "#     r=8, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "#     target_modules=[\"out_proj\", \"layernorm_qkv.1\"],  # inner Linear inside the Sequential\n",
    "# )\n",
    "\n",
    "#model_hla = LoraModel(base, lora_cfg, adapter_name=\"hla\")\n",
    "model_hla = LoraModel(ESMC.from_pretrained(\"esmc_300m\"), lora_cfg, adapter_name=\"hla\")\n",
    "\n",
    "\n",
    "# freeze everything; unfreeze only LoRA params\n",
    "for p in model_hla.parameters():\n",
    "    p.requires_grad = False\n",
    "for name, p in model_hla.named_parameters():\n",
    "    if \"lora_A\" in name or \"lora_B\" in name:\n",
    "        p.requires_grad = True\n",
    "\n",
    "model_hla.to(\"cuda\")\n",
    "model_hla.train()\n",
    "\n",
    "optim_hla = torch.optim.AdamW(\n",
    "    (p for p in model_hla.parameters() if p.requires_grad),\n",
    "    lr=1e-3, weight_decay=0.01\n",
    ")\n",
    "\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler  = GradScaler(enabled=False)                     # <-- no scaler\n",
    "\n",
    "for batch in hla_loader:\n",
    "    input_ids = batch[\"masked_input_ids\"].to(device, dtype=torch.long)\n",
    "    labels    = batch[\"labels\"].to(device)\n",
    "\n",
    "    with autocast(\"cuda\", enabled=use_amp, dtype=torch.bfloat16):  # <-- bf16\n",
    "        out    = model_hla(input_ids)\n",
    "        logits = out.sequence_logits\n",
    "        #print(type(logits), logits.shape, logits.requires_grad)  # expect: Tensor, [B,L,V], True\n",
    "\n",
    "        loss   = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            labels.view(-1),\n",
    "            ignore_index=-100\n",
    "        )\n",
    "\n",
    "    loss.backward()                                     # <-- standard backward\n",
    "    optim_hla.step()\n",
    "    optim_hla.zero_grad(set_to_none=True)\n",
    "\n",
    "    # clear memory\n",
    "    del out, logits, loss, input_ids, labels, batch\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "optimizer_to_cpu(optim_hla)\n",
    "\n",
    "# save HLA model\n",
    "\n",
    "checkpoint_filename = 'hla_encoder_checkpoint.pth'\n",
    "\n",
    "checkpoint_dict = {\n",
    "    #'epoch': num_epochs,\n",
    "    'hla_model_state_dict': model_hla.state_dict(),\n",
    "    'optimizer_state_dict': optim_hla.state_dict(),\n",
    "    #'final_loss': avg_epoch_loss,\n",
    "    #'binding_threshold': threshold  # Add threshold to saved state\n",
    "}\n",
    "\n",
    "torch.save(checkpoint_dict, checkpoints_dir/checkpoint_filename)\n",
    "\n",
    "print(f\"Checkpoint saved to {checkpoint_filename}\")\n",
    "\n",
    "model_hla.to(\"cpu\")\n",
    "del optim_hla, model_hla, checkpoint_dict, checkpoint_filename\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a9ef0",
   "metadata": {},
   "source": [
    "#### Load Encoders and Boltz Embeddings\n",
    "\n",
    "a) Load encoders and create boltz loader function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3939828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f38124f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Encoders loaded successfully as fixed feature extractors!\n",
      "  (Optimizer state skipped - not needed for this use case)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LOADING ENCODERS FOR USE AS FIXED FEATURE EXTRACTORS\n",
    "\n",
    "Use case: You're NOT continuing to train the encoders. Instead, you'll use them\n",
    "as fixed/pre-trained components in the next part of your training pipeline (likely\n",
    "the multimodal model that combines TCR, peptide, and HLA embeddings).\n",
    "\n",
    "WHAT YOU NEED:\n",
    "✅ Model weights (to get the fine-tuned LoRA adapters)\n",
    "✅ Set to .eval() mode (to disable dropout, use batch norm stats, etc.)\n",
    "\n",
    "WHAT YOU DON'T NEED:\n",
    "❌ Optimizer state - only needed if you were continuing to train the encoders\n",
    "\n",
    "NOTE: The checkpoint contains 'optimizer_state_dict', but we're ignoring it\n",
    "since we're using these models as frozen feature extractors.\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Load the checkpoint dictionaries (just Python dicts from disk)\n",
    "# checkpoint_tcr = torch.load(checkpoints_dir/'tcr_encoder_checkpoint.pth', map_location=device)\n",
    "# checkpoint_pep = torch.load(checkpoints_dir/'peptide_encoder_checkpoint.pth', map_location=device)\n",
    "# checkpoint_hla = torch.load(checkpoints_dir/'hla_encoder_checkpoint.pth', map_location=device)\n",
    "checkpoint_tcr = torch.load(checkpoints_dir/'tcr_encoder_checkpoint.pth', map_location='cpu')\n",
    "checkpoint_pep = torch.load(checkpoints_dir/'peptide_encoder_checkpoint.pth', map_location='cpu')\n",
    "checkpoint_hla = torch.load(checkpoints_dir/'hla_encoder_checkpoint.pth', map_location='cpu')\n",
    "\n",
    "# Step 2: Recreate the model architectures (same as during training)\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "    target_modules=[\"out_proj\", \"layernorm_qkv.1\"],\n",
    ")\n",
    "\n",
    "# Create models with same architecture as during training\n",
    "tcr_encoder = LoraModel(ESMC.from_pretrained(\"esmc_300m\"), lora_cfg, adapter_name=\"tcr\")\n",
    "peptide_encoder = LoraModel(ESMC.from_pretrained(\"esmc_300m\"), lora_cfg, adapter_name=\"pep\")\n",
    "hla_encoder = LoraModel(ESMC.from_pretrained(\"esmc_300m\"), lora_cfg, adapter_name=\"hla\")\n",
    "\n",
    "# Step 3: Load the model state dictionaries (this loads base weights + LoRA adapters)\n",
    "# NOTE: We're NOT loading optimizer_state_dict - not needed for inference/feature extraction\n",
    "tcr_encoder.load_state_dict(checkpoint_tcr['tcr_model_state_dict'])\n",
    "peptide_encoder.load_state_dict(checkpoint_pep['peptide_model_state_dict'])\n",
    "hla_encoder.load_state_dict(checkpoint_hla['hla_model_state_dict'])\n",
    "\n",
    "# Step 4: Set to evaluation mode and move to CPU\n",
    "# .eval() mode ensures: no dropout, batch norm uses running stats, etc.\n",
    "# tcr_encoder.to('cpu').eval()\n",
    "# peptide_encoder.to('cpu').eval()\n",
    "# hla_encoder.to('cpu').eval()\n",
    "\n",
    "# Load to GPU becuase we need to get the embeddings in the forward pass\n",
    "tcr_encoder.to(device).eval()\n",
    "peptide_encoder.to(device).eval()\n",
    "hla_encoder.to(device).eval()\n",
    "\n",
    "print(\"✓ Encoders loaded successfully as fixed feature extractors!\")\n",
    "print(\"  (Optimizer state skipped - not needed for this use case)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fb951ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/natasha/multimodal_model/scripts/train\n",
      "456 377\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "\n",
    "home = '/home/natasha/multimodal_model'\n",
    "#manifest_path = os.path.join(home, 'data', 'manifests', 'boltz_100_manifest.csv')\n",
    "positive_manifest_path = os.path.join(home, 'data', 'manifests', 'boltz_100_manifest.csv')\n",
    "negative_manifest_path = os.path.join(home, 'data', 'negative_manifests', 'boltz_100_manifest.csv')\n",
    "#print(manifest_path)\n",
    "\n",
    "#manifest_data = pd.read_csv(manifest_path)\n",
    "positive_manifest_data = pd.read_csv(positive_manifest_path)\n",
    "negative_manifest_data = pd.read_csv(negative_manifest_path)\n",
    "manifest_data = pd.concat([positive_manifest_data, negative_manifest_data])\n",
    "\n",
    "L_T_max_boltz = max(manifest_data['tcra_len']+manifest_data['tcrb_len'])\n",
    "L_PH_max_boltz = max(manifest_data['pep_len']+manifest_data['hla_len'])\n",
    "\n",
    "print(L_T_max_boltz, L_PH_max_boltz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c05801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get Boltz dataset\n",
    "# # old version\n",
    "\n",
    "# class BoltzDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     Dataset for loading Boltz z-embeddings one by one,\n",
    "#     with chain lengths from the manifest.\n",
    "#     Each pair has its own .npz file.\n",
    "#     ORIGINAL VERSION - returns numpy arrays\n",
    "#     \"\"\"\n",
    "#     def __init__(self, manifest_path, base_path):\n",
    "#         self.manifest = pd.read_csv(manifest_path)\n",
    "#         self.base_path = base_path\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.manifest)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         yaml_rel_path = self.manifest.iloc[idx]['yaml_path']\n",
    "#         pair_id = os.path.splitext(os.path.basename(yaml_rel_path))[0]\n",
    "#         emb_path = os.path.join(\n",
    "#             self.base_path,\n",
    "#             'outputs',\n",
    "#             'boltz_runs',\n",
    "#             'positives',\n",
    "#             pair_id,\n",
    "#             f'boltz_results_{pair_id}',\n",
    "#             'predictions',\n",
    "#             pair_id,\n",
    "#             f'embeddings_{pair_id}.npz'\n",
    "#         )\n",
    "#         with np.load(emb_path) as arr:\n",
    "#             z = arr['z']  # Returns numpy array as-is\n",
    "#         pep_len = self.manifest.iloc[idx]['pep_len']\n",
    "#         tcra_len = self.manifest.iloc[idx]['tcra_len']\n",
    "#         tcrb_len = self.manifest.iloc[idx]['tcrb_len']\n",
    "#         hla_len = self.manifest.iloc[idx]['hla_len']\n",
    "#         return z, pep_len, tcra_len, tcrb_len, hla_len\n",
    "\n",
    "\n",
    "\n",
    "# def boltz_collate_fn(batch):\n",
    "#     \"\"\"\n",
    "#     Pad Boltz z to max L in batch; return torch tensors\n",
    "#     \"\"\"\n",
    "#     zs, pep_lens, tcra_lens, tcrb_lens, hla_lens = zip(*batch)\n",
    "\n",
    "#     # Each z: [L, L, dim] or [1, L, L, dim]\n",
    "#     zs = [np.squeeze(z, axis=0) for z in zs]  # ensure (L, L, dB)\n",
    "\n",
    "#     max_len = max(z.shape[0] for z in zs)\n",
    "#     dim = zs[0].shape[-1]\n",
    "\n",
    "#     padded_zs = np.zeros((len(zs), max_len, max_len, dim), dtype=zs[0].dtype)\n",
    "#     for i, z in enumerate(zs):\n",
    "#         L = z.shape[0]\n",
    "#         padded_zs[i, :L, :L, :] = z\n",
    "\n",
    "#     zs_torch = torch.from_numpy(padded_zs).float()\n",
    "\n",
    "#     pep_lens_t  = torch.as_tensor(pep_lens,  dtype=torch.long)\n",
    "#     tcra_lens_t = torch.as_tensor(tcra_lens, dtype=torch.long)\n",
    "#     tcrb_lens_t = torch.as_tensor(tcrb_lens, dtype=torch.long)\n",
    "#     hla_lens_t  = torch.as_tensor(hla_lens,  dtype=torch.long)\n",
    "\n",
    "#     return {\n",
    "#         \"z\": zs_torch,          # (B, L_pad, L_pad, dB)\n",
    "#         \"pep_len\":  pep_lens_t, # (B,)\n",
    "#         \"tcra_len\": tcra_lens_t,\n",
    "#         \"tcrb_len\": tcrb_lens_t,\n",
    "#         \"hla_len\":  hla_lens_t,\n",
    "#     }\n",
    "\n",
    "\n",
    "# # # Usage:\n",
    "# # dataset_original = BoltzDataset(manifest_path, home)\n",
    "# # dataloader_original = DataLoader(\n",
    "# #     dataset_original, batch_size=8, shuffle=True, collate_fn=boltz_collate_fn\n",
    "# # )\n",
    "\n",
    "\n",
    "# # batch = next(iter(dataloader_original))\n",
    "# # print(\"Batch z shapes:\", [z.shape for z in batch[\"z\"]])\n",
    "# # print(\"Pep lengths:\", batch[\"pep_len\"])\n",
    "\n",
    "# # 5.6s to load 100 pairs\n",
    "# # 56 s to load 1000 pairs\n",
    "# # 560 s to load 10000 pairs (9 mins)\n",
    "# # 5600 s to load 100000 pairs (16 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "089c2fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BoltzDataset(Dataset):\n",
    "    def __init__(self, manifest_path, base_path, split_dir, strict=False):\n",
    "        \"\"\"\n",
    "        split_dir: e.g. 'positives' or 'negatives'\n",
    "        strict=False: if True, raise when a file is missing; else filter it out.\n",
    "        \"\"\"\n",
    "        self.base_path = base_path\n",
    "        self.split_dir = split_dir\n",
    "        manifest = pd.read_csv(manifest_path)\n",
    "\n",
    "        valid_rows = []\n",
    "        missing = 0\n",
    "\n",
    "        for i in range(len(manifest)):\n",
    "            yaml_rel_path = manifest.iloc[i]['yaml_path']\n",
    "            pair_id = os.path.splitext(os.path.basename(yaml_rel_path))[0]\n",
    "            emb_path = os.path.join(\n",
    "                self.base_path,\n",
    "                'outputs', 'boltz_runs',\n",
    "                self.split_dir,\n",
    "                pair_id,\n",
    "                f'boltz_results_{pair_id}',\n",
    "                'predictions',\n",
    "                pair_id,\n",
    "                f'embeddings_{pair_id}.npz'\n",
    "            )\n",
    "            if os.path.exists(emb_path):\n",
    "                row = manifest.iloc[i].copy()\n",
    "                row[\"pair_id\"] = pair_id\n",
    "                row[\"emb_path\"] = emb_path\n",
    "                valid_rows.append(row)\n",
    "            else:\n",
    "                missing += 1\n",
    "                if strict:\n",
    "                    raise FileNotFoundError(f\"Missing: {emb_path}\")\n",
    "\n",
    "        self.manifest = pd.DataFrame(valid_rows).reset_index(drop=True)\n",
    "        print(f\"[BoltzDataset:{split_dir}] kept {len(self.manifest)} rows, filtered {missing} missing files.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.manifest)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.manifest.iloc[idx]\n",
    "        emb_path = row[\"emb_path\"]\n",
    "\n",
    "        with np.load(emb_path) as arr:\n",
    "            z = arr[\"z\"]\n",
    "\n",
    "        z = np.squeeze(z, axis=0) if z.ndim == 4 else z  # (L,L,dB)\n",
    "        L_pad = z.shape[0]\n",
    "\n",
    "        pep_len  = int(row[\"pep_len\"])\n",
    "        tcra_len = int(row[\"tcra_len\"])\n",
    "        tcrb_len = int(row[\"tcrb_len\"])\n",
    "        hla_len  = int(row[\"hla_len\"])\n",
    "\n",
    "        # --- clamp lengths so (tcra+tcrb+pep+hla) never exceeds what z contains ---\n",
    "        L_T  = tcra_len + tcrb_len\n",
    "        L_PH = pep_len + hla_len\n",
    "        L_total = L_T + L_PH\n",
    "\n",
    "        if L_total > L_pad:\n",
    "            L_T_new = min(L_T, L_pad)\n",
    "            remaining = L_pad - L_T_new\n",
    "            L_PH_new = min(L_PH, remaining)\n",
    "\n",
    "            tcra_new = min(tcra_len, L_T_new)\n",
    "            tcrb_new = min(tcrb_len, L_T_new - tcra_new)\n",
    "\n",
    "            pep_new = min(pep_len, L_PH_new)\n",
    "            hla_new = min(hla_len, L_PH_new - pep_new)\n",
    "\n",
    "            tcra_len, tcrb_len, pep_len, hla_len = tcra_new, tcrb_new, pep_new, hla_new\n",
    "\n",
    "        return z, pep_len, tcra_len, tcrb_len, hla_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a3c69c",
   "metadata": {},
   "source": [
    "b) Load Embeddings: Pre-compute embeddings so we don't have to do it at the forward step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f3b60ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b75ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESM embeddings \n",
    "\n",
    "all_emb_T, all_emb_P, all_emb_H = [], [], []\n",
    "all_mask_T, all_mask_P, all_mask_H = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for tcr_batch, pep_batch, hla_batch in zip(tcr_loader, pep_loader, hla_loader):\n",
    "        # move to device\n",
    "        # tcr_ids = torch.stack(tcr_batch[\"clean_input_ids\"]).to(device)\n",
    "        # pep_ids = torch.stack(pep_batch[\"clean_input_ids\"]).to(device)\n",
    "        # hla_ids = torch.stack(hla_batch[\"clean_input_ids\"]).to(device)\n",
    "\n",
    "        tcr_ids  = tcr_batch[\"clean_input_ids\"].to(device)\n",
    "        pep_ids  = pep_batch[\"clean_input_ids\"].to(device)\n",
    "        hla_ids  = hla_batch[\"clean_input_ids\"].to(device)\n",
    "\n",
    "        tcr_mask = tcr_batch[\"attention_mask\"]\n",
    "        pep_mask = pep_batch[\"attention_mask\"]\n",
    "        hla_mask = hla_batch[\"attention_mask\"]\n",
    "\n",
    "        out_T = tcr_encoder.model(sequence_tokens=tcr_ids)\n",
    "        out_P = peptide_encoder.model(sequence_tokens=pep_ids)\n",
    "        out_H = hla_encoder.model(sequence_tokens=hla_ids)\n",
    "\n",
    "        all_emb_T.append(out_T.embeddings.cpu())\n",
    "        all_emb_P.append(out_P.embeddings.cpu())\n",
    "        all_emb_H.append(out_H.embeddings.cpu())\n",
    "\n",
    "        all_mask_T.append(tcr_mask)\n",
    "        all_mask_P.append(pep_mask)\n",
    "        all_mask_H.append(hla_mask)\n",
    "\n",
    "emb_T = torch.cat(all_emb_T, dim=0)\n",
    "emb_P = torch.cat(all_emb_P, dim=0)\n",
    "emb_H = torch.cat(all_emb_H, dim=0)\n",
    "\n",
    "mask_T = torch.cat(all_mask_T, dim=0)\n",
    "mask_P = torch.cat(all_mask_P, dim=0)\n",
    "mask_H = torch.cat(all_mask_H, dim=0)\n",
    "\n",
    "torch.save(\n",
    "    {\"emb_T\": emb_T, \"emb_P\": emb_P, \"emb_H\": emb_H,\n",
    "     \"mask_T\": mask_T, \"mask_P\": mask_P, \"mask_H\": mask_H},\n",
    "    \"train_embeddings.pt\"\n",
    ")\n",
    "\n",
    "tcr_encoder.to('cpu')\n",
    "peptide_encoder.to('cpu')\n",
    "hla_encoder.to('cpu')\n",
    "\n",
    "# Clear GPU cache to ensure nothing is hanging around\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# took 1.8s to load 100 full pairs on GPU\n",
    "# 18s to load 1,000 full pairs on GPU\n",
    "# 3m to load 10,000 full pairs on GPU\n",
    "# 9m to load 30,000 full pairs on GPU \n",
    "# so we're good to go for training purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ce6fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Boltz Embeddings\n",
    "# # Old - trying to fix to plot histograms\n",
    "\n",
    "# # Before defining positives and negaitves for comparison\n",
    "# # dataset_original = BoltzDataset(manifest_path, home)\n",
    "# # boltz_loader = DataLoader(\n",
    "# #     dataset_original, batch_size=8, shuffle=True, collate_fn=boltz_collate_fn\n",
    "# # )\n",
    "\n",
    "# dataset_original = BoltzDataset(positive_manifest_path, home)\n",
    "# boltz_loader = DataLoader(\n",
    "#     dataset_original, batch_size=8, shuffle=True, collate_fn=boltz_collate_fn\n",
    "# )\n",
    "\n",
    "# # load negatives\n",
    "\n",
    "# dataset_negatives = BoltzDataset(negative_manifest_path, home)\n",
    "# negatives_loader = DataLoader(\n",
    "#     dataset_negatives, batch_size=8, shuffle=True, collate_fn=boltz_collate_fn\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0e09630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BoltzDataset:positives] kept 100 rows, filtered 0 missing files.\n",
      "[BoltzDataset:negatives] kept 32 rows, filtered 68 missing files.\n"
     ]
    }
   ],
   "source": [
    "dataset_original  = BoltzDataset(positive_manifest_path, home, split_dir=\"positives\")\n",
    "boltz_loader      = DataLoader(dataset_original, batch_size=8, shuffle=True, collate_fn=boltz_collate_fn)\n",
    "\n",
    "dataset_negatives = BoltzDataset(negative_manifest_path, home, split_dir=\"negatives\")\n",
    "negatives_loader  = DataLoader(dataset_negatives, batch_size=8, shuffle=True, collate_fn=boltz_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5617bb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg batch z pad: 831\n",
      "neg max L (after clamping): 831\n"
     ]
    }
   ],
   "source": [
    "b = next(iter(negatives_loader))\n",
    "print(\"neg batch z pad:\", b[\"z\"].shape[1])\n",
    "print(\"neg max L (after clamping):\", int((b[\"tcra_len\"]+b[\"tcrb_len\"]+b[\"pep_len\"]+b[\"hla_len\"]).max().item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01f6fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ PLOTTING FUNCTIONS ------------\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_zstar_metrics(Zstar: torch.Tensor, eps: float = 1e-12):\n",
    "    \"\"\"\n",
    "    Zstar: (B, n, n) assumed symmetric (or close).\n",
    "    Returns dict of per-sample tensors (B,) for:\n",
    "      - diag_mean_mean\n",
    "      - offdiag_mean_mean\n",
    "      - z_eig_top1_share\n",
    "      - z_eig_top2_share\n",
    "      - z_trace\n",
    "    \"\"\"\n",
    "    # force symmetry for safety\n",
    "    Z = 0.5 * (Zstar + Zstar.transpose(-1, -2))  # (B,n,n)\n",
    "    B, n, _ = Z.shape\n",
    "\n",
    "    # diag / offdiag means\n",
    "    diag = torch.diagonal(Z, dim1=-2, dim2=-1)          # (B,n)\n",
    "    diag_mean_mean = diag.mean(dim=-1)                  # (B,)\n",
    "\n",
    "    total_sum = Z.sum(dim=(-2, -1))                     # (B,)\n",
    "    diag_sum  = diag.sum(dim=-1)                        # (B,)\n",
    "    off_sum   = total_sum - diag_sum                    # (B,)\n",
    "    off_count = n * (n - 1)\n",
    "    offdiag_mean_mean = off_sum / (off_count + eps)     # (B,)\n",
    "\n",
    "    # trace\n",
    "    z_trace = diag_sum                                  # (B,)\n",
    "\n",
    "    # eigenvalue shares (use abs so “dominance” is scale-invariant under sign flips)\n",
    "    evals = torch.linalg.eigvalsh(Z)                    # (B,n), ascending\n",
    "    abs_evals = evals.abs()\n",
    "\n",
    "    # sort descending for top shares\n",
    "    abs_sorted, _ = torch.sort(abs_evals, dim=-1, descending=True)  # (B,n)\n",
    "    denom = abs_sorted.sum(dim=-1) + eps                             # (B,)\n",
    "\n",
    "    z_eig_top1_share = abs_sorted[:, 0] / denom                      # (B,)\n",
    "    z_eig_top2_share = (abs_sorted[:, 0] + abs_sorted[:, 1]) / denom # (B,)\n",
    "\n",
    "    return {\n",
    "        \"diag_mean_mean\": diag_mean_mean.detach().cpu(),\n",
    "        \"offdiag_mean_mean\": offdiag_mean_mean.detach().cpu(),\n",
    "        \"z_eig_top1_share\": z_eig_top1_share.detach().cpu(),\n",
    "        \"z_eig_top2_share\": z_eig_top2_share.detach().cpu(),\n",
    "        \"z_trace\": z_trace.detach().cpu(),\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_zstar_histograms(metrics_pos: dict, metrics_neg: dict, bins: int = 30, suptitle: str = \"\"):\n",
    "    \"\"\"\n",
    "    metrics_* : dict[str, Tensor(B,)] from compute_zstar_metrics\n",
    "    Plots 5 histograms: diag_mean_mean, offdiag_mean_mean, z_eig_top1_share, z_eig_top2_share, z_trace\n",
    "    \"\"\"\n",
    "    keys = [\"diag_mean_mean\", \"offdiag_mean_mean\", \"z_eig_top1_share\", \"z_eig_top2_share\", \"z_trace\"]\n",
    "\n",
    "    plt.figure(figsize=(18, 4))\n",
    "    if suptitle:\n",
    "        plt.suptitle(suptitle)\n",
    "\n",
    "    for i, k in enumerate(keys, 1):\n",
    "        plt.subplot(1, 5, i)\n",
    "        x_pos = metrics_pos[k].numpy()\n",
    "        x_neg = metrics_neg[k].numpy()\n",
    "        plt.hist(x_pos, bins=bins, alpha=0.6, label=\"Positives\")\n",
    "        plt.hist(x_neg, bins=bins, alpha=0.6, label=\"Negatives\")\n",
    "        plt.title(k)\n",
    "        plt.xlabel(k)\n",
    "        plt.ylabel(\"Pairs\")\n",
    "        if i == 1:\n",
    "            plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def forward_to_Zstar_only(\n",
    "    batch_seq: dict,\n",
    "    batch_boltz: dict,\n",
    "    tcr_factorised,\n",
    "    pmhc_factorised,\n",
    "    boltz_factoriser,\n",
    "    gP,\n",
    "    gH,\n",
    "    device,\n",
    "    eps: float = 1e-8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs *inference-only* forward for a batch and returns Zstar (B,2d,2d).\n",
    "    Assumes batch_seq has emb/mask keys and batch_boltz has z + length keys.\n",
    "    \"\"\"\n",
    "    # sequence side\n",
    "    emb_T  = batch_seq[\"emb_T\"].to(device)\n",
    "    mask_T = batch_seq[\"mask_T\"].to(device)\n",
    "    emb_P  = batch_seq[\"emb_P\"].to(device)\n",
    "    mask_P = batch_seq[\"mask_P\"].to(device)\n",
    "    emb_H  = batch_seq[\"emb_H\"].to(device)\n",
    "    mask_H = batch_seq[\"mask_H\"].to(device)\n",
    "\n",
    "    zT  = tcr_factorised(emb_T, mask_T)\n",
    "    zT  = zT / (zT.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "    zPH = pmhc_factorised(emb_P, mask_P, emb_H, mask_H)\n",
    "    zPH = zPH / (zPH.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "    e_hat = torch.cat([zT, zPH], dim=-1)\n",
    "\n",
    "    # boltz side\n",
    "    z_boltz = batch_boltz[\"z\"].to(device)\n",
    "    L_p     = batch_boltz[\"pep_len\"].to(device)\n",
    "    L_alpha = batch_boltz[\"tcra_len\"].to(device)\n",
    "    L_beta  = batch_boltz[\"tcrb_len\"].to(device)\n",
    "    L_h     = batch_boltz[\"hla_len\"].to(device)\n",
    "\n",
    "    # Zstar\n",
    "    _, _, Zstar = non_contrastive_hamiltonian_loss(\n",
    "        zT=zT,\n",
    "        zPH=zPH,\n",
    "        e_hat=e_hat,\n",
    "        z_boltz_batch=z_boltz,\n",
    "        L_alpha=L_alpha,\n",
    "        L_beta=L_beta,\n",
    "        L_p=L_p,\n",
    "        L_h=L_h,\n",
    "        gP=gP,\n",
    "        gH=gH,\n",
    "        boltz_factoriser=boltz_factoriser,\n",
    "        alpha=1.0,\n",
    "        beta=1.0,\n",
    "        delta=1.0,\n",
    "        gamma_var=1.0,\n",
    "        eps=1e-4,\n",
    "        return_Zstar=True,\n",
    "        use_limit_Zstar=False,\n",
    "    )\n",
    "\n",
    "    return Zstar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5143b90",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab3504",
   "metadata": {},
   "source": [
    "##### a) Projection heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca830729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorised Encoder to get z_T and Z_pMHC\n",
    "# z = vec(A^TXB)H\n",
    "# X - (B, L_pad, D)\n",
    "# B - (D, rD)\n",
    "# A - (L_pad, rL)\n",
    "# H - (rD * rL, d)\n",
    "\n",
    "\n",
    "eps = 1e-8\n",
    "\n",
    "class ESMFactorisedEncoder(nn.Module):\n",
    "    def __init__(self, D, rL, rD, d, L_max):\n",
    "        \"\"\"\n",
    "        D    : ESM embedding dim (e.g. 960)\n",
    "        rL   : positional rank\n",
    "        rD   : channel rank\n",
    "        d    : latent dim\n",
    "        L_max: max true length for this modality in the batch\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.D   = D\n",
    "        self.rL  = rL\n",
    "        self.rD  = rD\n",
    "        self.d   = d\n",
    "        self.L_max = L_max\n",
    "\n",
    "        # Channel mixing: D -> rD\n",
    "        self.B_c = nn.Parameter(torch.empty(D, rD))\n",
    "        nn.init.xavier_uniform_(self.B_c)\n",
    "\n",
    "        # Positional mixing: positions 0..L_max-1 -> rL\n",
    "        self.A_c = nn.Parameter(torch.empty(L_max, rL))\n",
    "        nn.init.xavier_uniform_(self.A_c)\n",
    "\n",
    "        # Final map: (rL * rD) -> d\n",
    "        self.H_c = nn.Parameter(torch.empty(rL * rD, d))\n",
    "        nn.init.xavier_uniform_(self.H_c)\n",
    "\n",
    "    def forward(self, emb, mask):\n",
    "        \"\"\"\n",
    "        emb  : (B, L_pad, D) token embeddings\n",
    "        mask : (B, L_pad)   1 = real token, 0 = pad\n",
    "        returns z : (B, d)\n",
    "        \"\"\"\n",
    "        device = emb.device\n",
    "        B, L_pad, D = emb.shape\n",
    "        assert D == self.D\n",
    "\n",
    "        # Compute true lengths\n",
    "        L_true = mask.sum(dim=1)            # (B,)\n",
    "        z_list = []\n",
    "\n",
    "        for b in range(B):\n",
    "            Lb = int(L_true[b].item())\n",
    "            if Lb == 0:\n",
    "                # Degenerate case: no tokens -> zero vector\n",
    "                z_b = torch.zeros(self.d, device=device)\n",
    "                z_list.append(z_b)\n",
    "                continue\n",
    "\n",
    "            Xb = emb[b, :Lb, :]                      # (Lb, D)\n",
    "            mb = mask[b, :Lb].unsqueeze(-1).float()  # (Lb, 1)\n",
    "            Xb = Xb * mb                             # (Lb, D)\n",
    "\n",
    "            # 1) Channel compression: D -> rD\n",
    "            Yb = Xb @ self.B_c                       # (Lb, rD)\n",
    "\n",
    "            # 2) Positional compression: Lb -> rL\n",
    "            A_pos = self.A_c[:Lb, :]                 # (Lb, rL)\n",
    "            Ub = A_pos.T @ Yb                        # (rL, rD)\n",
    "\n",
    "            # 3) Flatten and map to latent d\n",
    "            Ub_flat = Ub.reshape(-1)                 # (rL * rD,)\n",
    "            z_b = Ub_flat @ self.H_c                 # (d,)\n",
    "\n",
    "            # 4) Normalise (optional; you can drop this if you want magnitude to carry info)\n",
    "            #z_b = z_b / (z_b.norm() + eps)\n",
    "            #normalise after function because need to combine p and hla first\n",
    "\n",
    "            z_list.append(z_b)\n",
    "\n",
    "        z = torch.stack(z_list, dim=0)               # (B, d)\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "670d3f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorised Encoder to get Z_pMHC with restrictions on pMHC dimensions \n",
    "# Calls ESMFactorisedEncoder for peptide and HLA separately\n",
    "\n",
    "class PMHCFactorisedEncoder(nn.Module):\n",
    "    def __init__(self, D, rL, rD, d, L_P_max, L_H_max, R_PH=0.7):\n",
    "        \"\"\"\n",
    "        D      : ESM embedding dim\n",
    "        rL     : positional rank\n",
    "        rD     : channel rank\n",
    "        d      : total latent dim for pMHC\n",
    "        L_P_max: max true peptide length\n",
    "        L_H_max: max true HLA length\n",
    "        R_PH   : fraction of d reserved for peptide (e.g. 0.7)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.D    = D\n",
    "        self.rL   = rL\n",
    "        self.rD   = rD\n",
    "        self.d    = d\n",
    "        self.R_PH = R_PH\n",
    "\n",
    "        # Split d into peptide and HLA sub-dims\n",
    "        d_P = int(round(R_PH * d))\n",
    "        d_H = d - d_P\n",
    "        assert d_P > 0 and d_H > 0, \"Choose d and R_PH so both > 0\"\n",
    "\n",
    "        self.d_P = d_P\n",
    "        self.d_H = d_H\n",
    "\n",
    "        # Separate factorised encoders for peptide and HLA\n",
    "        self.pep_encoder = ESMFactorisedEncoder(D, rL, rD, d_P, L_P_max)\n",
    "        self.hla_encoder = ESMFactorisedEncoder(D, rL, rD, d_H, L_H_max)\n",
    "\n",
    "    def forward(self, emb_P, mask_P, emb_H, mask_H):\n",
    "        \"\"\"\n",
    "        emb_P: (B, L_P_pad, D)\n",
    "        mask_P: (B, L_P_pad)\n",
    "        emb_H: (B, L_H_pad, D)\n",
    "        mask_H: (B, L_H_pad)\n",
    "        returns zPH: (B, d) with first d_P dims peptide, last d_H dims HLA\n",
    "        \"\"\"\n",
    "        eps = 1e-8\n",
    "        device = emb_P.device\n",
    "\n",
    "        zP = self.pep_encoder(emb_P, mask_P)  # (B, d_P)\n",
    "        zH = self.hla_encoder(emb_H, mask_H)  # (B, d_H)\n",
    "\n",
    "        # Optional: normalise segments separately so one can't trivially dominate\n",
    "        zP = zP / (zP.norm(dim=-1, keepdim=True) + eps)\n",
    "        zH = zH / (zH.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "        # Concatenate: 70% dims peptide, 30% dims HLA\n",
    "        zPH = torch.cat([zP, zH], dim=-1)     # (B, d)\n",
    "\n",
    "        # Optional: global normalisation\n",
    "        zPH = zPH / (zPH.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "        return zPH\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01874950",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoltzFactorised(nn.Module):\n",
    "    \"\"\"\n",
    "    Factorised Boltz embeddings for projection into latent shared space before NC loss\n",
    "\n",
    "    Inputs:\n",
    "    - z_boltz: (B, L_pad, L_pad, d_boltz) full Boltz z for the batch\n",
    "    - L_alpha, L_beta, L_p, L_h: (B,) lengths of the TCR alpha, TCR beta, peptide, HLA\n",
    "    - gP, gH: (B,) scalar or (B,) peptide/HLA gates in [0,1], norm-preserving in quadrature???? As in, gP**2 + gH**2 = 1\n",
    "\n",
    "    Outputs:\n",
    "    - Zstar_batch: (B, 2d, 2d) operator acting on e_hat_t, e_hat_pmc in R^2d\n",
    "    \"\"\"\n",
    "    def __init__(self, dB, rB, rT, rPH, d, L_max, L_PH_max):\n",
    "        \"\"\"\n",
    "        dB      : channel dimension of Boltz embeddings\n",
    "        rB      : rank of Boltz channel factorisation\n",
    "        rT      : rank of TCR positional encoding\n",
    "        rPH     : rank of pMHC positional encoding\n",
    "        d       : latent dimension of shared space\n",
    "        L_T_max   : maximum length of any sequence in the batch\n",
    "        L_PH_max: maximum length of any pMHC sequence in the batch\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dB     = dB\n",
    "        self.rB     = rB\n",
    "        self.rT     = rT\n",
    "        self.rPH    = rPH\n",
    "        self.d      = d\n",
    "        self.L_T_max  = L_max\n",
    "        self.L_PH_max = L_PH_max\n",
    "\n",
    "        # ---- 1) Channel mixing: dB -> rB ---- \n",
    "        self.B_Z = torch.nn.Parameter(torch.empty(dB, rB))\n",
    "        nn.init.xavier_uniform_(self.B_Z)\n",
    "\n",
    "        # ---- 2)a) TCR positional encoding: rT -> rT ---- \n",
    "        self.A_T = torch.nn.Parameter(torch.empty(L_T_max, rT))\n",
    "        nn.init.xavier_uniform_(self.A_T)\n",
    "\n",
    "        # ---- 2)b) pMHC positional encoding: rPH -> rPH ---- \n",
    "        self.A_PH = torch.nn.Parameter(torch.empty(L_PH_max, rPH))\n",
    "        nn.init.xavier_uniform_(self.A_PH)\n",
    "\n",
    "        # ---- 3) Learnable maps from factorised z (r* x r* x rB) -> d x d ---- \n",
    "        # flatten sizes for each block\n",
    "        n_TT   = rT  * rT  * rB\n",
    "        n_TPH  = rT  * rPH * rB\n",
    "        n_PHT  = rPH * rT  * rB\n",
    "        n_PHPH = rPH * rPH * rB\n",
    "        dd     = d * d\n",
    "\n",
    "        self.H_TT   = nn.Parameter(torch.empty(n_TT,   dd))\n",
    "        self.H_TPH  = nn.Parameter(torch.empty(n_TPH,  dd))\n",
    "        self.H_PHT  = nn.Parameter(torch.empty(n_PHT,  dd))\n",
    "        self.H_PHPH = nn.Parameter(torch.empty(n_PHPH, dd))\n",
    "\n",
    "        nn.init.xavier_uniform_(self.H_TT)\n",
    "        nn.init.xavier_uniform_(self.H_TPH)\n",
    "        nn.init.xavier_uniform_(self.H_PHT)\n",
    "        nn.init.xavier_uniform_(self.H_PHPH)\n",
    "\n",
    "        # ---- 4) Final linear layer: d -> d ---- \n",
    "        self.W_out = nn.Parameter(torch.empty(d, d))\n",
    "        nn.init.xavier_uniform_(self.W_out)\n",
    "    \n",
    "    def _get_gate_scalar(self, g, b):\n",
    "        \"\"\"\n",
    "        Helper: allow g to be a scalar tensor () or per-sample tensor (B,).\n",
    "        Returns a Python float for sample b.\n",
    "        \"\"\"\n",
    "        if g.dim() == 0:\n",
    "            return float(g.item())\n",
    "        else:\n",
    "            return float(g[b].item())\n",
    "\n",
    "    def forward(self, z_boltz, L_alpha, L_beta, L_p, L_h, gP, gH):\n",
    "        \"\"\"\n",
    "        Z_boltz : (B, L_pad, L_pad, dB)\n",
    "        L_alpha : (B,) true alpha lengths\n",
    "        L_beta  : (B,) true beta lengths\n",
    "        L_p     : (B,) true peptide lengths\n",
    "        L_h     : (B,) true HLA lengths\n",
    "        gP      : scalar () or (B,) peptide gate (already sqrt(R_PH))\n",
    "        gH      : scalar () or (B,) HLA gate (already sqrt(1-R_PH))\n",
    "\n",
    "        Returns:\n",
    "          Zstar_batch: (B, 2d, 2d)\n",
    "        \"\"\"\n",
    "\n",
    "        device = z_boltz.device\n",
    "        B, L_pad, _, dB = z_boltz.shape\n",
    "        assert dB == self.dB\n",
    "\n",
    "        Zstar_list = []\n",
    "\n",
    "        for b in range(B):\n",
    "            La  = int(L_alpha[b].item())\n",
    "            Lb  = int(L_beta[b].item())\n",
    "            Lp_ = int(L_p[b].item())\n",
    "            Lh_ = int(L_h[b].item())\n",
    "\n",
    "            L_T     = La + Lb\n",
    "            L_PH    = Lp_ + Lh_\n",
    "            L       = L_T + L_PH\n",
    "\n",
    "            # if we have missing z it just returns identity\n",
    "            if L == 0:\n",
    "                I_2d = torch.eye(2* self.d, device=device)\n",
    "                Zstar_list.append(I_2d)\n",
    "                continue\n",
    "            \n",
    "\n",
    "            # restrict to true tokens for the sample\n",
    "            Z = z_boltz[b, :L, :L, :] # (L, L, dB)\n",
    "\n",
    "            # ---- 2) Per channel normalisation (right now omit this step) ----\n",
    "            # another potential here is to use Adam optimiser to learn the normalisation\n",
    "            # mu = Z.mean(dim=(0,1), keepdim=True) # (1, 1, dB)\n",
    "            # std = Z.std(dim=(0,1), keepdim=True) # (1, 1, dB)\n",
    "            # Zc = (Z - mu) / std            # (L, L, dB)\n",
    "            # Zc = Zc / (math.sqrt(L) + eps) # scale by sqrt(L)\n",
    "            Zc = Z.clone()\n",
    "\n",
    "            # ----- 3) Gating ----\n",
    "            # TCR gates\n",
    "            if La > 0 and Lb > 0:\n",
    "                gA_b = 2**-0.5\n",
    "                gB_b = 2**-0.5\n",
    "            elif La > 0 and Lb == 0:\n",
    "                gA_b = 1\n",
    "                gB_b = 0\n",
    "            elif La == 0 and Lb > 0:\n",
    "                gA_b = 0\n",
    "                gB_b = 1\n",
    "            else:\n",
    "                gA_b = 0\n",
    "                gB_b = 0\n",
    "\n",
    "            # Peptide/HLA gates (from R set in encoder part)\n",
    "            gP_b = self._get_gate_scalar(gP, b)\n",
    "            gH_b = self._get_gate_scalar(gH, b)\n",
    "\n",
    "            # ---- 4) Build token-level gate vector over [alpha | beta | p | h] ----\n",
    "            gate = torch.zeros(L, device=device) # (L,)\n",
    "\n",
    "            idx0 = 0\n",
    "            idx1 = idx0 + La\n",
    "            idx2 = idx1 + Lb\n",
    "            idx3 = idx2 + Lp_\n",
    "            idx4 = idx3 + Lh_\n",
    "\n",
    "            if La  > 0: gate[idx0:idx1] = gA_b\n",
    "            if Lb  > 0: gate[idx1:idx2] = gB_b\n",
    "            if Lp_ > 0: gate[idx2:idx3] = gP_b\n",
    "            if Lh_ > 0: gate[idx3:idx4] = gH_b\n",
    "\n",
    "            gate_row = gate.view(L, 1, 1) # (L, 1, 1)\n",
    "            gate_col = gate.view(1, L, 1) # (1, L, 1)\n",
    "\n",
    "            Zg = Zc * gate_row * gate_col # (L, L, dB)\n",
    "\n",
    "            # ---- 5) Get TCR/pMHC blocks ----\n",
    "            sT = slice(0, L_T)           # [0, L_T) -> TCR (alpha + beta)\n",
    "            sPH = slice(L_T, L_T + L_PH) # [L_T, L] -> pMHC (P+H)\n",
    "\n",
    "            Z_TT  = Zg[sT, sT, :]    # (L_T, L_T, dB)\n",
    "            Z_TPH = Zg[sT, sPH, :]   # (L_T, L_PH, dB)\n",
    "            Z_PHT = Zg[sPH, sT, :]   # (L_PH, L_T, dB)\n",
    "            Z_PHPH = Zg[sPH, sPH, :] # (L_PH, L_PH, dB)\n",
    "\n",
    "            # ---- 6) channel/dimension compression ----\n",
    "            B_Z = self.B_Z # (dB, rB) operator across channels\n",
    "            Y_TT   = torch.einsum('ijc,cr->ijr', Z_TT,   B_Z)   # (L_T,  L_T,  rB)\n",
    "            Y_TPH  = torch.einsum('ijc,cr->ijr', Z_TPH,  B_Z)   # (L_T,  L_PH, rB)\n",
    "            Y_PHT  = torch.einsum('ijc,cr->ijr', Z_PHT,  B_Z)   # (L_PH, L_T,  rB)\n",
    "            Y_PHPH = torch.einsum('ijc,cr->ijr', Z_PHPH, B_Z)   # (L_PH, L_PH, rB)\n",
    "\n",
    "            # ---- 7) TCR positional compression with A_T / A_PH ----\n",
    "            # Sample-specific rows for the correct lengths for the per-sample positional tensors\n",
    "            if L_T > 0:\n",
    "                A_T_b = self.A_T[:L_T, :] # (L_T, rT)\n",
    "            else: \n",
    "                # no TCRs, so treat as zero contribution (or could be noise, decide later)\n",
    "                A_T_b = self.A_T[:1, :] * 0.0 # (1, rT) dummy\n",
    "\n",
    "            if L_PH > 0:\n",
    "                A_PH_b = self.A_PH[:L_PH, :] # (L_PH, rPH)\n",
    "            else:\n",
    "                # no pMHCs, so treat as zero contribution (or could be noise, decide later)\n",
    "                A_PH_b = self.A_PH[:1, :] * 0.0 # (1, rPH) dummy\n",
    "            \n",
    "            rT  = self.rT\n",
    "            rPH = self.rPH\n",
    "            rB  = self.rB\n",
    "            d   = self.d\n",
    "\n",
    "            # N.B. U tensors are not learned, they are discarded as intermediary steps for compression\n",
    "            # TCR-TCR (L_T, L_T, rB) -> (rT, rT, rB)\n",
    "            if L_T > 0:\n",
    "                U_TT = torch.einsum('ip,ijr->pjr', A_T_b, Y_TT) # (rT, L_T, rB)\n",
    "                V_TT = torch.einsum('pjr,jq->pqr', U_TT, A_T_b) # (rT, rT, rB)\n",
    "            else:\n",
    "                V_TT = torch.zeros(rT, rT, rB, device=device) # (rT, rT, rB)\n",
    "\n",
    "            # TCR–pMHC: (L_T, L_PH, rB) -> (rT, rPH, rB)\n",
    "            if L_T > 0 and L_PH > 0:\n",
    "                U_TPH = torch.einsum('ip,ijr->pjr', A_T_b,  Y_TPH)   # (rT,  L_PH, rB)\n",
    "                V_TPH = torch.einsum('pjr,jq->pqr', U_TPH, A_PH_b)   # (rT,  rPH, rB)\n",
    "            else:\n",
    "                V_TPH = torch.zeros(rT, rPH, rB, device=device)\n",
    "\n",
    "            # pMHC–TCR: (L_PH, L_T, rB) -> (rPH, rT, rB)\n",
    "            if L_T > 0 and L_PH > 0:\n",
    "                U_PHT = torch.einsum('ip,ijr->pjr', A_PH_b, Y_PHT)   # (rPH, L_T,  rB)\n",
    "                V_PHT = torch.einsum('pjr,jq->pqr', U_PHT, A_T_b)    # (rPH, rT,  rB)\n",
    "            else:\n",
    "                V_PHT = torch.zeros(rPH, rT, rB, device=device)\n",
    "\n",
    "            # pMHC–pMHC: (L_PH, L_PH, rB) -> (rPH, rPH, rB)\n",
    "            if L_PH > 0:\n",
    "                U_PHPH = torch.einsum('ip,ijr->pjr', A_PH_b, Y_PHPH) # (rPH, L_PH, rB)\n",
    "                V_PHPH = torch.einsum('pjr,jq->pqr', U_PHPH, A_PH_b) # (rPH, rPH, rB)\n",
    "            else:\n",
    "                V_PHPH = torch.zeros(rPH, rPH, rB, device=device)\n",
    "\n",
    "            # ---- 8) Flatten factorised blocks and map to d×d via H_* ----\n",
    "            v_TT_flat   = V_TT.reshape(-1)    # (rT*rT*rB,)\n",
    "            v_TPH_flat  = V_TPH.reshape(-1)   # (rT*rPH*rB,)\n",
    "            v_PHT_flat  = V_PHT.reshape(-1)   # (rPH*rT*rB,)\n",
    "            v_PHPH_flat = V_PHPH.reshape(-1)  # (rPH*rPH*rB,)\n",
    "\n",
    "            k_TT_flat   = v_TT_flat   @ self.H_TT   # (d*d,)\n",
    "            k_TPH_flat  = v_TPH_flat  @ self.H_TPH  # (d*d,)\n",
    "            k_PHT_flat  = v_PHT_flat  @ self.H_PHT  # (d*d,)\n",
    "            k_PHPH_flat = v_PHPH_flat @ self.H_PHPH # (d*d,)\n",
    "\n",
    "            K_TT   = k_TT_flat.view(d, d)\n",
    "            K_TPH  = k_TPH_flat.view(d, d)\n",
    "            K_PHT  = k_PHT_flat.view(d, d)\n",
    "            K_PHPH = k_PHPH_flat.view(d, d)\n",
    "\n",
    "            # Optional: enforce symmetry on diagonal blocks\n",
    "            # Enforce symmetry on all blocks?\n",
    "            # K_TT   = 0.5 * (K_TT   + K_TT.t())\n",
    "            # K_PHPH = 0.5 * (K_PHPH + K_PHPH.t())\n",
    "            # Yes - everywhere\n",
    "\n",
    "            # ---- 9) Assemble 2d x 2d operator for this sample ----\n",
    "            I_d = torch.eye(d, device=device)\n",
    "            Zstar_b = torch.zeros(2*d, 2*d, device=device)\n",
    "\n",
    "            # Zstar_b[:d,  :d]  = I_d + K_TT\n",
    "            # Zstar_b[:d,  d:]  = I_d + K_TPH\n",
    "            # Zstar_b[d:,  :d]  = I_d + K_PHT\n",
    "            # Zstar_b[d:,  d:]  = I_d + K_PHPH\n",
    "\n",
    "\n",
    "            scale_K = 0.01  # start tiny (0.001–0.05 range)\n",
    "            Zstar_b[:d,  :d]  = I_d + scale_K * K_TT\n",
    "            Zstar_b[:d,  d:]  = I_d + scale_K * K_TPH\n",
    "            Zstar_b[d:,  :d]  = I_d + scale_K * K_PHT\n",
    "            Zstar_b[d:,  d:]  = I_d + scale_K * K_PHPH\n",
    "\n",
    "\n",
    "            Zstar_list.append(Zstar_b)\n",
    "\n",
    "        Zstar_batch = torch.stack(Zstar_list, dim=0)  # (B, 2d, 2d)\n",
    "        # symmetrise whole matrix\n",
    "        Zstar_batch = 0.5 * (Zstar_batch + Zstar_batch.transpose(1, 2)) # or at beginning\n",
    "        # also enforce within the learning too\n",
    "        return Zstar_batch \n",
    "\n",
    "\n",
    "# index convention for the einsum\n",
    "# i = row token position, “i” is historically “index” or “first axis”\n",
    "# j\t= column token position, second positional axis (matrix-like)\n",
    "# c\t= channel\n",
    "# p, q\t= latent positional modes, P = projection / latent position\n",
    "# r = latent channel modes, R = rank / channel rank\n",
    "# b\t= batch index, B = batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d5161",
   "metadata": {},
   "source": [
    "##### b) Batch Loop with Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d75f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingTripletDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path):\n",
    "        data = torch.load(path)\n",
    "        self.emb_T = data[\"emb_T\"]   # (N, L_T_pad, D)\n",
    "        self.emb_P = data[\"emb_P\"]   # (N, L_P_pad, D)\n",
    "        self.emb_H = data[\"emb_H\"]   # (N, L_H_pad, D)\n",
    "\n",
    "        self.mask_T = data[\"mask_T\"] # (N, L_T_pad)\n",
    "        self.mask_P = data[\"mask_P\"]\n",
    "        self.mask_H = data[\"mask_H\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.emb_T.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"emb_T\":  self.emb_T[idx],\n",
    "            \"mask_T\": self.mask_T[idx],\n",
    "            \"emb_P\":  self.emb_P[idx],\n",
    "            \"mask_P\": self.mask_P[idx],\n",
    "            \"emb_H\":  self.emb_H[idx],\n",
    "            \"mask_H\": self.mask_H[idx],\n",
    "        }\n",
    "\n",
    "train_dataset = EmbeddingTripletDataset(\"train_embeddings.pt\")\n",
    "train_loader  = DataLoader(train_dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79caf85",
   "metadata": {},
   "source": [
    "c) Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7f97dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def vicreg_variance(u, gamma=1.0, eps=1e-4):\n",
    "    \"\"\"\n",
    "    u: (B, d) embeddings for one modality (TCR or pMHC)\n",
    "    gamma: minimum desired std per dimension (VICReg uses gamma=1.0)\n",
    "    \"\"\"\n",
    "    B, d = u.shape\n",
    "    u_centered = u - u.mean(dim=0, keepdim=True)            # (B, d), make mean 0\n",
    "    #std = torch.sqrt(u_centered.var(dim=0) + eps)           # (d,)\n",
    "    std = torch.sqrt(u_centered.var(dim=0, unbiased=False) + eps)\n",
    "\n",
    "    # (1/d) * sum_j ReLU(gamma - std_j)\n",
    "    var_loss = F.relu(gamma - std).mean()\n",
    "    return var_loss\n",
    "\n",
    "\n",
    "def vicreg_covariance(u, eps=1e-4):\n",
    "    \"\"\"\n",
    "    u: (B, d) embeddings for one modality (TCR or pMHC)\n",
    "    Returns (1/d^2) * sum_{j!=k} Cov(u_j, u_k)^2\n",
    "    \"\"\"\n",
    "    B, d = u.shape\n",
    "    u_centered = u - u.mean(dim=0, keepdim=True)            # (B, d)\n",
    "\n",
    "    # covariance matrix C = (u^T u) / (B-1)\n",
    "    cov = (u_centered.T @ u_centered) / (B - 1)             # (d, d)\n",
    "\n",
    "    # zero diag, keep off-diagonals, as we don't want diagonal terms (variances)\n",
    "    diag = torch.diag(cov)\n",
    "    cov_off = cov - torch.diag_embed(diag)\n",
    "\n",
    "    cov_loss = (cov_off ** 2).sum() / (d * d)\n",
    "    return cov_loss\n",
    "\n",
    "# Z spectral variance loss - similar to VICreg variance but for Zstar\n",
    "def Z_spectral_variance_loss(Z, target_ratio=1.5, iters=5, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Encourages the top eigenvalue not to dominate the spectrum.\n",
    "    target_ratio ≈ lambda_max / mean_lambda\n",
    "    \"\"\"\n",
    "    B, n, _ = Z.shape\n",
    "\n",
    "    # Frobenius norm gives sum of squared eigenvalues\n",
    "    frob = Z.flatten(1).norm(dim=1)  # (B,)\n",
    "    mean_lambda = frob / (n ** 0.5 + eps)\n",
    "\n",
    "    # Power iteration for largest eigenvalue\n",
    "    v = torch.randn(B, n, 1, device=Z.device)\n",
    "    v = v / (v.norm(dim=1, keepdim=True) + eps)\n",
    "\n",
    "    for _ in range(iters):\n",
    "        v = torch.bmm(Z, v)\n",
    "        v = v / (v.norm(dim=1, keepdim=True) + eps)\n",
    "\n",
    "    Zv = torch.bmm(Z, v)\n",
    "    lambda_max = torch.bmm(v.transpose(1,2), Zv).squeeze(-1).squeeze(-1)\n",
    "    lambda_max = lambda_max.abs()\n",
    "\n",
    "    ratio = lambda_max / (mean_lambda + eps)\n",
    "\n",
    "    # Penalise if ratio too large\n",
    "    return torch.relu(ratio - target_ratio).pow(2).mean()\n",
    "\n",
    "\n",
    "# Z isotropy loss - similar to VICreg covariance but for Zstar\n",
    "def Z_isotropy_loss(Z, num_vec=4, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Penalises directional anisotropy of Z.\n",
    "    \"\"\"\n",
    "    B, n, _ = Z.shape\n",
    "    norms = []\n",
    "\n",
    "    for _ in range(num_vec):\n",
    "        v = torch.randn(B, n, device=Z.device)\n",
    "        v = v / (v.norm(dim=1, keepdim=True) + eps)\n",
    "\n",
    "        Zv = torch.bmm(Z, v.unsqueeze(-1)).squeeze(-1)\n",
    "        norms.append(Zv.norm(dim=1))\n",
    "\n",
    "    norms = torch.stack(norms, dim=1)\n",
    "    return norms.var(dim=1).mean()\n",
    "\n",
    "\n",
    "# third regulariser - anchor Zstar norms to initial Zstar norms\n",
    "def Z_anchor_loss(Z, Z0_norm):\n",
    "    Znorm = Z.flatten(1).norm(dim=1).mean()\n",
    "    return (Znorm - Z0_norm).pow(2), Znorm\n",
    "\n",
    "\n",
    "def Z_eig_floor_loss(Z, tau=-5.0, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Penalise Z if its minimum eigenvalue drops below tau.\n",
    "    For symmetric Z, eigvalsh is stable and returns real eigenvalues.\n",
    "\n",
    "    Z: (B, n, n), assumed (approximately) symmetric.\n",
    "    tau: floor. If lambda_min < tau => penalty (tau - lambda_min)^2.\n",
    "    Returns: (loss_scalar, lambda_min_mean, lambda_min_min)\n",
    "    \"\"\"\n",
    "    # Force symmetry for numerical safety (cheap)\n",
    "    Zsym = 0.5 * (Z + Z.transpose(-1, -2))\n",
    "\n",
    "    # Eigenvalues for each sample (B, n)\n",
    "    evals = torch.linalg.eigvalsh(Zsym)\n",
    "\n",
    "    # Minimum eigenvalue per sample (B,)\n",
    "    lam_min = evals[:, 0]\n",
    "\n",
    "    # Penalise if below floor\n",
    "    penalty = torch.relu(tau - lam_min + eps).pow(2).mean()\n",
    "\n",
    "    return penalty, lam_min.mean().item(), lam_min.min().item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "748d6f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_contrastive_hamiltonian_loss(\n",
    "    zT,\n",
    "    zPH,\n",
    "    e_hat,\n",
    "    z_boltz_batch,\n",
    "    L_alpha,\n",
    "    L_beta,\n",
    "    L_p,\n",
    "    L_h,\n",
    "    gP,\n",
    "    gH,\n",
    "    boltz_factoriser,\n",
    "    alpha=1.0,\n",
    "    beta=1.0,\n",
    "    delta=1.0,\n",
    "    gamma_var=1.0,\n",
    "    eps=1e-4,\n",
    "    return_Zstar=False,\n",
    "    use_limit_Zstar=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Base loss only:\n",
    "      L_total_base = alpha * L_inv + beta * (L_var_T + L_var_PH) + delta * (L_cov_T + L_cov_PH)\n",
    "\n",
    "    Z-regularisers (anchor/spectral/isotropy) should be added in the training loop\n",
    "    because anchor needs state across steps (Z0_norm).\n",
    "    \"\"\"\n",
    "\n",
    "    device = e_hat.device\n",
    "    B, two_d = e_hat.shape\n",
    "    d = two_d // 2\n",
    "\n",
    "    # ---- 1) Boltz → Z★(n) ----\n",
    "    if use_limit_Zstar:\n",
    "        I = torch.eye(d, device=device).unsqueeze(0).expand(B, d, d)\n",
    "        Zstar = torch.zeros(B, 2*d, 2*d, device=device)\n",
    "        Zstar[:, :d, :d] = I\n",
    "        Zstar[:, :d, d:] = I\n",
    "        Zstar[:, d:, :d] = I\n",
    "        Zstar[:, d:, d:] = I\n",
    "    else:\n",
    "        Zstar = boltz_factoriser(\n",
    "            z_boltz_batch.to(device),\n",
    "            L_alpha.to(device),\n",
    "            L_beta.to(device),\n",
    "            L_p.to(device),\n",
    "            L_h.to(device),\n",
    "            gP.to(device),\n",
    "            gH.to(device),\n",
    "        )  # (B, 2d, 2d)\n",
    "\n",
    "        # enforce symmetry at loss level\n",
    "        Zstar = 0.5 * (Zstar + Zstar.transpose(-1, -2))\n",
    "\n",
    "    # ---- 2) Hamiltonian proxy ----\n",
    "    quad = torch.einsum(\"bi,bij,bj->b\", e_hat, Zstar, e_hat)  # (B,)\n",
    "    H = -0.5 * quad                                           # (B,)\n",
    "    L_inv = H.mean()\n",
    "\n",
    "    # ---- 3) VICReg variance/covariance ----\n",
    "    L_var_T  = vicreg_variance(zT,  gamma=gamma_var, eps=eps)\n",
    "    L_var_PH = vicreg_variance(zPH, gamma=gamma_var, eps=eps)\n",
    "\n",
    "    L_cov_T  = vicreg_covariance(zT,  eps=eps)\n",
    "    L_cov_PH = vicreg_covariance(zPH, eps=eps)\n",
    "\n",
    "    L_var_total = L_var_T + L_var_PH\n",
    "    L_cov_total = L_cov_T + L_cov_PH\n",
    "\n",
    "    # ---- 4) Base total ----\n",
    "    L_total_base = alpha * L_inv + beta * L_var_total + delta * L_cov_total\n",
    "\n",
    "    components = {\n",
    "        \"L_total_base\": L_total_base.item(),\n",
    "        \"L_inv\":        L_inv.item(),\n",
    "        \"L_var_T\":      L_var_T.item(),\n",
    "        \"L_var_PH\":     L_var_PH.item(),\n",
    "        \"L_cov_T\":      L_cov_T.item(),\n",
    "        \"L_cov_PH\":     L_cov_PH.item(),\n",
    "    }\n",
    "\n",
    "    # limit-case diagnostics\n",
    "    if use_limit_Zstar:\n",
    "        cos = (zT * zPH).sum(dim=-1)  # (B,)\n",
    "        H_expected = -1.0 - cos\n",
    "        components[\"H_expected_mean\"] = H_expected.mean().item()\n",
    "        components[\"H_actual_mean\"]   = H.mean().item()\n",
    "        components[\"H_max_abs_diff\"]  = (H - H_expected).abs().max().item()\n",
    "        components[\"cos_mean\"]        = cos.mean().item()\n",
    "        components[\"H_min\"]           = H.min().item()\n",
    "        components[\"H_max\"]           = H.max().item()\n",
    "\n",
    "    if return_Zstar:\n",
    "        return L_total_base, components, Zstar\n",
    "    else:\n",
    "        return L_total_base, components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9633f782",
   "metadata": {},
   "source": [
    "Training Loop with Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c58d5142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "neg z shape: torch.Size([8, 824, 824, 128])\n",
      "neg max L from lengths: 824\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): subscript i has size 438 for operand 1 which does not broadcast with previously seen size 236",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 158\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneg z shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, z_boltz_neg\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# (B, L_pad, L_pad, dB)\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneg max L from lengths:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28mint\u001b[39m((L_alpha_neg \u001b[38;5;241m+\u001b[39m L_beta_neg \u001b[38;5;241m+\u001b[39m L_p_neg \u001b[38;5;241m+\u001b[39m L_h_neg)\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()))\n\u001b[0;32m--> 158\u001b[0m     Zstar_neg \u001b[38;5;241m=\u001b[39m \u001b[43mboltz_factoriser\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mz_boltz_neg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mL_alpha_neg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mL_beta_neg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mL_p_neg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mL_h_neg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgH\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     metrics_neg \u001b[38;5;241m=\u001b[39m compute_zstar_metrics(Zstar_neg)\n\u001b[1;32m    169\u001b[0m boltz_factoriser\u001b[38;5;241m.\u001b[39mtrain(was_train)\n",
      "File \u001b[0;32m~/miniconda3/envs/tcr-multimodal/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tcr-multimodal/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[36], line 200\u001b[0m, in \u001b[0;36mBoltzFactorised.forward\u001b[0;34m(self, z_boltz, L_alpha, L_beta, L_p, L_h, gP, gH)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# N.B. U tensors are not learned, they are discarded as intermediary steps for compression\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# TCR-TCR (L_T, L_T, rB) -> (rT, rT, rB)\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m L_T \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 200\u001b[0m     U_TT \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mip,ijr->pjr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_T_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_TT\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (rT, L_T, rB)\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     V_TT \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpjr,jq->pqr\u001b[39m\u001b[38;5;124m'\u001b[39m, U_TT, A_T_b) \u001b[38;5;66;03m# (rT, rT, rB)\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tcr-multimodal/lib/python3.10/site-packages/torch/functional.py:373\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    375\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: einsum(): subscript i has size 438 for operand 1 which does not broadcast with previously seen size 236"
     ]
    }
   ],
   "source": [
    "##### ------------------ TRAINING WITH Z CONSTRAINTS ------------------\n",
    "\n",
    "# ---- Hyperparams ----\n",
    "B, L_T_pad, D = emb_T.shape\n",
    "rL = 8\n",
    "rD = 16\n",
    "d  = 128\n",
    "\n",
    "eps = 1e-8  # you use eps in normalisation; define it explicitly\n",
    "\n",
    "# rL = 8\n",
    "# rD = 16\n",
    "# d  = 128\n",
    "\n",
    "R_PH = 0.7\n",
    "gP = torch.tensor(R_PH ** 0.5, device=device)        # scalar gate\n",
    "gH = torch.tensor((1.0 - R_PH) ** 0.5, device=device)\n",
    "\n",
    "alpha = 1.0\n",
    "beta  = 1.0\n",
    "delta = 1.0\n",
    "gamma_var = 1.0\n",
    "\n",
    "# ---- Z regulariser weights ----\n",
    "lambda_anchor = 1e-6\n",
    "lambda_spec   = 0 #remove for  now\n",
    "lambda_iso    = 0 #remove for  now\n",
    "\n",
    "target_ratio = 1.5\n",
    "spec_iters = 5\n",
    "iso_num_vec = 4\n",
    "\n",
    "# ---- Z eigenvalue floor ----\n",
    "lambda_eigfloor = 1e-1    # start small; you may need 1e-1 if L_inv still runs away\n",
    "tau_eigfloor    = -5.0    # floor on smallest eigenvalue (tune this)\n",
    "\n",
    "# ---- Global max lengths ----\n",
    "L_T_max = train_dataset.emb_T.shape[1]\n",
    "L_P_max = train_dataset.emb_P.shape[1]\n",
    "L_H_max = train_dataset.emb_H.shape[1]\n",
    "\n",
    "# ---- Models ----\n",
    "tcr_factorised  = ESMFactorisedEncoder(D, rL, rD, d, L_max=L_T_max).to(device)\n",
    "pmhc_factorised = PMHCFactorisedEncoder(\n",
    "    D, rL, rD, d,\n",
    "    L_P_max=L_P_max,\n",
    "    L_H_max=L_H_max,\n",
    "    R_PH=R_PH\n",
    ").to(device)\n",
    "\n",
    "boltz_factoriser = BoltzFactorised(\n",
    "    dB=128,\n",
    "    rB=16,\n",
    "    rT=8,\n",
    "    rPH=8,\n",
    "    d=d,\n",
    "    L_max=L_T_max_boltz,\n",
    "    L_PH_max=L_PH_max_boltz,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": tcr_factorised.parameters(),   \"lr\": 1e-3},\n",
    "    {\"params\": pmhc_factorised.parameters(),  \"lr\": 1e-3},\n",
    "    {\"params\": boltz_factoriser.parameters(), \"lr\": 1e-5},\n",
    "])\n",
    "\n",
    "# ---- Anchored-norm state ----\n",
    "Z0_norm = None\n",
    "anchor_ema = 0.99  # your 0.99/0.01 EMA\n",
    "\n",
    "neg_iter = iter(negatives_loader)  # your existing loader\n",
    "# If your negatives have a separate boltz loader, do the same:\n",
    "# neg_boltz_iter = iter(boltz_negatives_loader)\n",
    "plot_every = 50\n",
    "\n",
    "\n",
    "for step, (batch, boltz_batch) in enumerate(zip(train_loader, boltz_loader)):\n",
    "\n",
    "    print(step)\n",
    "\n",
    "    # -----------------------\n",
    "    # 1) SEQUENCE SIDE\n",
    "    # -----------------------\n",
    "    emb_T  = batch[\"emb_T\"].to(device)\n",
    "    mask_T = batch[\"mask_T\"].to(device)\n",
    "    emb_P  = batch[\"emb_P\"].to(device)\n",
    "    mask_P = batch[\"mask_P\"].to(device)\n",
    "    emb_H  = batch[\"emb_H\"].to(device)\n",
    "    mask_H = batch[\"mask_H\"].to(device)\n",
    "\n",
    "    zT  = tcr_factorised(emb_T, mask_T)\n",
    "    zT  = zT / (zT.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "    zPH = pmhc_factorised(emb_P, mask_P, emb_H, mask_H)\n",
    "    zPH = zPH / (zPH.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "    e_hat = torch.cat([zT, zPH], dim=-1)\n",
    "\n",
    "    # -----------------------\n",
    "    # 2) BOLTZ SIDE\n",
    "    # -----------------------\n",
    "    z_boltz = boltz_batch[\"z\"]\n",
    "    L_p     = boltz_batch[\"pep_len\"]\n",
    "    L_alpha = boltz_batch[\"tcra_len\"]\n",
    "    L_beta  = boltz_batch[\"tcrb_len\"]\n",
    "    L_h     = boltz_batch[\"hla_len\"]\n",
    "\n",
    "    # -----------------------\n",
    "    # 3) Base loss forward + Zstar\n",
    "    # -----------------------\n",
    "    loss_base, components, Zstar = non_contrastive_hamiltonian_loss(\n",
    "        zT=zT,\n",
    "        zPH=zPH,\n",
    "        e_hat=e_hat,\n",
    "        z_boltz_batch=z_boltz,\n",
    "        L_alpha=L_alpha,\n",
    "        L_beta=L_beta,\n",
    "        L_p=L_p,\n",
    "        L_h=L_h,\n",
    "        gP=gP,\n",
    "        gH=gH,\n",
    "        boltz_factoriser=boltz_factoriser,\n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        delta=delta,\n",
    "        gamma_var=gamma_var,\n",
    "        eps=1e-4,\n",
    "        return_Zstar=True,\n",
    "        use_limit_Zstar=False,\n",
    "    )\n",
    "\n",
    "    # -----------------------\n",
    "    # PLOTTING HISTOGRAMS (inside loop, but does NOT gate training)\n",
    "    # -----------------------\n",
    "    if step % plot_every == 0:\n",
    "        with torch.no_grad():\n",
    "            metrics_pos = compute_zstar_metrics(Zstar)\n",
    "\n",
    "        try:\n",
    "            boltz_batch_neg = next(neg_iter)\n",
    "        except StopIteration:\n",
    "            neg_iter = iter(negatives_loader)\n",
    "            boltz_batch_neg = next(neg_iter)\n",
    "\n",
    "        z_boltz_neg = boltz_batch_neg[\"z\"].to(device)\n",
    "        L_p_neg     = boltz_batch_neg[\"pep_len\"].to(device)\n",
    "        L_alpha_neg = boltz_batch_neg[\"tcra_len\"].to(device)\n",
    "        L_beta_neg  = boltz_batch_neg[\"tcrb_len\"].to(device)\n",
    "        L_h_neg     = boltz_batch_neg[\"hla_len\"].to(device)\n",
    "\n",
    "        was_train = boltz_factoriser.training\n",
    "        boltz_factoriser.eval()\n",
    "        with torch.no_grad():\n",
    "            print(\"neg z shape:\", z_boltz_neg.shape)  # (B, L_pad, L_pad, dB)\n",
    "            print(\"neg max L from lengths:\",\n",
    "                int((L_alpha_neg + L_beta_neg + L_p_neg + L_h_neg).max().item()))\n",
    "\n",
    "            Zstar_neg = boltz_factoriser(\n",
    "                z_boltz_neg,\n",
    "                L_alpha_neg,\n",
    "                L_beta_neg,\n",
    "                L_p_neg,\n",
    "                L_h_neg,\n",
    "                gP.to(device),\n",
    "                gH.to(device),\n",
    "            )\n",
    "            metrics_neg = compute_zstar_metrics(Zstar_neg)\n",
    "\n",
    "        boltz_factoriser.train(was_train)\n",
    "\n",
    "        plot_zstar_histograms(\n",
    "            metrics_pos,\n",
    "            metrics_neg,\n",
    "            bins=30,\n",
    "            suptitle=f\"Z★ features (step={step}) — Positives vs Negatives\"\n",
    "        )\n",
    "\n",
    "    # -----------------------\n",
    "    # 4) Z REGULARISERS (ALWAYS computed every step)\n",
    "    # -----------------------\n",
    "    L_anchor_raw, Znorm_mean = Z_anchor_loss(Zstar, Z0_norm if Z0_norm is not None else 0.0)\n",
    "\n",
    "    if Z0_norm is None:\n",
    "        Z0_norm = Znorm_mean.detach()\n",
    "    else:\n",
    "        Z0_norm = 0.99 * Z0_norm + 0.01 * Znorm_mean.detach()\n",
    "\n",
    "    L_anchor = lambda_anchor * L_anchor_raw\n",
    "    L_spec   = lambda_spec * Z_spectral_variance_loss(Zstar, target_ratio=target_ratio, iters=spec_iters)\n",
    "    L_iso    = lambda_iso  * Z_isotropy_loss(Zstar, num_vec=iso_num_vec)\n",
    "\n",
    "    L_eigfloor_raw, lam_min_mean, lam_min_min = Z_eig_floor_loss(Zstar, tau=tau_eigfloor)\n",
    "    L_eigfloor = lambda_eigfloor * L_eigfloor_raw\n",
    "\n",
    "    loss = loss_base + L_anchor + L_spec + L_iso + L_eigfloor\n",
    "\n",
    "    # log\n",
    "    Znorm_per_sample = Zstar.flatten(1).norm(dim=1)\n",
    "    components.update({\n",
    "        \"L_total\":        loss.item(),\n",
    "        \"L_anchor\":       L_anchor.item(),\n",
    "        \"L_spec\":         L_spec.item(),\n",
    "        \"L_iso\":          L_iso.item(),\n",
    "        \"L_eigfloor\":     L_eigfloor.item(),\n",
    "        \"Znorm_mean\":     Znorm_mean.item(),\n",
    "        \"Z0_mean\":        Z0_norm.item(),\n",
    "        \"Znorm_max\":      Znorm_per_sample.max().item(),\n",
    "        \"lam_min_mean\":   lam_min_mean,\n",
    "        \"lam_min_min\":    lam_min_min,\n",
    "        \"Zstar_abs_mean\": Zstar.abs().mean().item(),\n",
    "        \"Zstar_max_abs\":  Zstar.abs().max().item(),\n",
    "    })\n",
    "\n",
    "    # -----------------------\n",
    "    # 5) Pre-step prints\n",
    "    # -----------------------\n",
    "    if step == 0:\n",
    "        print(\"=== FIRST BATCH (pre-step) ===\")\n",
    "        print(components)\n",
    "        print(\"zT norm mean:\",  zT.norm(dim=-1).mean().item())\n",
    "        print(\"zPH norm mean:\", zPH.norm(dim=-1).mean().item())\n",
    "\n",
    "    # -----------------------\n",
    "    # 6) Backward\n",
    "    # -----------------------\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "\n",
    "    # optional: print every step again (you had this at end)\n",
    "    print(components)\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(boltz_factoriser.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabaa254",
   "metadata": {},
   "source": [
    "Training Loop without Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc0a7ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "=== FIRST BATCH (pre-step) ===\n",
      "{'L_total_base': 0.8405921459197998, 'L_inv': -1.0792791843414307, 'L_var_T': 0.9572292566299438, 'L_var_PH': 0.962637722492218, 'L_cov_T': 9.38673224482045e-07, 'L_cov_PH': 3.3443009215261554e-06, 'L_total': 0.8416640758514404, 'L_anchor': 0.0010719199199229479, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 32.74018859863281, 'Z0_mean': 32.74018859863281, 'Znorm_max': 33.624874114990234, 'Zstar_abs_mean': 0.07694073766469955, 'Zstar_max_abs': 1.5871294736862183}\n",
      "zT norm mean: 1.0\n",
      "zPH norm mean: 1.0\n",
      "grad ||tcr||: 3.052505971247148\n",
      "grad ||pmhc||: 2.700382748012117\n",
      "grad ||boltz||: 9.091772057260457\n",
      "term_A mean/max: 0.97406405210495 1.1129586696624756\n",
      "term_B mean/max: 1.0660457611083984 1.3093974590301514\n",
      "term_C mean/max: 0.11844874918460846 0.19927363097667694\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 32.74018859863281 33.624874114990234\n",
      "Z0 mean: 32.74018859863281\n",
      "{'L_total_base': 0.8405921459197998, 'L_inv': -1.0792791843414307, 'L_var_T': 0.9572292566299438, 'L_var_PH': 0.962637722492218, 'L_cov_T': 9.38673224482045e-07, 'L_cov_PH': 3.3443009215261554e-06, 'L_total': 0.8416640758514404, 'L_anchor': 0.0010719199199229479, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 32.74018859863281, 'Z0_mean': 32.74018859863281, 'Znorm_max': 33.624874114990234, 'Zstar_abs_mean': 0.07694073766469955, 'Zstar_max_abs': 1.5871294736862183, 'L_eigfloor': 0.0, 'lam_min_mean': -2.6681227684020996, 'lam_min_min': -2.9029853343963623}\n",
      "1\n",
      "term_A mean/max: 1.6516116857528687 1.7059895992279053\n",
      "term_B mean/max: 1.327394723892212 1.4982490539550781\n",
      "term_C mean/max: 0.8329610824584961 1.1441190242767334\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 33.163631439208984 34.013065338134766\n",
      "Z0 mean: 32.744422912597656\n",
      "{'L_total_base': 0.02022930420935154, 'L_inv': -1.9059836864471436, 'L_var_T': 0.9612693190574646, 'L_var_PH': 0.9649421572685242, 'L_cov_T': 7.106071961970883e-07, 'L_cov_PH': 8.032955065573333e-07, 'L_total': 0.020229483023285866, 'L_anchor': 1.7930383933162375e-07, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 33.163631439208984, 'Z0_mean': 32.744422912597656, 'Znorm_max': 34.013065338134766, 'Zstar_abs_mean': 0.07850518077611923, 'Zstar_max_abs': 1.549753189086914, 'L_eigfloor': 0.0, 'lam_min_mean': -2.7415013313293457, 'lam_min_min': -2.936993360519409}\n",
      "2\n",
      "term_A mean/max: 2.1334218978881836 2.423257350921631\n",
      "term_B mean/max: 1.4162551164627075 1.6262035369873047\n",
      "term_C mean/max: 1.3757026195526123 1.5683155059814453\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 33.100006103515625 33.81802749633789\n",
      "Z0 mean: 32.747982025146484\n",
      "{'L_total_base': -0.53678959608078, 'L_inv': -2.4626898765563965, 'L_var_T': 0.9616833329200745, 'L_var_PH': 0.9642139673233032, 'L_cov_T': 9.954044344340218e-07, 'L_cov_PH': 1.9191277260688366e-06, 'L_total': -0.5367894768714905, 'L_anchor': 1.2643940294765343e-07, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 33.100006103515625, 'Z0_mean': 32.747982025146484, 'Znorm_max': 33.81802749633789, 'Zstar_abs_mean': 0.07816262543201447, 'Zstar_max_abs': 1.5425664186477661, 'L_eigfloor': 0.0, 'lam_min_mean': -2.7164196968078613, 'lam_min_min': -2.8454580307006836}\n",
      "3\n",
      "term_A mean/max: 2.465498685836792 2.822991371154785\n",
      "term_B mean/max: 1.5811463594436646 1.7697714567184448\n",
      "term_C mean/max: 1.9658377170562744 2.109330654144287\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 33.28099060058594 34.160675048828125\n",
      "Z0 mean: 32.75331115722656\n",
      "{'L_total_base': -1.07386314868927, 'L_inv': -3.0062413215637207, 'L_var_T': 0.9629569053649902, 'L_var_PH': 0.9694193601608276, 'L_cov_T': 4.6903022621336277e-07, 'L_cov_PH': 1.4197587461239891e-06, 'L_total': -1.073862910270691, 'L_anchor': 2.8409814945007383e-07, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 33.28099060058594, 'Z0_mean': 32.75331115722656, 'Znorm_max': 34.160675048828125, 'Zstar_abs_mean': 0.07878193259239197, 'Zstar_max_abs': 1.559786319732666, 'L_eigfloor': 0.0, 'lam_min_mean': -2.738642692565918, 'lam_min_min': -2.9694342613220215}\n",
      "4\n",
      "term_A mean/max: 2.778252601623535 2.959306478500366\n",
      "term_B mean/max: 1.7103443145751953 1.8809230327606201\n",
      "term_C mean/max: 2.3296191692352295 2.404386520385742\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 32.940673828125 33.71785354614258\n",
      "Z0 mean: 32.755184173583984\n",
      "{'L_total_base': -1.4694640636444092, 'L_inv': -3.4091081619262695, 'L_var_T': 0.9688433408737183, 'L_var_PH': 0.9707998037338257, 'L_cov_T': 2.8672195639956044e-07, 'L_cov_PH': 6.592965746676782e-07, 'L_total': -1.4694640636444092, 'L_anchor': 3.510476886958713e-08, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 32.940673828125, 'Z0_mean': 32.755184173583984, 'Znorm_max': 33.71785354614258, 'Zstar_abs_mean': 0.07754464447498322, 'Zstar_max_abs': 1.6029866933822632, 'L_eigfloor': 0.0, 'lam_min_mean': -2.662823438644409, 'lam_min_min': -2.7856431007385254}\n",
      "5\n",
      "term_A mean/max: 3.235260486602783 3.522480010986328\n",
      "term_B mean/max: 1.8438780307769775 1.9056038856506348\n",
      "term_C mean/max: 2.8282032012939453 2.952573299407959\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 33.32421875 34.362361907958984\n",
      "Z0 mean: 32.76087188720703\n",
      "{'L_total_base': -2.011643886566162, 'L_inv': -3.9536709785461426, 'L_var_T': 0.9710299968719482, 'L_var_PH': 0.9709963798522949, 'L_cov_T': 1.8348998764849966e-07, 'L_cov_PH': 6.130096608103486e-07, 'L_total': -2.011643648147583, 'L_anchor': 3.238003500882769e-07, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 33.32421875, 'Z0_mean': 32.76087188720703, 'Znorm_max': 34.362361907958984, 'Zstar_abs_mean': 0.0790587067604065, 'Zstar_max_abs': 1.6105067729949951, 'L_eigfloor': 0.0, 'lam_min_mean': -2.721193790435791, 'lam_min_min': -2.9732136726379395}\n",
      "6\n",
      "term_A mean/max: 3.8071131706237793 4.097410202026367\n",
      "term_B mean/max: 2.0490493774414062 2.168205738067627\n",
      "term_C mean/max: 3.2917985916137695 3.413231372833252\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 33.862632751464844 34.54997253417969\n",
      "Z0 mean: 32.771888732910156\n",
      "{'L_total_base': -2.624014139175415, 'L_inv': -4.573980808258057, 'L_var_T': 0.9757389426231384, 'L_var_PH': 0.9742271900177002, 'L_cov_T': 8.584758859342401e-08, 'L_cov_PH': 3.532970822561765e-07, 'L_total': -2.6240129470825195, 'L_anchor': 1.21387699891784e-06, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 33.862632751464844, 'Z0_mean': 32.771888732910156, 'Znorm_max': 34.54997253417969, 'Zstar_abs_mean': 0.08070990443229675, 'Zstar_max_abs': 1.5341893434524536, 'L_eigfloor': 0.0, 'lam_min_mean': -2.8423969745635986, 'lam_min_min': -2.9876179695129395}\n",
      "7\n",
      "term_A mean/max: 3.9456307888031006 4.206984519958496\n",
      "term_B mean/max: 2.246619462966919 2.3639957904815674\n",
      "term_C mean/max: 3.6615543365478516 3.8270020484924316\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 33.76707458496094 34.643123626708984\n",
      "Z0 mean: 32.78184127807617\n",
      "{'L_total_base': -2.975107192993164, 'L_inv': -4.926902770996094, 'L_var_T': 0.97409588098526, 'L_var_PH': 0.9776995182037354, 'L_cov_T': 8.397223894007766e-08, 'L_cov_PH': 7.606799101722572e-08, 'L_total': -2.9751062393188477, 'L_anchor': 9.903949376166565e-07, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 33.76707458496094, 'Z0_mean': 32.78184127807617, 'Znorm_max': 34.643123626708984, 'Zstar_abs_mean': 0.08078661561012268, 'Zstar_max_abs': 1.6183931827545166, 'L_eigfloor': 0.0, 'lam_min_mean': -2.7795963287353516, 'lam_min_min': -2.9762284755706787}\n",
      "8\n",
      "term_A mean/max: 4.366969585418701 4.709043025970459\n",
      "term_B mean/max: 2.376633405685425 2.438462018966675\n",
      "term_C mean/max: 4.055475234985352 4.263920307159424\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 33.55878448486328 34.7598762512207\n",
      "Z0 mean: 32.789608001708984\n",
      "{'L_total_base': -3.4437170028686523, 'L_inv': -5.399538993835449, 'L_var_T': 0.9763456583023071, 'L_var_PH': 0.9794760346412659, 'L_cov_T': 5.6785665947245434e-08, 'L_cov_PH': 1.1809270006324368e-07, 'L_total': -3.443716287612915, 'L_anchor': 6.036407285137102e-07, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 33.55878448486328, 'Z0_mean': 32.789608001708984, 'Znorm_max': 34.7598762512207, 'Zstar_abs_mean': 0.0794868916273117, 'Zstar_max_abs': 1.5665712356567383, 'L_eigfloor': 0.0, 'lam_min_mean': -2.7294187545776367, 'lam_min_min': -2.976754665374756}\n",
      "9\n",
      "term_A mean/max: 5.026423454284668 5.307099342346191\n",
      "term_B mean/max: 2.6322741508483887 2.7918553352355957\n",
      "term_C mean/max: 4.511255264282227 4.659682750701904\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 34.43989562988281 35.221343994140625\n",
      "Z0 mean: 32.80611038208008\n",
      "{'L_total_base': -4.127717018127441, 'L_inv': -6.0849761962890625, 'L_var_T': 0.9780799150466919, 'L_var_PH': 0.9791795015335083, 'L_cov_T': 4.3486096501510474e-08, 'L_cov_PH': 4.194480140995438e-08, 'L_total': -4.127714157104492, 'L_anchor': 2.7234491426497698e-06, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 34.43989562988281, 'Z0_mean': 32.80611038208008, 'Znorm_max': 35.221343994140625, 'Zstar_abs_mean': 0.0825597494840622, 'Zstar_max_abs': 1.5596174001693726, 'L_eigfloor': 0.0, 'lam_min_mean': -2.873938798904419, 'lam_min_min': -3.0008862018585205}\n",
      "10\n",
      "term_A mean/max: 5.5020928382873535 5.813536167144775\n",
      "term_B mean/max: 2.753726005554199 2.859466075897217\n",
      "term_C mean/max: 5.038987636566162 5.321328163146973\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 34.36842346191406 35.0925407409668\n",
      "Z0 mean: 32.82173538208008\n",
      "{'L_total_base': -4.6829094886779785, 'L_inv': -6.647403240203857, 'L_var_T': 0.9811240434646606, 'L_var_PH': 0.9833698272705078, 'L_cov_T': 1.5075135806341677e-08, 'L_cov_PH': 3.62793599606448e-08, 'L_total': -4.6829071044921875, 'L_anchor': 2.4408220724581042e-06, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 34.36842346191406, 'Z0_mean': 32.82173538208008, 'Znorm_max': 35.0925407409668, 'Zstar_abs_mean': 0.08214471489191055, 'Zstar_max_abs': 1.6070640087127686, 'L_eigfloor': 0.0, 'lam_min_mean': -2.854515790939331, 'lam_min_min': -3.006798505783081}\n",
      "11\n",
      "term_A mean/max: 5.701440811157227 6.21292781829834\n",
      "term_B mean/max: 2.866988182067871 3.073014259338379\n",
      "term_C mean/max: 5.397226810455322 5.655472755432129\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 33.8980712890625 34.7422981262207\n",
      "Z0 mean: 32.83250045776367\n",
      "{'L_total_base': -5.016797065734863, 'L_inv': -6.982828140258789, 'L_var_T': 0.9838137626647949, 'L_var_PH': 0.9822170734405518, 'L_cov_T': 1.3047478653049893e-08, 'L_cov_PH': 3.248511859510472e-08, 'L_total': -5.016796112060547, 'L_anchor': 1.1584990033952636e-06, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 33.8980712890625, 'Z0_mean': 32.83250045776367, 'Znorm_max': 34.7422981262207, 'Zstar_abs_mean': 0.08077168464660645, 'Zstar_max_abs': 1.616367220878601, 'L_eigfloor': 0.0, 'lam_min_mean': -2.735461711883545, 'lam_min_min': -2.912433385848999}\n",
      "12\n",
      "term_A mean/max: 5.871006488800049 6.206693649291992\n",
      "term_B mean/max: 3.168187141418457 3.308495044708252\n",
      "term_C mean/max: 5.733527183532715 5.897137641906738\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 33.589317321777344 34.13768768310547\n",
      "Z0 mean: 32.84006881713867\n",
      "{'L_total_base': -5.417607307434082, 'L_inv': -7.386360168457031, 'L_var_T': 0.9847467541694641, 'L_var_PH': 0.9840062856674194, 'L_cov_T': 1.3695691691850698e-08, 'L_cov_PH': 2.1891271018148473e-08, 'L_total': -5.417606830596924, 'L_anchor': 5.727717962145107e-07, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 33.589317321777344, 'Z0_mean': 32.84006881713867, 'Znorm_max': 34.13768768310547, 'Zstar_abs_mean': 0.07966683804988861, 'Zstar_max_abs': 1.5408499240875244, 'L_eigfloor': 0.0, 'lam_min_mean': -2.627443313598633, 'lam_min_min': -2.703632354736328}\n"
     ]
    }
   ],
   "source": [
    "##### ------------------ TRAINING WITH Z CONSTRAINTS ------------------\n",
    "\n",
    "# ---- Hyperparams ----\n",
    "B, L_T_pad, D = emb_T.shape\n",
    "rL = 8\n",
    "rD = 16\n",
    "d  = 128\n",
    "\n",
    "eps = 1e-8  # you use eps in normalisation; define it explicitly\n",
    "\n",
    "# rL = 8\n",
    "# rD = 16\n",
    "# d  = 128\n",
    "\n",
    "R_PH = 0.7\n",
    "gP = torch.tensor(R_PH ** 0.5, device=device)        # scalar gate\n",
    "gH = torch.tensor((1.0 - R_PH) ** 0.5, device=device)\n",
    "\n",
    "alpha = 1.0\n",
    "beta  = 1.0\n",
    "delta = 1.0\n",
    "gamma_var = 1.0\n",
    "\n",
    "# ---- Z regulariser weights ----\n",
    "lambda_anchor = 1e-6\n",
    "lambda_spec   = 0 #remove for  now\n",
    "lambda_iso    = 0 #remove for  now\n",
    "\n",
    "target_ratio = 1.5\n",
    "spec_iters = 5\n",
    "iso_num_vec = 4\n",
    "\n",
    "# ---- Z eigenvalue floor ----\n",
    "lambda_eigfloor = 1e-1    # start small; you may need 1e-1 if L_inv still runs away\n",
    "tau_eigfloor    = -5.0    # floor on smallest eigenvalue (tune this)\n",
    "\n",
    "# ---- Global max lengths ----\n",
    "L_T_max = train_dataset.emb_T.shape[1]\n",
    "L_P_max = train_dataset.emb_P.shape[1]\n",
    "L_H_max = train_dataset.emb_H.shape[1]\n",
    "\n",
    "# ---- Models ----\n",
    "tcr_factorised  = ESMFactorisedEncoder(D, rL, rD, d, L_max=L_T_max).to(device)\n",
    "pmhc_factorised = PMHCFactorisedEncoder(\n",
    "    D, rL, rD, d,\n",
    "    L_P_max=L_P_max,\n",
    "    L_H_max=L_H_max,\n",
    "    R_PH=R_PH\n",
    ").to(device)\n",
    "\n",
    "boltz_factoriser = BoltzFactorised(\n",
    "    dB=128,\n",
    "    rB=16,\n",
    "    rT=8,\n",
    "    rPH=8,\n",
    "    d=d,\n",
    "    L_max=L_T_max_boltz,\n",
    "    L_PH_max=L_PH_max_boltz,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": tcr_factorised.parameters(),   \"lr\": 1e-3},\n",
    "    {\"params\": pmhc_factorised.parameters(),  \"lr\": 1e-3},\n",
    "    {\"params\": boltz_factoriser.parameters(), \"lr\": 1e-5},\n",
    "])\n",
    "\n",
    "# ---- Anchored-norm state ----\n",
    "Z0_norm = None\n",
    "anchor_ema = 0.99  # your 0.99/0.01 EMA\n",
    "\n",
    "for step, (batch, boltz_batch) in enumerate(zip(train_loader, boltz_loader)):\n",
    "\n",
    "    print(step)\n",
    "\n",
    "    # -----------------------\n",
    "    # 1) SEQUENCE SIDE\n",
    "    # -----------------------\n",
    "    emb_T  = batch[\"emb_T\"].to(device)\n",
    "    mask_T = batch[\"mask_T\"].to(device)\n",
    "    emb_P  = batch[\"emb_P\"].to(device)\n",
    "    mask_P = batch[\"mask_P\"].to(device)\n",
    "    emb_H  = batch[\"emb_H\"].to(device)\n",
    "    mask_H = batch[\"mask_H\"].to(device)\n",
    "\n",
    "    zT  = tcr_factorised(emb_T, mask_T)                 # (B, d)\n",
    "    zT  = zT / (zT.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "    zPH = pmhc_factorised(emb_P, mask_P, emb_H, mask_H) # (B, d)\n",
    "    zPH = zPH / (zPH.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "    e_hat = torch.cat([zT, zPH], dim=-1)                # (B, 2d)\n",
    "\n",
    "    # -----------------------\n",
    "    # 2) BOLTZ SIDE\n",
    "    # -----------------------\n",
    "    z_boltz = boltz_batch[\"z\"]\n",
    "    L_p     = boltz_batch[\"pep_len\"]\n",
    "    L_alpha = boltz_batch[\"tcra_len\"]\n",
    "    L_beta  = boltz_batch[\"tcrb_len\"]\n",
    "    L_h     = boltz_batch[\"hla_len\"]\n",
    "\n",
    "    # -----------------------\n",
    "    # 3) Base loss forward + Zstar\n",
    "    # -----------------------\n",
    "    loss_base, components, Zstar = non_contrastive_hamiltonian_loss(\n",
    "        zT=zT,\n",
    "        zPH=zPH,\n",
    "        e_hat=e_hat,\n",
    "        z_boltz_batch=z_boltz,\n",
    "        L_alpha=L_alpha,\n",
    "        L_beta=L_beta,\n",
    "        L_p=L_p,\n",
    "        L_h=L_h,\n",
    "        gP=gP,\n",
    "        gH=gH,\n",
    "        boltz_factoriser=boltz_factoriser,\n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        delta=delta,\n",
    "        gamma_var=gamma_var,\n",
    "        eps=1e-4,\n",
    "        return_Zstar=True,\n",
    "        use_limit_Zstar=False,\n",
    "    )\n",
    "\n",
    "    # -----------------------\n",
    "    # 4) Z REGULARISERS (computed on SAME Zstar)\n",
    "    # -----------------------\n",
    "    # -----------------------\n",
    "    # 2b) Z-regularisers (ALL computed on the SAME Zstar)\n",
    "    # -----------------------\n",
    "\n",
    "    # (i) anchor loss (your existing)\n",
    "    L_anchor_raw, Znorm_mean = Z_anchor_loss(Zstar, Z0_norm if Z0_norm is not None else 0.0)\n",
    "    # initialise/update anchor target\n",
    "    if Z0_norm is None:\n",
    "        Z0_norm = Znorm_mean.detach()\n",
    "    else:\n",
    "        Z0_norm = 0.99 * Z0_norm + 0.01 * Znorm_mean.detach()\n",
    "\n",
    "    L_anchor = lambda_anchor * L_anchor_raw\n",
    "\n",
    "    # (ii) spectral ratio penalty (your existing)\n",
    "    L_spec = lambda_spec * Z_spectral_variance_loss(Zstar, target_ratio=target_ratio, iters=spec_iters)\n",
    "\n",
    "    # (iii) isotropy penalty (your existing)\n",
    "    L_iso = lambda_iso * Z_isotropy_loss(Zstar, num_vec=iso_num_vec)\n",
    "\n",
    "    # (iv) NEW: eigenvalue floor penalty (prevents runaway negative curvature)\n",
    "    L_eigfloor_raw, lam_min_mean, lam_min_min = Z_eig_floor_loss(Zstar, tau=tau_eigfloor)\n",
    "    L_eigfloor = lambda_eigfloor * L_eigfloor_raw\n",
    "\n",
    "    # Final loss\n",
    "    loss = loss_base + L_anchor + L_spec + L_iso + L_eigfloor\n",
    "\n",
    "\n",
    "    # log\n",
    "    Znorm_per_sample = Zstar.flatten(1).norm(dim=1)  # (B,)\n",
    "    components.update({\n",
    "        \"L_total\":       loss.item(),\n",
    "        \"L_anchor\":      L_anchor.item(),\n",
    "        \"L_spec\":        L_spec.item(),\n",
    "        \"L_iso\":         L_iso.item(),\n",
    "        \"Znorm_mean\":    Znorm_mean.item(),\n",
    "        \"Z0_mean\":       Z0_norm.item(),\n",
    "        \"Znorm_max\":     Znorm_per_sample.max().item(),\n",
    "        \"Zstar_abs_mean\": Zstar.abs().mean().item(),\n",
    "        \"Zstar_max_abs\":  Zstar.abs().max().item(),\n",
    "    })\n",
    "\n",
    "    # -----------------------\n",
    "    # 5) Pre-step prints\n",
    "    # -----------------------\n",
    "    if step == 0:\n",
    "        print(\"=== FIRST BATCH (pre-step) ===\")\n",
    "        print(components)\n",
    "        print(\"zT norm mean:\",  zT.norm(dim=-1).mean().item())\n",
    "        print(\"zPH norm mean:\", zPH.norm(dim=-1).mean().item())\n",
    "\n",
    "    # -----------------------\n",
    "    # 6) Backward\n",
    "    # -----------------------\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "\n",
    "    if step == 0:\n",
    "        def grad_norm(m):\n",
    "            tot = 0.0\n",
    "            for p in m.parameters():\n",
    "                if p.grad is not None:\n",
    "                    tot += p.grad.detach().float().norm().item()**2\n",
    "            return tot**0.5\n",
    "\n",
    "        print(\"grad ||tcr||:\",   grad_norm(tcr_factorised))\n",
    "        print(\"grad ||pmhc||:\",  grad_norm(pmhc_factorised))\n",
    "        print(\"grad ||boltz||:\", grad_norm(boltz_factoriser))\n",
    "\n",
    "    # -----------------------\n",
    "    # 7) Debug: decompose quadratic into blocks (same Zstar)\n",
    "    # -----------------------\n",
    "    d_local = zT.shape[-1]\n",
    "    A_block = Zstar[:, :d_local, :d_local]\n",
    "    C_block = Zstar[:, :d_local, d_local:]\n",
    "    B_block = Zstar[:, d_local:, d_local:]\n",
    "\n",
    "    term_A = torch.einsum(\"bi,bij,bj->b\", zT,  A_block, zT)\n",
    "    term_B = torch.einsum(\"bi,bij,bj->b\", zPH, B_block, zPH)\n",
    "    term_C = 2.0 * torch.einsum(\"bi,bij,bj->b\", zT, C_block, zPH)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"term_A mean/max:\", term_A.mean().item(), term_A.abs().max().item())\n",
    "        print(\"term_B mean/max:\", term_B.mean().item(), term_B.abs().max().item())\n",
    "        print(\"term_C mean/max:\", term_C.mean().item(), term_C.abs().max().item())\n",
    "\n",
    "        sym_err = (Zstar - Zstar.transpose(-1, -2)).abs().max().item()\n",
    "        print(\"Zstar symmetry max|Z-ZT|:\", sym_err)\n",
    "        print(\"Znorm mean/max:\", Znorm_mean.item(), Znorm_per_sample.max().item())\n",
    "        print(\"Z0 mean:\", Z0_norm.item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        components[\"L_total_base\"] = float(loss_base.item())\n",
    "        components[\"L_total\"]      = float(loss.item())\n",
    "\n",
    "        components[\"L_anchor\"]     = float(L_anchor.item())\n",
    "        components[\"L_spec\"]       = float(L_spec.item())\n",
    "        components[\"L_iso\"]        = float(L_iso.item())\n",
    "        components[\"L_eigfloor\"]   = float(L_eigfloor.item())\n",
    "\n",
    "        components[\"lam_min_mean\"] = lam_min_mean\n",
    "        components[\"lam_min_min\"]  = lam_min_min\n",
    "\n",
    "        # Useful Z stats\n",
    "        Zflat = Zstar.flatten(1)\n",
    "        components[\"Zstar_abs_mean\"] = float(Zstar.abs().mean().item())\n",
    "        components[\"Zstar_max_abs\"]  = float(Zstar.abs().max().item())\n",
    "\n",
    "\n",
    "    print(components)\n",
    "\n",
    "    # -----------------------\n",
    "    # 8) Gradient clipping (after backward, before step)\n",
    "    # -----------------------\n",
    "    torch.nn.utils.clip_grad_norm_(boltz_factoriser.parameters(), max_norm=1.0) #was 10, but getting huge gradients\n",
    "\n",
    "    # -----------------------\n",
    "    # 9) Optimiser step\n",
    "    # -----------------------\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862961c1",
   "metadata": {},
   "source": [
    "##### Previous Z constraints and Limit Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e88fbe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_contrastive_hamiltonian_loss(\n",
    "    zT,\n",
    "    zPH,\n",
    "    e_hat,\n",
    "    z_boltz_batch,\n",
    "    L_alpha,\n",
    "    L_beta,\n",
    "    L_p,\n",
    "    L_h,\n",
    "    gP,\n",
    "    gH,\n",
    "    boltz_factoriser,\n",
    "    alpha=1.0,\n",
    "    beta=1.0,\n",
    "    delta=1.0,\n",
    "    gamma_var=1.0,\n",
    "    eps=1e-4,\n",
    "    return_Zstar=False,\n",
    "    use_limit_Zstar=False,          # limit case\n",
    "    lambda_Z=0.0,                   # regulariser weight\n",
    "):\n",
    "    \"\"\"\n",
    "    zT, zPH    : (B, d) TCR and pMHC embeddings (the two halves of e_hat)\n",
    "    e_hat      : (B, 2d) concatenation [zT || zPH]\n",
    "    z_boltz_*  : Boltz inputs, as in your BoltzFactorised.forward\n",
    "    boltz_factoriser : instance of BoltzFactorised\n",
    "\n",
    "    alpha : weight on invariance (Hamiltonian) term\n",
    "    beta  : weight on variance terms\n",
    "    delta : weight on covariance terms\n",
    "    gamma_var : VICReg variance threshold (usually 1.0)\n",
    "    \"\"\"\n",
    "\n",
    "    device = e_hat.device\n",
    "    B, two_d = e_hat.shape\n",
    "    d = two_d // 2\n",
    "\n",
    "\n",
    "    # ---- 1) Boltz → Z★(n) for each sample ---- (B, 2d, 2d)\n",
    "    \n",
    "    # Try to simulate limit case - all identity matrixes\n",
    "    if use_limit_Zstar:\n",
    "        # paper limit: Z* = [[I, I],[I, I]]\n",
    "        I = torch.eye(d, device=device).unsqueeze(0).expand(B, d, d)\n",
    "        Zstar = torch.zeros(B, 2*d, 2*d, device=device)\n",
    "        Zstar[:, :d, :d] = I\n",
    "        Zstar[:, :d, d:] = I\n",
    "        Zstar[:, d:, :d] = I\n",
    "        Zstar[:, d:, d:] = I\n",
    "    else:\n",
    "        Zstar = boltz_factoriser(\n",
    "        z_boltz_batch.to(device),\n",
    "        L_alpha.to(device),\n",
    "        L_beta.to(device),\n",
    "        L_p.to(device),\n",
    "        L_h.to(device),\n",
    "        gP.to(device),\n",
    "        gH.to(device),\n",
    "    )  # (B, 2d, 2d)\n",
    "        # symmetrise output every forward pass\n",
    "        # even if this is done in the factoriser, doing it here is cheap and guarantees it\n",
    "        Zstar = 0.5 * (Zstar + Zstar.transpose(-1, -2))\n",
    "        \n",
    "\n",
    "    # ---- 2) Hamiltonian proxy per sample: H^(n) = -1/2 e^T Z* e ----\n",
    "    # einsum: 'bi,bij,bj->b' gives e^T Z* e for each n\n",
    "    quad = torch.einsum(\"bi,bij,bj->b\", e_hat, Zstar, e_hat)  # (B,)\n",
    "    H = -0.5 * quad                                           # (B,)\n",
    "\n",
    "    if use_limit_Zstar:\n",
    "        cos = (zT * zPH).sum(dim=-1)          # (B,) since both unit norm\n",
    "        H_expected = -1.0 - cos\n",
    "        max_diff = (H - H_expected).abs().max().item()\n",
    "\n",
    "\n",
    "    # Invariance term: average Hamiltonian over batch\n",
    "    L_inv = H.mean()\n",
    "\n",
    "    # ---- 3) Variance & covariance terms per block (TCR and pMHC) ----\n",
    "    # zT and zPH are each (B, d)\n",
    "    L_var_T  = vicreg_variance(zT,  gamma=gamma_var, eps=eps)\n",
    "    L_var_PH = vicreg_variance(zPH, gamma=gamma_var, eps=eps)\n",
    "\n",
    "    L_cov_T  = vicreg_covariance(zT,  eps=eps)\n",
    "    L_cov_PH = vicreg_covariance(zPH, eps=eps)\n",
    "\n",
    "    # ---- 4) Combine as in your equation (4) ----\n",
    "    L_var_total = L_var_T + L_var_PH\n",
    "    L_cov_total = L_cov_T + L_cov_PH\n",
    "\n",
    "    L_total = (\n",
    "        alpha * L_inv\n",
    "        + beta * L_var_total\n",
    "        + delta * L_cov_total\n",
    "    )\n",
    "\n",
    "    components = {\n",
    "        \"L_total\":    L_total.item(),\n",
    "        \"L_inv\":      L_inv.item(),\n",
    "        \"L_var_T\":    L_var_T.item(),\n",
    "        \"L_var_PH\":   L_var_PH.item(),\n",
    "        \"L_cov_T\":    L_cov_T.item(),\n",
    "        \"L_cov_PH\":   L_cov_PH.item(),\n",
    "    }\n",
    "\n",
    "    if use_limit_Zstar:\n",
    "        components[\"H_expected_mean\"] = H_expected.mean().item()\n",
    "        components[\"H_actual_mean\"]   = H.mean().item()\n",
    "        components[\"H_max_abs_diff\"]  = max_diff\n",
    "        components[\"cos_mean\"]        = cos.mean().item()\n",
    "        components[\"H_min\"]           = H.min().item()\n",
    "        components[\"H_max\"]           = H.max().item()\n",
    "\n",
    "\n",
    "    if return_Zstar:\n",
    "        return L_total, components, Zstar\n",
    "    else:\n",
    "        return L_total, components\n",
    "\n",
    "    #return L_total, components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c58fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "=== FIRST BATCH (pre-step) ===\n",
      "{'L_total': 5.370140075683594, 'L_inv': 3.4459939002990723, 'L_var_T': 0.9593005180358887, 'L_var_PH': 0.9648425579071045, 'L_cov_T': 7.847547749406658e-07, 'L_cov_PH': 2.0807594864891144e-06, 'L_anchor': 0.0, 'Znorm_mean': 1849.241455078125, 'Z0_mean': 1849.241455078125, 'Znorm_max': 1927.605712890625}\n",
      "zT norm mean: 1.0\n",
      "zPH norm mean: 1.0\n",
      "grad ||tcr||: 216.6576049986074\n",
      "grad ||pmhc||: 137.79171387229712\n",
      "grad ||boltz||: 737.664848521035\n",
      "term_A mean/max: -9.978446960449219 30.804811477661133\n",
      "term_B mean/max: 0.6554511785507202 8.332881927490234\n",
      "term_C mean/max: 2.431009531021118 9.845224380493164\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 5.154047012329102\n",
      "Zstar max abs: 58.64965057373047\n",
      "Znorm mean/max: 1849.241455078125 1927.605712890625\n",
      "Z0 mean: 1849.241455078125\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'approx_spectral_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 204\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZnorm mean/max:\u001b[39m\u001b[38;5;124m\"\u001b[39m, Znorm\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem(), Znorm\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZ0 mean:\u001b[39m\u001b[38;5;124m\"\u001b[39m, Z0_norm\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m--> 204\u001b[0m     lam \u001b[38;5;241m=\u001b[39m \u001b[43mapprox_spectral_norm\u001b[49m(Zstar, iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda_max approx mean/max:\u001b[39m\u001b[38;5;124m\"\u001b[39m, lam\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem(), lam\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mprint\u001b[39m(components)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'approx_spectral_norm' is not defined"
     ]
    }
   ],
   "source": [
    "##### ------------------ ADDING Z CONSTRAINTS ------------------\n",
    "\n",
    "# ---- Hyperparams ----\n",
    "B, L_T_pad, D = emb_T.shape\n",
    "rL = 8\n",
    "rD = 16\n",
    "d  = 128\n",
    "\n",
    "R_PH = 0.7\n",
    "gP_scalar = R_PH ** 0.5\n",
    "gH_scalar = (1.0 - R_PH) ** 0.5\n",
    "gP = torch.tensor(gP_scalar, device=device)   # scalar gates\n",
    "gH = torch.tensor(gH_scalar, device=device)\n",
    "\n",
    "alpha = 1.0   # weight on invariance term\n",
    "beta  = 1.0   # weight on var terms\n",
    "delta = 1.0   # weight on cov terms\n",
    "\n",
    "# ---- Get global max lengths for ESM factorised encoders ----\n",
    "L_T_max = train_dataset.emb_T.shape[1]\n",
    "L_P_max = train_dataset.emb_P.shape[1]\n",
    "L_H_max = train_dataset.emb_H.shape[1]\n",
    "\n",
    "# do this outside of the batch training loop so the weights are not reinitialised for each batch\n",
    "tcr_factorised  = ESMFactorisedEncoder(D, rL, rD, d, L_max=L_T_max).to(device)\n",
    "pmhc_factorised = PMHCFactorisedEncoder(\n",
    "    D, rL, rD, d,\n",
    "    L_P_max=L_P_max,\n",
    "    L_H_max=L_H_max,\n",
    "    R_PH=R_PH\n",
    ").to(device)\n",
    "\n",
    "# ---- Boltz factoriser (choose sensible hyperparams) ----\n",
    "# dB = channel dimension of Boltz embeddings, e.g. boltz_batch[\"z\"].shape[-1]\n",
    "#dB   = boltz_loader[0][0].shape[-1]   # hard code, this line gives an erro\n",
    "dB   = 128\n",
    "rB   = 16     \n",
    "rT   = 8       \n",
    "rPH  = 8       \n",
    "# Use values computed from manifest (defined in cell above)\n",
    "L_T_max_boltz  = L_T_max_boltz  # max TCR length (alpha+beta) across manifest\n",
    "L_PH_max_boltz = L_PH_max_boltz  # max pMHC length (pep+hla) across manifest\n",
    "\n",
    "\n",
    "boltz_factoriser = BoltzFactorised(\n",
    "    dB=dB,\n",
    "    rB=rB,\n",
    "    rT=rT,\n",
    "    rPH=rPH,\n",
    "    d=d,\n",
    "    L_max=L_T_max_boltz,\n",
    "    L_PH_max=L_PH_max_boltz,\n",
    ").to(device)\n",
    "\n",
    "# ---- Collect parameters & define optimiser ----\n",
    "params = (\n",
    "    list(tcr_factorised.parameters()) +\n",
    "    list(pmhc_factorised.parameters()) +\n",
    "    list(boltz_factoriser.parameters())\n",
    ")\n",
    "\n",
    "#optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": tcr_factorised.parameters(),  \"lr\": 1e-3},\n",
    "    {\"params\": pmhc_factorised.parameters(), \"lr\": 1e-3},\n",
    "    {\"params\": boltz_factoriser.parameters(), \"lr\": 1e-4},  # or 3e-4\n",
    "])\n",
    "\n",
    "# -----------------------\n",
    "# Anchored-norm state - bound Zstar norms to initial Zstar norms\n",
    "# -----------------------\n",
    "lambda_anchor = 1e-2   # start here; if Z still ramps up try 1e-1, if it kills learning try 1e-3\n",
    "Z0_norm = None         # will be set on first iteration (detached)\n",
    "\n",
    "for step, (batch, boltz_batch) in enumerate(zip(train_loader, boltz_loader)):\n",
    "\n",
    "    print(step)\n",
    "\n",
    "    # ---- SEQUENCE SIDE ----\n",
    "    emb_T  = batch[\"emb_T\"].to(device)\n",
    "    mask_T = batch[\"mask_T\"].to(device)\n",
    "    emb_P  = batch[\"emb_P\"].to(device)\n",
    "    mask_P = batch[\"mask_P\"].to(device)\n",
    "    emb_H  = batch[\"emb_H\"].to(device)\n",
    "    mask_H = batch[\"mask_H\"].to(device)\n",
    "\n",
    "    zT  = tcr_factorised(emb_T, mask_T)\n",
    "    zT  = zT / (zT.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "    zPH = pmhc_factorised(emb_P, mask_P, emb_H, mask_H)\n",
    "    zPH = zPH / (zPH.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "\n",
    "    e_hat = torch.cat([zT, zPH], dim=-1)  # (B, 2d)\n",
    "\n",
    "    # ---- BOLTZ SIDE ----\n",
    "    z_boltz = boltz_batch[\"z\"]\n",
    "    L_p     = boltz_batch[\"pep_len\"]\n",
    "    L_alpha = boltz_batch[\"tcra_len\"]\n",
    "    L_beta  = boltz_batch[\"tcrb_len\"]\n",
    "    L_h     = boltz_batch[\"hla_len\"]\n",
    "\n",
    "    # -----------------------\n",
    "    # 2) Loss forward (returns Zstar)\n",
    "    # -----------------------\n",
    "    loss, components, Zstar = non_contrastive_hamiltonian_loss(\n",
    "        zT=zT,\n",
    "        zPH=zPH,\n",
    "        e_hat=e_hat,\n",
    "        z_boltz_batch=z_boltz,\n",
    "        L_alpha=L_alpha,\n",
    "        L_beta=L_beta,\n",
    "        L_p=L_p,\n",
    "        L_h=L_h,\n",
    "        gP=gP,\n",
    "        gH=gH,\n",
    "        boltz_factoriser=boltz_factoriser,\n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        delta=delta,\n",
    "        gamma_var=1.0,\n",
    "        return_Zstar=True,\n",
    "        use_limit_Zstar=False,   # IMPORTANT: anchored norm is for the REAL case\n",
    "        lambda_Z=0.0,\n",
    "    )\n",
    "\n",
    "    # -----------------------\n",
    "    # 2b) Anchored-norm penalty on Zstar\n",
    "    #     L_anchor = lambda * (||Z|| - ||Z0||)^2\n",
    "    # -----------------------\n",
    "    # Per-sample Frobenius norm (B,)\n",
    "    Znorm = Zstar.flatten(1).norm(dim=1)\n",
    "\n",
    "    # Initialise anchor on first iteration\n",
    "    if Z0_norm is None:\n",
    "        Z0_norm = Znorm.mean().detach()\n",
    "    else:\n",
    "        Z0_norm = 0.99 * Z0_norm + 0.01 * Znorm.mean().detach()\n",
    "\n",
    "    L_anchor = lambda_anchor * (Znorm.mean() - Z0_norm).pow(2)\n",
    "\n",
    "    #L_anchor = lambda_anchor * (Znorm - Z0_norm).pow(2).mean()\n",
    "\n",
    "    # Add to total loss\n",
    "    loss = loss + L_anchor\n",
    "\n",
    "    # Track in components\n",
    "    components[\"L_anchor\"]   = L_anchor.item()\n",
    "    components[\"Znorm_mean\"] = Znorm.mean().item()\n",
    "    components[\"Z0_mean\"]    = Z0_norm.mean().item()\n",
    "    components[\"Znorm_max\"]  = Znorm.max().item()\n",
    "\n",
    "    # -----------------------\n",
    "    # 3) Pre-step prints\n",
    "    # -----------------------\n",
    "    if step == 0:\n",
    "        print(\"=== FIRST BATCH (pre-step) ===\")\n",
    "        print(components)\n",
    "        print(\"zT norm mean:\",  zT.norm(dim=-1).mean().item())\n",
    "        print(\"zPH norm mean:\", zPH.norm(dim=-1).mean().item())\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # 4) Backward pass\n",
    "    # -----------------------------------------\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "\n",
    "    if step == 0:\n",
    "        def grad_norm(m):\n",
    "            tot = 0.0\n",
    "            for p in m.parameters():\n",
    "                if p.grad is not None:\n",
    "                    tot += p.grad.detach().float().norm().item()**2\n",
    "            return tot**0.5\n",
    "\n",
    "        print(\"grad ||tcr||:\",   grad_norm(tcr_factorised))\n",
    "        print(\"grad ||pmhc||:\",  grad_norm(pmhc_factorised))\n",
    "        print(\"grad ||boltz||:\", grad_norm(boltz_factoriser))\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # 5) Debug: decompose quadratic into blocks\n",
    "    # -----------------------------------------\n",
    "    d = zT.shape[-1]\n",
    "    A_block = Zstar[:, :d, :d]\n",
    "    C_block = Zstar[:, :d, d:]\n",
    "    B_block = Zstar[:, d:, d:]\n",
    "\n",
    "    term_A = torch.einsum(\"bi,bij,bj->b\", zT,  A_block, zT)\n",
    "    term_B = torch.einsum(\"bi,bij,bj->b\", zPH, B_block, zPH)\n",
    "    term_C = 2.0 * torch.einsum(\"bi,bij,bj->b\", zT, C_block, zPH)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"term_A mean/max:\", term_A.mean().item(), term_A.abs().max().item())\n",
    "        print(\"term_B mean/max:\", term_B.mean().item(), term_B.abs().max().item())\n",
    "        print(\"term_C mean/max:\", term_C.mean().item(), term_C.abs().max().item())\n",
    "\n",
    "        sym_err = (Zstar - Zstar.transpose(-1, -2)).abs().max().item()\n",
    "        print(\"Zstar symmetry max|Z-ZT|:\", sym_err)\n",
    "\n",
    "        print(\"Zstar abs mean:\", Zstar.abs().mean().item())\n",
    "        print(\"Zstar max abs:\",  Zstar.abs().max().item())\n",
    "        print(\"Znorm mean/max:\", Znorm.mean().item(), Znorm.max().item())\n",
    "        print(\"Z0 mean:\", Z0_norm.mean().item())\n",
    "\n",
    "        # lam = approx_spectral_norm(Zstar, iters=3)\n",
    "        # print(\"lambda_max approx mean/max:\", lam.mean().item(), lam.max().item())\n",
    "\n",
    "    print(components)\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # 6) Gradient clipping\n",
    "    # -----------------------------------------\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(\n",
    "        boltz_factoriser.parameters(),\n",
    "        max_norm=10.0\n",
    "    )\n",
    "\n",
    "    # 7) Optimiser step\n",
    "    # -----------------------------------------\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e662b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "=== FIRST BATCH (pre-step) ===\n",
      "{'L_total': 1.0994528532028198, 'L_inv': -0.8148517608642578, 'L_var_T': 0.9488778114318848, 'L_var_PH': 0.9654240608215332, 'L_cov_T': 1.3895734127800097e-06, 'L_cov_PH': 1.3311723705555778e-06, 'H_expected_mean': -0.814851701259613, 'H_actual_mean': -0.8148517608642578, 'H_max_abs_diff': 1.1920928955078125e-07, 'cos_mean': -0.18514826893806458, 'H_min': -0.8685272932052612, 'H_max': -0.7375632524490356}\n",
      "zT norm mean: 1.0\n",
      "zPH norm mean: 1.0\n",
      "grad ||tcr||: 2.022240165751352\n",
      "grad ||pmhc||: 1.7334930506630921\n",
      "grad ||boltz||: 0.0\n",
      "term_A mean/max: 1.0 1.000000238418579\n",
      "term_B mean/max: 1.0 1.0000001192092896\n",
      "term_C mean/max: -0.37029653787612915 0.5248736143112183\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 1.0994528532028198, 'L_inv': -0.8148517608642578, 'L_var_T': 0.9488778114318848, 'L_var_PH': 0.9654240608215332, 'L_cov_T': 1.3895734127800097e-06, 'L_cov_PH': 1.3311723705555778e-06, 'H_expected_mean': -0.814851701259613, 'H_actual_mean': -0.8148517608642578, 'H_max_abs_diff': 1.1920928955078125e-07, 'cos_mean': -0.18514826893806458, 'H_min': -0.8685272932052612, 'H_max': -0.7375632524490356}\n",
      "1\n",
      "term_A mean/max: 1.0 1.0000001192092896\n",
      "term_B mean/max: 1.0 1.0000001192092896\n",
      "term_C mean/max: 0.44099241495132446 0.6235586404800415\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 0.695022463798523, 'L_inv': -1.2204961776733398, 'L_var_T': 0.949549674987793, 'L_var_PH': 0.965965986251831, 'L_cov_T': 1.985519702429883e-06, 'L_cov_PH': 9.935907883118489e-07, 'H_expected_mean': -1.2204961776733398, 'H_actual_mean': -1.2204961776733398, 'H_max_abs_diff': 1.1920928955078125e-07, 'cos_mean': 0.22049620747566223, 'H_min': -1.311779260635376, 'H_max': -1.1276122331619263}\n",
      "2\n",
      "term_A mean/max: 1.0 1.0000001192092896\n",
      "term_B mean/max: 1.0 1.0000001192092896\n",
      "term_C mean/max: 1.009225606918335 1.2536461353302002\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 0.413161963224411, 'L_inv': -1.5046128034591675, 'L_var_T': 0.9518704414367676, 'L_var_PH': 0.9659006595611572, 'L_cov_T': 1.85925841833523e-06, 'L_cov_PH': 1.8095537370754755e-06, 'H_expected_mean': -1.5046128034591675, 'H_actual_mean': -1.5046128034591675, 'H_max_abs_diff': 1.1920928955078125e-07, 'cos_mean': 0.5046128034591675, 'H_min': -1.6268231868743896, 'H_max': -1.3047724962234497}\n",
      "3\n",
      "term_A mean/max: 1.0 1.0000001192092896\n",
      "term_B mean/max: 1.0 1.0\n",
      "term_C mean/max: 1.2926286458969116 1.4493439197540283\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 0.27945810556411743, 'L_inv': -1.646314263343811, 'L_var_T': 0.9597615599632263, 'L_var_PH': 0.9660083055496216, 'L_cov_T': 7.449630174960475e-07, 'L_cov_PH': 1.8190477248936077e-06, 'H_expected_mean': -1.6463143825531006, 'H_actual_mean': -1.646314263343811, 'H_max_abs_diff': 1.1920928955078125e-07, 'cos_mean': 0.6463143229484558, 'H_min': -1.7246718406677246, 'H_max': -1.557450294494629}\n",
      "4\n",
      "term_A mean/max: 1.0 1.0000001192092896\n",
      "term_B mean/max: 1.0 1.0000001192092896\n",
      "term_C mean/max: 1.3754228353500366 1.6229746341705322\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 0.23606190085411072, 'L_inv': -1.6877113580703735, 'L_var_T': 0.958151638507843, 'L_var_PH': 0.9656203985214233, 'L_cov_T': 4.781861662195297e-07, 'L_cov_PH': 6.898515039210906e-07, 'H_expected_mean': -1.6877113580703735, 'H_actual_mean': -1.6877113580703735, 'H_max_abs_diff': 1.1920928955078125e-07, 'cos_mean': 0.6877114176750183, 'H_min': -1.8114873170852661, 'H_max': -1.5912147760391235}\n",
      "5\n",
      "term_A mean/max: 1.0 1.0000001192092896\n",
      "term_B mean/max: 1.0 1.000000238418579\n",
      "term_C mean/max: 1.5180070400238037 1.716447353363037\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 0.17450019717216492, 'L_inv': -1.7590035200119019, 'L_var_T': 0.9636013507843018, 'L_var_PH': 0.9699010848999023, 'L_cov_T': 4.685532530857017e-07, 'L_cov_PH': 8.194713814191346e-07, 'H_expected_mean': -1.7590035200119019, 'H_actual_mean': -1.7590035200119019, 'H_max_abs_diff': 2.384185791015625e-07, 'cos_mean': 0.7590035200119019, 'H_min': -1.8582236766815186, 'H_max': -1.6360281705856323}\n",
      "6\n",
      "term_A mean/max: 1.0 1.0000001192092896\n",
      "term_B mean/max: 1.0 1.0000001192092896\n",
      "term_C mean/max: 1.5879346132278442 1.6841408014297485\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 0.14879612624645233, 'L_inv': -1.7939672470092773, 'L_var_T': 0.967350423336029, 'L_var_PH': 0.9754124283790588, 'L_cov_T': 2.3451619313163974e-07, 'L_cov_PH': 2.9126061917850166e-07, 'H_expected_mean': -1.7939672470092773, 'H_actual_mean': -1.7939672470092773, 'H_max_abs_diff': 1.1920928955078125e-07, 'cos_mean': 0.7939673662185669, 'H_min': -1.8420703411102295, 'H_max': -1.6630403995513916}\n",
      "7\n",
      "term_A mean/max: 0.9999998807907104 1.0000001192092896\n",
      "term_B mean/max: 1.0 1.0000001192092896\n",
      "term_C mean/max: 1.6500672101974487 1.744586706161499\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 0.11821837723255157, 'L_inv': -1.82503342628479, 'L_var_T': 0.9703555107116699, 'L_var_PH': 0.9728957414627075, 'L_cov_T': 1.3216450156505744e-07, 'L_cov_PH': 4.2095197727576306e-07, 'H_expected_mean': -1.8250336647033691, 'H_actual_mean': -1.82503342628479, 'H_max_abs_diff': 1.1920928955078125e-07, 'cos_mean': 0.8250336050987244, 'H_min': -1.8722933530807495, 'H_max': -1.802437663078308}\n",
      "8\n",
      "term_A mean/max: 1.0 1.0000001192092896\n",
      "term_B mean/max: 1.0 1.000000238418579\n",
      "term_C mean/max: 1.7340093851089478 1.7741613388061523\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 0.08041555434465408, 'L_inv': -1.867004632949829, 'L_var_T': 0.9740890264511108, 'L_var_PH': 0.9733308553695679, 'L_cov_T': 7.456435469066491e-08, 'L_cov_PH': 2.3057424414218985e-07, 'H_expected_mean': -1.867004632949829, 'H_actual_mean': -1.867004632949829, 'H_max_abs_diff': 1.1920928955078125e-07, 'cos_mean': 0.8670046329498291, 'H_min': -1.8870806694030762, 'H_max': -1.8383238315582275}\n",
      "9\n",
      "term_A mean/max: 1.0 1.0000001192092896\n",
      "term_B mean/max: 1.0 1.000000238418579\n",
      "term_C mean/max: 1.7896214723587036 1.823689579963684\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 0.058375462889671326, 'L_inv': -1.894810676574707, 'L_var_T': 0.9758937954902649, 'L_var_PH': 0.9772922992706299, 'L_cov_T': 4.573912093519539e-08, 'L_cov_PH': 5.919613244032007e-08, 'H_expected_mean': -1.894810676574707, 'H_actual_mean': -1.894810676574707, 'H_max_abs_diff': 1.1920928955078125e-07, 'cos_mean': 0.8948107361793518, 'H_min': -1.9118448495864868, 'H_max': -1.8709397315979004}\n",
      "10\n",
      "term_A mean/max: 1.0 1.0000001192092896\n",
      "term_B mean/max: 1.0 1.000000238418579\n",
      "term_C mean/max: 1.8215527534484863 1.8722779750823975\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 0.04458432272076607, 'L_inv': -1.9107763767242432, 'L_var_T': 0.97609943151474, 'L_var_PH': 0.9792611002922058, 'L_cov_T': 5.148707060698143e-08, 'L_cov_PH': 1.1528659626947046e-07, 'H_expected_mean': -1.9107763767242432, 'H_actual_mean': -1.9107763767242432, 'H_max_abs_diff': 2.384185791015625e-07, 'cos_mean': 0.9107763767242432, 'H_min': -1.9361391067504883, 'H_max': -1.872605562210083}\n",
      "11\n",
      "term_A mean/max: 1.0 1.0000001192092896\n",
      "term_B mean/max: 1.0000001192092896 1.000000238418579\n",
      "term_C mean/max: 1.8290305137634277 1.867927074432373\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 0.0413324199616909, 'L_inv': -1.9145152568817139, 'L_var_T': 0.9787105321884155, 'L_var_PH': 0.97713702917099, 'L_cov_T': 3.031027517863549e-08, 'L_cov_PH': 1.439305350459108e-07, 'H_expected_mean': -1.9145152568817139, 'H_actual_mean': -1.9145152568817139, 'H_max_abs_diff': 1.1920928955078125e-07, 'cos_mean': 0.9145152568817139, 'H_min': -1.9339635372161865, 'H_max': -1.8904445171356201}\n",
      "12\n",
      "term_A mean/max: 0.9999999403953552 1.0\n",
      "term_B mean/max: 1.0 1.0000001192092896\n",
      "term_C mean/max: 1.873944640159607 1.9084373712539673\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 0.02440294250845909, 'L_inv': -1.9369723796844482, 'L_var_T': 0.979761004447937, 'L_var_PH': 0.9816142320632935, 'L_cov_T': 5.7534712993856374e-08, 'L_cov_PH': 2.887480476942983e-08, 'H_expected_mean': -1.9369722604751587, 'H_actual_mean': -1.9369723796844482, 'H_max_abs_diff': 2.384185791015625e-07, 'cos_mean': 0.9369723200798035, 'H_min': -1.9542187452316284, 'H_max': -1.8996388912200928}\n"
     ]
    }
   ],
   "source": [
    "##### ------------------ LIMIT CASE ------------------\n",
    "\n",
    "# ---- Hyperparams ----\n",
    "B, L_T_pad, D = emb_T.shape\n",
    "rL = 8\n",
    "rD = 16\n",
    "d  = 128\n",
    "\n",
    "R_PH = 0.7\n",
    "gP_scalar = R_PH ** 0.5\n",
    "gH_scalar = (1.0 - R_PH) ** 0.5\n",
    "gP = torch.tensor(gP_scalar, device=device)   # scalar gates\n",
    "gH = torch.tensor(gH_scalar, device=device)\n",
    "\n",
    "alpha = 1.0   # weight on invariance term\n",
    "beta  = 1.0   # weight on var terms\n",
    "delta = 1.0   # weight on cov terms\n",
    "\n",
    "# ---- Get global max lengths for ESM factorised encoders ----\n",
    "L_T_max = train_dataset.emb_T.shape[1]\n",
    "L_P_max = train_dataset.emb_P.shape[1]\n",
    "L_H_max = train_dataset.emb_H.shape[1]\n",
    "\n",
    "# do this outside of the batch training loop so the weights are not reinitialised for each batch\n",
    "tcr_factorised  = ESMFactorisedEncoder(D, rL, rD, d, L_max=L_T_max).to(device)\n",
    "pmhc_factorised = PMHCFactorisedEncoder(\n",
    "    D, rL, rD, d,\n",
    "    L_P_max=L_P_max,\n",
    "    L_H_max=L_H_max,\n",
    "    R_PH=R_PH\n",
    ").to(device)\n",
    "\n",
    "# ---- Boltz factoriser (choose sensible hyperparams) ----\n",
    "# dB = channel dimension of Boltz embeddings, e.g. boltz_batch[\"z\"].shape[-1]\n",
    "#dB   = boltz_loader[0][0].shape[-1]   # hard code, this line gives an erro\n",
    "dB   = 128\n",
    "rB   = 16     \n",
    "rT   = 8       \n",
    "rPH  = 8       \n",
    "# Use values computed from manifest (defined in cell above)\n",
    "L_T_max_boltz  = L_T_max_boltz  # max TCR length (alpha+beta) across manifest\n",
    "L_PH_max_boltz = L_PH_max_boltz  # max pMHC length (pep+hla) across manifest\n",
    "\n",
    "\n",
    "boltz_factoriser = BoltzFactorised(\n",
    "    dB=dB,\n",
    "    rB=rB,\n",
    "    rT=rT,\n",
    "    rPH=rPH,\n",
    "    d=d,\n",
    "    L_max=L_T_max_boltz,\n",
    "    L_PH_max=L_PH_max_boltz,\n",
    ").to(device)\n",
    "\n",
    "# ---- Collect parameters & define optimiser ----\n",
    "params = (\n",
    "    list(tcr_factorised.parameters()) +\n",
    "    list(pmhc_factorised.parameters()) +\n",
    "    list(boltz_factoriser.parameters())\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "\n",
    "#for batch, boltz_batch in zip(train_loader, boltz_loader):\n",
    "for step, (batch, boltz_batch) in enumerate(zip(train_loader, boltz_loader)):\n",
    "\n",
    "    print(step)\n",
    "\n",
    "    # ---- SEQUENCE SIDE ----\n",
    "    emb_T  = batch[\"emb_T\"].to(device)      # (B, L_T_pad, D)\n",
    "    mask_T = batch[\"mask_T\"].to(device)\n",
    "    emb_P  = batch[\"emb_P\"].to(device)\n",
    "    mask_P = batch[\"mask_P\"].to(device)\n",
    "    emb_H  = batch[\"emb_H\"].to(device)\n",
    "    mask_H = batch[\"mask_H\"].to(device)\n",
    "\n",
    "    # Factorised encoders\n",
    "    zT  = tcr_factorised(emb_T, mask_T)                 # (B, d)\n",
    "    zT  = zT / (zT.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "    zPH = pmhc_factorised(emb_P, mask_P, emb_H, mask_H) # (B, d)\n",
    "    zPH = zPH / (zPH.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "\n",
    "    e_hat = torch.cat([zT, zPH], dim=-1)                # (B, 2d)\n",
    "\n",
    "    # ---- BOLTZ SIDE ----\n",
    "    z_boltz = boltz_batch[\"z\"]       # (B, L_pad, L_pad, dB)  <-- correct key\n",
    "    L_p     = boltz_batch[\"pep_len\"]\n",
    "    L_alpha = boltz_batch[\"tcra_len\"]\n",
    "    L_beta  = boltz_batch[\"tcrb_len\"]\n",
    "    L_h     = boltz_batch[\"hla_len\"]\n",
    "\n",
    "    # -----------------------\n",
    "    # 2) Loss forward (also returns Zstar for debugging)\n",
    "    # -----------------------\n",
    "\n",
    "\n",
    "    # add 'Zstar' as an argument if you want to return it\n",
    "    loss, components, Zstar = non_contrastive_hamiltonian_loss(\n",
    "    zT=zT,\n",
    "    zPH=zPH,\n",
    "    e_hat=e_hat,\n",
    "    z_boltz_batch=z_boltz,\n",
    "    L_alpha=L_alpha,\n",
    "    L_beta=L_beta,\n",
    "    L_p=L_p,\n",
    "    L_h=L_h,\n",
    "    gP=gP,\n",
    "    gH=gH,\n",
    "    boltz_factoriser=boltz_factoriser,\n",
    "    alpha=alpha,\n",
    "    beta=beta,\n",
    "    delta=delta,\n",
    "    gamma_var=1.0,\n",
    "    return_Zstar=True,\n",
    "    use_limit_Zstar=True,\n",
    "    lambda_Z=0.0,\n",
    "    )\n",
    "    \n",
    "    # -----------------------\n",
    "    # 3) Pre-step prints (same forward pass)\n",
    "    # -----------------------\n",
    "    if step == 0:\n",
    "        print(\"=== FIRST BATCH (pre-step) ===\")\n",
    "        print(components)\n",
    "        print(\"zT norm mean:\",  zT.norm(dim=-1).mean().item())\n",
    "        print(\"zPH norm mean:\", zPH.norm(dim=-1).mean().item())\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # 4) Backward pass (compute gradients)\n",
    "    # -----------------------------------------\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "\n",
    "    # Print gradient norms (must be after backward)\n",
    "    if step == 0:\n",
    "        def grad_norm(m):\n",
    "            tot = 0.0\n",
    "            for p in m.parameters():\n",
    "                if p.grad is not None:\n",
    "                    tot += p.grad.detach().float().norm().item()**2\n",
    "            return tot**0.5\n",
    "\n",
    "        print(\"grad ||tcr||:\",   grad_norm(tcr_factorised))\n",
    "        print(\"grad ||pmhc||:\",  grad_norm(pmhc_factorised))\n",
    "        print(\"grad ||boltz||:\", grad_norm(boltz_factoriser))\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # 5) Debug: decompose quadratic into blocks\n",
    "    #    (this uses the SAME Zstar as forward)\n",
    "    # -----------------------------------------\n",
    "    d = zT.shape[-1]\n",
    "    A_block = Zstar[:, :d, :d]\n",
    "    C_block = Zstar[:, :d, d:]\n",
    "    B_block = Zstar[:, d:, d:]\n",
    "\n",
    "    term_A = torch.einsum(\"bi,bij,bj->b\", zT,  A_block, zT)\n",
    "    term_B = torch.einsum(\"bi,bij,bj->b\", zPH, B_block, zPH)\n",
    "    term_C = 2.0 * torch.einsum(\"bi,bij,bj->b\", zT, C_block, zPH)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"term_A mean/max:\", term_A.mean().item(), term_A.abs().max().item())\n",
    "        print(\"term_B mean/max:\", term_B.mean().item(), term_B.abs().max().item())\n",
    "        print(\"term_C mean/max:\", term_C.mean().item(), term_C.abs().max().item())\n",
    "\n",
    "        sym_err = (Zstar - Zstar.transpose(-1, -2)).abs().max().item()\n",
    "        print(\"Zstar symmetry max|Z-ZT|:\", sym_err)\n",
    "\n",
    "        print(\"Zstar abs mean:\", Zstar.abs().mean().item())\n",
    "        print(\"Zstar max abs:\",  Zstar.abs().max().item())\n",
    "\n",
    "    print(components)\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # 6) Optimiser step (update parameters)\n",
    "    # -----------------------------------------\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06272c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #dB   = boltz_loader[0][0].shape[-1]   # or hard-code if you know it\n",
    "\n",
    "# #boltz_loader['z'][0][0]\n",
    "\n",
    "# B, L_T_pad, D = emb_T.shape\n",
    "\n",
    "# print(B, L_T_pad, D)\n",
    "\n",
    "# # Get first batch from DataLoader to inspect shape\n",
    "# # DataLoaders are iterables, not subscriptable - use next(iter(...)) to get a batch\n",
    "# first_batch = next(iter(boltz_loader))\n",
    "# dB = first_batch['z'].shape[-1]   # Get channel dimension from the 'z' tensor\n",
    "# print(f\"dB (Boltz channel dimension): {dB}\")\n",
    "# print(f\"First batch 'z' shape: {first_batch['z'].shape}\")\n",
    "# print(f\"First element of first batch: {first_batch['z'][0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a7011",
   "metadata": {},
   "source": [
    "##### Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232dc16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'masked_input_ids': tensor([[ 0, 32, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        ...,\n",
      "        [ 0,  6, 32,  ...,  1,  1,  1],\n",
      "        [ 0, 32, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 32,  ...,  2,  1,  1]]), 'labels': tensor([[-100,    6, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [-100, -100,   17,  ..., -100, -100, -100],\n",
      "        [-100,    6, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100,   17,  ..., -100, -100, -100]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 0, 0]]), 'clean_input_ids': tensor([[ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        ...,\n",
      "        [ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 17,  ...,  2,  1,  1]]), 'clean_sequences': ['GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALHDYKLSFGAGTTVTVRANEAQVTQNPRYLITVTGKKLTVTCSQNMNHEYMSWYRQDPGLGLRQIYYSMNVEVTDKGDVPEGYKVSRKEKRNFPLILESPSPNQTSLYFCASSLSAGLDEQFFGPGTRLTVLE', 'GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALRPAFGGGTRVLVKPNETGVTQTPRHLVMGMTNKKSLKCEQHLGHNAMYWYKQSAKKPLELMFVYSLEERVENNSVPSRFSPECPNSSHLFLHLHTLQPEDSALYLCASSERNHLEAFFGQGTRLTVVE', 'GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALGSGNQFYFGTGTSLTVIPNEAQVTQNPRYLITVTGKKLTVTCSQNMNHEYMSWYRQDPGLGLRQIYYSMNVEVTDKGDVPEGYKVSRKEKRNFPLILESPSPNQTSLYFCASGQRDYNEQFFGPGTRLTVLE', 'GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALRYGKLTFGQGTILTVHPNEAQVTQNPRYLITVTGKKLTVTCSQNMNHEYMSWYRQDPGLGLRQIYYSMNVEVTDKGDVPEGYKVSRKEKRNFPLILESPSPNQTSLYFCASSLSGSPLHFGNGTRLTVTE', 'GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALDSGNTGKLIFGQGTTLQVKPDEAGVAQSPRYKIIEKRQSVAFWCNPISGHATLYWYQQILGQGPKLLIQFQNNGVVDDSQLPKDRFSAERLKGVDSTLKIQPAKLEDSAVYLCASTTGGGGYEQYFGPGTRLTVTE', 'GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALIYNTDKLIFGTGTRLQVFPNDSGVTQTPKHLITATGQRVTLRCSPRSGDLSVYWYQQSLDQGLQFLIQYYNGEERAKGNILERFSAQQFPDLHSELNLSSLELGDSALYFCASSVNPAQGSGANVLTFGAGSRLTVLE', 'GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALGDDKIIFGKGTRLHILPNGAGVSQSPSNKVTEKGKDVELRCDPISGHTALYWYRQSLGQGLEFLIYFQGNSAPDKSGLPSDRFSAERTGGSVSTLTIQRTQQEDSAVYLCASSLASEGFTEAFFGQGTRLTVVE', 'GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALSDPRGGSEKLVFGKGTKLTVNPYDTEVTQTPKHLVMGMTNKKSLKCEQHMGHRAMYWYKQKAKKPPELMFVYSYEKLSINESVPSRFSPECPNSSLLNLHLHALQPEDSALYLCASSQDVGQGVLYGYTFGSGTRLTVVE'], 'masked_sequences': ['<mask>NSVTQMEGPVTLSEEAFLT<mask>NCT<mask>TATGYP<mask>LF<mask>YVQDPGEG<mask>QLLLKA<mask>KADDKGSNKGFLATYRKETTSFHLEKGSV<mask>TSDS<mask>V<mask>FCALHDYK<mask>SFGAGTTV<mask>VRA<mask>EAQVTQNPRYLITVTGKKLTV<mask>CSQNMNHEYMSWY<mask>QDPGAGLRQ<mask>YY<mask>MN<mask>EV<mask>RKH<mask>VPEGYK<mask>SRK<mask>KRNFPLI<mask><mask>SPSP<mask>QTSLY<mask>CASSLSAGLDEQFFGP<mask>TRLTVLE', 'GNSV<mask>QMEGPVTLS<mask>EAFLTINCTYTATGYPNLFWYVQYPGEG<mask>QLLLKATKADDK<mask>SNKG<mask>EATYRKETTSFHLEKGS<mask>Q<mask><mask>DSAVYFCA<mask>RPAFGGGTRVLVKP<mask>ETAVTQTPRHLVP<mask>ETNKK<mask>LKCE<mask>H<mask>GHNAMYWDKQ<mask><mask>KKPLELM<mask>VAS<mask>E<mask>RV<mask>NNSVPSR<mask>SPECPNSS<mask>LF<mask>HLHTLQPEDSALYLC<mask>SSERNHLEAF<mask>GQGTRL<mask>V<mask>E', 'GNSVTQME<mask><mask>VTLD<mask>EAFLTINCTYTATGYPSLFWY<mask>QYPGEGLQLLLKATKADDKGSNKGFEA<mask>YRKETTSFHL<mask>KGSVQVSDSAV<mask>F<mask>ALGSGN<mask>FYSGTGTSLTVIP<mask>EAQVTQ<mask>PRY<mask>IT<mask>TE<mask><mask>LTVTCSQNMNHEYMSWYRQDPG<mask>GLP<mask>IYYSMNVEH<mask>DKGDVPEGYKV<mask>RKE<mask>RNF<mask>LILP<mask>PSPNQT<mask>LYFCASGQRD<mask>NEQFF<mask>PGTRLT<mask>L<mask>', 'GNSVT<mask>MEGPVTLSEEAFLTINCTYTATG<mask>PSL<mask>WYVQYPGEGLQLLLKA<mask><mask>LDDKGSNKGF<mask>ATYKKET<mask>SFHLEKGS<mask>QVSDSAV<mask>FFALRYGT<mask>TFGQGTILTVHPNEAQVTQ<mask><mask>RYNIT<mask>TGKKLT<mask>TC<mask><mask>NM<mask>HEYMS<mask>YRQ<mask>PG<mask>GLRQIYYSDNVEVTDKGDVPEGYK<mask>SRKEKRNFPLILESPSP<mask>QT<mask>LYFCAS<mask><mask>SGSPLHFGNGT<mask>L<mask>VTE', 'GN<mask>VTQMEGPVTL<mask><mask>E<mask>FLTINCTYTATGYPSLFWYVQY<mask>G<mask><mask><mask>QLLLKATKWDDTGSNK<mask>F<mask>A<mask>YRKETTS<mask><mask>LEKGSV<mask>SSDSAVYFCALD<mask>GNTGK<mask>IFGQGTSLQVK<mask>D<mask><mask>GVAQ<mask>PP<mask>K<mask>IEK<mask>QSVAFWCNPISGHATLYWYQQILGQ<mask>PKLLIQ<mask>QNNGVVD<mask>SQLPKDRFSAERLKGVDST<mask>KIQP<mask>KLEDSAVYLCASTTGGGGYEQYFGPGTRLTFTE', 'G<mask>SVTQMEGPVTLSEEAFLEI<mask>CTYTATGY<mask>ELFWYV<mask><mask>PGEG<mask>QLLLKATKADDKGSNK<mask>FEAQYRKETTS<mask>HLEKGSVQVS<mask>SAVYFCALIY<mask>TDKLP<mask>GT<mask><mask>RLQRFP<mask>DSGVTQTPKHLITAT<mask>Q<mask>VTLRCSPRSGDLSVYWYQQSLDQGLQFLI<mask><mask><mask>NGEER<mask>K<mask>NILE<mask>FSAQQFP<mask>LHSELNLSSLEL<mask>DS<mask><mask>VF<mask>ASSVNPAQGSGANVLTFGAG<mask>RLTVLE', '<mask>NSV<mask>QMEGPVTLSELAFLTINVTYTATGYP<mask>LFWYVQ<mask><mask>GEGL<mask><mask>LLKATKADDKGSN<mask>G<mask>EATYRK<mask>T<mask>SFH<mask>EKGY<mask><mask>V<mask>DSAVYFC<mask>LGDDKIIFGKGTR<mask>HILPNGAGVSQSPSN<mask>VTEKGKDV<mask>LRCDPISGHTALYW<mask>RQSLQQ<mask>LEFLIYFQGNSAPDK<mask>GLPSDH<mask>SAERTGGSVSTLT<mask>QRT<mask>QEDSAVYV<mask>ASSLASEGFTEAFFGQGT<mask>LTVVE', 'G<mask>SVTQMEGPVTLSEEA<mask>LT<mask>N<mask>TYTA<mask>EYPSLFWIVQYPGE<mask>LQLLLKATKADDKGSNK<mask>F<mask>A<mask>Y<mask><mask>ETTSFHLEKGKVQVS<mask>SAVYF<mask>ALSDPRGGSEKLV<mask>GKGTKLTV<mask>PYDTEVTQTPKHLVMG<mask>TNKKSLKK<mask>QHMG<mask>RAM<mask>WT<mask>QKAKKPP<mask>LMFVYSYCKLSINESVPS<mask>FSPE<mask>PNSSLLNLH<mask>HALQPED<mask>ALYLCASSQ<mask>VGQGVLYGY<mask>FG<mask>GTRLTVVE'], 'clean_sequences_ESMprotein': [ESMProtein(sequence='GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALHDYKLSFGAGTTVTVRANEAQVTQNPRYLITVTGKKLTVTCSQNMNHEYMSWYRQDPGLGLRQIYYSMNVEVTDKGDVPEGYKVSRKEKRNFPLILESPSPNQTSLYFCASSLSAGLDEQFFGPGTRLTVLE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALRPAFGGGTRVLVKPNETGVTQTPRHLVMGMTNKKSLKCEQHLGHNAMYWYKQSAKKPLELMFVYSLEERVENNSVPSRFSPECPNSSHLFLHLHTLQPEDSALYLCASSERNHLEAFFGQGTRLTVVE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALGSGNQFYFGTGTSLTVIPNEAQVTQNPRYLITVTGKKLTVTCSQNMNHEYMSWYRQDPGLGLRQIYYSMNVEVTDKGDVPEGYKVSRKEKRNFPLILESPSPNQTSLYFCASGQRDYNEQFFGPGTRLTVLE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALRYGKLTFGQGTILTVHPNEAQVTQNPRYLITVTGKKLTVTCSQNMNHEYMSWYRQDPGLGLRQIYYSMNVEVTDKGDVPEGYKVSRKEKRNFPLILESPSPNQTSLYFCASSLSGSPLHFGNGTRLTVTE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALDSGNTGKLIFGQGTTLQVKPDEAGVAQSPRYKIIEKRQSVAFWCNPISGHATLYWYQQILGQGPKLLIQFQNNGVVDDSQLPKDRFSAERLKGVDSTLKIQPAKLEDSAVYLCASTTGGGGYEQYFGPGTRLTVTE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALIYNTDKLIFGTGTRLQVFPNDSGVTQTPKHLITATGQRVTLRCSPRSGDLSVYWYQQSLDQGLQFLIQYYNGEERAKGNILERFSAQQFPDLHSELNLSSLELGDSALYFCASSVNPAQGSGANVLTFGAGSRLTVLE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALGDDKIIFGKGTRLHILPNGAGVSQSPSNKVTEKGKDVELRCDPISGHTALYWYRQSLGQGLEFLIYFQGNSAPDKSGLPSDRFSAERTGGSVSTLTIQRTQQEDSAVYLCASSLASEGFTEAFFGQGTRLTVVE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALSDPRGGSEKLVFGKGTKLTVNPYDTEVTQTPKHLVMGMTNKKSLKCEQHMGHRAMYWYKQKAKKPPELMFVYSYEKLSINESVPSRFSPECPNSSLLNLHLHALQPEDSALYLCASSQDVGQGVLYGYTFGSGTRLTVVE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False)], 'masked_sequences_ESMprotein': [ESMProtein(sequence='<mask>NSVTQMEGPVTLSEEAFLT<mask>NCT<mask>TATGYP<mask>LF<mask>YVQDPGEG<mask>QLLLKA<mask>KADDKGSNKGFLATYRKETTSFHLEKGSV<mask>TSDS<mask>V<mask>FCALHDYK<mask>SFGAGTTV<mask>VRA<mask>EAQVTQNPRYLITVTGKKLTV<mask>CSQNMNHEYMSWY<mask>QDPGAGLRQ<mask>YY<mask>MN<mask>EV<mask>RKH<mask>VPEGYK<mask>SRK<mask>KRNFPLI<mask><mask>SPSP<mask>QTSLY<mask>CASSLSAGLDEQFFGP<mask>TRLTVLE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GNSV<mask>QMEGPVTLS<mask>EAFLTINCTYTATGYPNLFWYVQYPGEG<mask>QLLLKATKADDK<mask>SNKG<mask>EATYRKETTSFHLEKGS<mask>Q<mask><mask>DSAVYFCA<mask>RPAFGGGTRVLVKP<mask>ETAVTQTPRHLVP<mask>ETNKK<mask>LKCE<mask>H<mask>GHNAMYWDKQ<mask><mask>KKPLELM<mask>VAS<mask>E<mask>RV<mask>NNSVPSR<mask>SPECPNSS<mask>LF<mask>HLHTLQPEDSALYLC<mask>SSERNHLEAF<mask>GQGTRL<mask>V<mask>E', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GNSVTQME<mask><mask>VTLD<mask>EAFLTINCTYTATGYPSLFWY<mask>QYPGEGLQLLLKATKADDKGSNKGFEA<mask>YRKETTSFHL<mask>KGSVQVSDSAV<mask>F<mask>ALGSGN<mask>FYSGTGTSLTVIP<mask>EAQVTQ<mask>PRY<mask>IT<mask>TE<mask><mask>LTVTCSQNMNHEYMSWYRQDPG<mask>GLP<mask>IYYSMNVEH<mask>DKGDVPEGYKV<mask>RKE<mask>RNF<mask>LILP<mask>PSPNQT<mask>LYFCASGQRD<mask>NEQFF<mask>PGTRLT<mask>L<mask>', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GNSVT<mask>MEGPVTLSEEAFLTINCTYTATG<mask>PSL<mask>WYVQYPGEGLQLLLKA<mask><mask>LDDKGSNKGF<mask>ATYKKET<mask>SFHLEKGS<mask>QVSDSAV<mask>FFALRYGT<mask>TFGQGTILTVHPNEAQVTQ<mask><mask>RYNIT<mask>TGKKLT<mask>TC<mask><mask>NM<mask>HEYMS<mask>YRQ<mask>PG<mask>GLRQIYYSDNVEVTDKGDVPEGYK<mask>SRKEKRNFPLILESPSP<mask>QT<mask>LYFCAS<mask><mask>SGSPLHFGNGT<mask>L<mask>VTE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GN<mask>VTQMEGPVTL<mask><mask>E<mask>FLTINCTYTATGYPSLFWYVQY<mask>G<mask><mask><mask>QLLLKATKWDDTGSNK<mask>F<mask>A<mask>YRKETTS<mask><mask>LEKGSV<mask>SSDSAVYFCALD<mask>GNTGK<mask>IFGQGTSLQVK<mask>D<mask><mask>GVAQ<mask>PP<mask>K<mask>IEK<mask>QSVAFWCNPISGHATLYWYQQILGQ<mask>PKLLIQ<mask>QNNGVVD<mask>SQLPKDRFSAERLKGVDST<mask>KIQP<mask>KLEDSAVYLCASTTGGGGYEQYFGPGTRLTFTE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='G<mask>SVTQMEGPVTLSEEAFLEI<mask>CTYTATGY<mask>ELFWYV<mask><mask>PGEG<mask>QLLLKATKADDKGSNK<mask>FEAQYRKETTS<mask>HLEKGSVQVS<mask>SAVYFCALIY<mask>TDKLP<mask>GT<mask><mask>RLQRFP<mask>DSGVTQTPKHLITAT<mask>Q<mask>VTLRCSPRSGDLSVYWYQQSLDQGLQFLI<mask><mask><mask>NGEER<mask>K<mask>NILE<mask>FSAQQFP<mask>LHSELNLSSLEL<mask>DS<mask><mask>VF<mask>ASSVNPAQGSGANVLTFGAG<mask>RLTVLE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='<mask>NSV<mask>QMEGPVTLSELAFLTINVTYTATGYP<mask>LFWYVQ<mask><mask>GEGL<mask><mask>LLKATKADDKGSN<mask>G<mask>EATYRK<mask>T<mask>SFH<mask>EKGY<mask><mask>V<mask>DSAVYFC<mask>LGDDKIIFGKGTR<mask>HILPNGAGVSQSPSN<mask>VTEKGKDV<mask>LRCDPISGHTALYW<mask>RQSLQQ<mask>LEFLIYFQGNSAPDK<mask>GLPSDH<mask>SAERTGGSVSTLT<mask>QRT<mask>QEDSAVYV<mask>ASSLASEGFTEAFFGQGT<mask>LTVVE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='G<mask>SVTQMEGPVTLSEEA<mask>LT<mask>N<mask>TYTA<mask>EYPSLFWIVQYPGE<mask>LQLLLKATKADDKGSNK<mask>F<mask>A<mask>Y<mask><mask>ETTSFHLEKGKVQVS<mask>SAVYF<mask>ALSDPRGGSEKLV<mask>GKGTKLTV<mask>PYDTEVTQTPKHLVMG<mask>TNKKSLKK<mask>QHMG<mask>RAM<mask>WT<mask>QKAKKPP<mask>LMFVYSYCKLSINESVPS<mask>FSPE<mask>PNSSLLNLH<mask>HALQPED<mask>ALYLCASSQ<mask>VGQGVLYGY<mask>FG<mask>GTRLTVVE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False)], 'masked_input_ids_ESMprotein_batched': _BatchedESMProteinTensor(sequence=tensor([[ 0, 32, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        ...,\n",
      "        [ 0,  6, 32,  ...,  1,  1,  1],\n",
      "        [ 0, 32, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 32,  ...,  2,  1,  1]]), structure=None, secondary_structure=None, sasa=None, function=None, residue_annotations=None, coordinates=None, potential_sequence_of_concern=False), 'clean_input_ids_ESMprotein_batched': _BatchedESMProteinTensor(sequence=tensor([[ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        ...,\n",
      "        [ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 17,  ...,  2,  1,  1]]), structure=None, secondary_structure=None, sasa=None, function=None, residue_annotations=None, coordinates=None, potential_sequence_of_concern=False)}\n",
      "{'masked_input_ids': tensor([[ 0, 11, 32, 13, 15,  8, 18,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 11, 32, 13, 14,  8, 23,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 10, 32, 10,  5, 19,  6, 19,  8, 15,  2,  1,  1,  1],\n",
      "        [ 0,  6, 11,  8,  6, 32, 14, 12,  7,  7, 10,  2,  1,  1],\n",
      "        [ 0,  8, 14, 10, 22, 19, 18, 32, 19,  8,  2,  1,  1,  1],\n",
      "        [ 0, 13, 11, 13, 14,  8, 18, 32,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 11, 11, 17, 14,  8, 32,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 11,  8, 11,  4, 16,  9, 14, 32,  6, 22,  2,  1,  1]]), 'labels': tensor([[-100, -100,   11, -100,   14, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100, -100,   11, -100, -100, -100,   18, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100, -100,    7, -100, -100, -100,   11, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100, -100, -100, -100, -100,    8, -100, -100, -100,   17, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100,   19, -100,    4, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100,   11, -100, -100, -100, -100, -100,    4, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100, -100, -100,   13, -100, -100,   18, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100,   16,   12, -100, -100, -100,\n",
      "         -100, -100]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]), 'clean_input_ids': tensor([[ 0, 11, 11, 13, 14,  8, 18,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 11, 11, 13, 14,  8, 18,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 10,  7, 10,  5, 19, 11, 19,  8, 15,  2,  1,  1,  1],\n",
      "        [ 0,  6, 11,  8,  6,  8, 14, 12,  7, 17, 10,  2,  1,  1],\n",
      "        [ 0,  8, 14, 10, 22, 19, 18, 19, 19,  4,  2,  1,  1,  1],\n",
      "        [ 0, 11, 11, 13, 14,  8, 18,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 11, 11, 13, 14,  8, 18,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 11,  8, 11,  4, 16,  9, 16, 12,  6, 22,  2,  1,  1]]), 'clean_sequences': ['TTDPSFLGRY', 'TTDPSFLGRY', 'RVRAYTYSK', 'GTSGSPIVNR', 'SPRWYFYYL', 'TTDPSFLGRY', 'TTDPSFLGRY', 'TSTLQEQIGW'], 'masked_sequences': ['T<mask>DKSFLGRY', 'T<mask>DPSCLGRY', 'R<mask>RAYGYSK', 'GTSG<mask>PIVVR', 'SPRWYF<mask>YS', 'DTDPSF<mask>GRY', 'TTNPS<mask>LGRY', 'TSTLQEP<mask>GW'], 'clean_sequences_ESMprotein': [ESMProtein(sequence='TTDPSFLGRY', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='TTDPSFLGRY', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='RVRAYTYSK', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GTSGSPIVNR', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='SPRWYFYYL', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='TTDPSFLGRY', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='TTDPSFLGRY', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='TSTLQEQIGW', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False)], 'masked_sequences_ESMprotein': [ESMProtein(sequence='T<mask>DKSFLGRY', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='T<mask>DPSCLGRY', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='R<mask>RAYGYSK', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GTSG<mask>PIVVR', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='SPRWYF<mask>YS', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='DTDPSF<mask>GRY', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='TTNPS<mask>LGRY', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='TSTLQEP<mask>GW', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False)], 'masked_input_ids_ESMprotein_batched': _BatchedESMProteinTensor(sequence=tensor([[ 0, 11, 32, 13, 15,  8, 18,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 11, 32, 13, 14,  8, 23,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 10, 32, 10,  5, 19,  6, 19,  8, 15,  2,  1,  1,  1],\n",
      "        [ 0,  6, 11,  8,  6, 32, 14, 12,  7,  7, 10,  2,  1,  1],\n",
      "        [ 0,  8, 14, 10, 22, 19, 18, 32, 19,  8,  2,  1,  1,  1],\n",
      "        [ 0, 13, 11, 13, 14,  8, 18, 32,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 11, 11, 17, 14,  8, 32,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 11,  8, 11,  4, 16,  9, 14, 32,  6, 22,  2,  1,  1]]), structure=None, secondary_structure=None, sasa=None, function=None, residue_annotations=None, coordinates=None, potential_sequence_of_concern=False), 'clean_input_ids_ESMprotein_batched': _BatchedESMProteinTensor(sequence=tensor([[ 0, 11, 11, 13, 14,  8, 18,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 11, 11, 13, 14,  8, 18,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 10,  7, 10,  5, 19, 11, 19,  8, 15,  2,  1,  1,  1],\n",
      "        [ 0,  6, 11,  8,  6,  8, 14, 12,  7, 17, 10,  2,  1,  1],\n",
      "        [ 0,  8, 14, 10, 22, 19, 18, 19, 19,  4,  2,  1,  1,  1],\n",
      "        [ 0, 11, 11, 13, 14,  8, 18,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 11, 11, 13, 14,  8, 18,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 11,  8, 11,  4, 16,  9, 16, 12,  6, 22,  2,  1,  1]]), structure=None, secondary_structure=None, sasa=None, function=None, residue_annotations=None, coordinates=None, potential_sequence_of_concern=False)}\n",
      "{'masked_input_ids': tensor([[ 0, 32,  5,  ..., 15,  7,  2],\n",
      "        [ 0, 20,  5,  ..., 32,  7,  2],\n",
      "        [ 0, 20,  5,  ..., 15,  7,  2],\n",
      "        ...,\n",
      "        [ 0, 20, 11,  ...,  1,  1,  1],\n",
      "        [ 0, 20,  9,  ..., 32,  7,  2],\n",
      "        [ 0, 20,  5,  ..., 15, 32,  2]]), 'labels': tensor([[-100,   20, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ...,   15, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [-100, -100,    4,  ..., -100, -100, -100],\n",
      "        [-100, -100,    5,  ...,   15, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100,    7, -100]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'clean_input_ids': tensor([[ 0, 20,  5,  ..., 15,  7,  2],\n",
      "        [ 0, 20,  5,  ..., 15,  7,  2],\n",
      "        [ 0, 20,  5,  ..., 15,  7,  2],\n",
      "        ...,\n",
      "        [ 0, 20,  4,  ...,  1,  1,  1],\n",
      "        [ 0, 20,  5,  ..., 15,  7,  2],\n",
      "        [ 0, 20,  5,  ..., 15,  7,  2]]), 'clean_sequences': ['MAVMAPRTLLLLLSGALALTQTWAGSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQKMEPRAPWIEQEGPEYWDQETRNMKAHSQTDRANLGTLRGYYNQSEDGSHTIQIMYGCDVGPDGRFLRGYRQDAYDGKDYIALNEDLRSWTAADMAAQITKRKWEAVHAAEQRRVYLEGRCVDGLRRYLENGKETLQRTDPPKTHMTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWELSSQPTIPIVGIIAGLVLLGAVITGAVVAAVMWRRKSSDRKGGSYTQAASSDSAQGSDVSLTACKV', 'MAVMAPRTLVLLLSGALALTQTWAGSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETLQRTDAPKTHMTHHAVSDHEATLRCWALSFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGQEQRYTCHVQHEGLPKPLTLRWEPSSQPTIPIVGIIAGLVLFGAVITGAVVAAVMWRRKSSDRKGGSYSQAASSDSAQGSDVSLTACKV', 'MAVMAPRTLLLLLSGALALTQTWAGSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQKMEPRAPWIEQEGPEYWDQETRNMKAHSQTDRANLGTLRGYYNQSEDGSHTIQIMYGCDVGPDGRFLRGYRQDAYDGKDYIALNEDLRSWTAADMAAQITKRKWEAVHAAEQRRVYLEGRCVDGLRRYLENGKETLQRTDPPKTHMTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWELSSQPTIPIVGIIAGLVLLGAVITGAVVAAVMWRRKSSDRKGGSYTQAASSDSAQGSDVSLTACKV', 'MAVMAPRTLVLLLSGALALTQTWAGSHSMRYFSTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDEETGKVKAHSQTDRENLRIALRYYNQSEAGSHTLQMMFGCDVGSDGRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQITKRKWEAAHVAEQQRAYLEGTCVDGLRRYLENGKETLQRTDPPKTHMTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWEPSSQPTVPIVGIIAGLVLLGAVITGAVVAAVMWRRNSSDRKGGSYSQAASSDSAQGSDVSLTACKV', 'MAVMAPRTLVLLLSGALALTQTWAGSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETLQRTDAPKTHMTHHAVSDHEATLRCWALSFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGQEQRYTCHVQHEGLPKPLTLRWEPSSQPTIPIVGIIAGLVLFGAVITGAVVAAVMWRRKSSDRKGGSYSQAASSDSAQGSDVSLTACKV', 'MLVMAPRTVLLLLSAALALTETWAGSHSMRYFDTAMSRPGRGEPRFISVGYVDDTQFVRFDSDAASPREEPRAPWIEQEGPEYWDRNTQIFKTNTQTDRESLRNLRGYYNQSEAGSHTLQSMYGCDVGPDGRLLRGHNQYAYDGKDYIALNEDLRSWTAADTAAQITQRKWEAARVAEQDRAYLEGTCVEWLRRYLENGKDTLERADPPKTHVTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDRTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWEPSSQSTVPIVGIVAGLAVLAVVVIGAVVAAVMCRRKSSGGKGGSYSQAACSDSAQGSDVSLTA', 'MAVMAPRTLLLLLSGALALTQTWAGSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQKMEPRAPWIEQEGPEYWDQETRNMKAHSQTDRANLGTLRGYYNQSEDGSHTIQIMYGCDVGPDGRFLRGYRQDAYDGKDYIALNEDLRSWTAADMAAQITKRKWEAVHAAEQRRVYLEGRCVDGLRRYLENGKETLQRTDPPKTHMTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWELSSQPTIPIVGIIAGLVLLGAVITGAVVAAVMWRRKSSDRKGGSYTQAASSDSAQGSDVSLTACKV', 'MAVMAPRTLLLLLSGALALTQTWAGSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQKMEPRAPWIEQEGPEYWDQETRNMKAHSQTDRANLGTLRGYYNQSEDGSHTIQIMYGCDVGPDGRFLRGYRQDAYDGKDYIALNEDLRSWTAADMAAQITKRKWEAVHAAEQRRVYLEGRCVDGLRRYLENGKETLQRTDPPKTHMTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWELSSQPTIPIVGIIAGLVLLGAVITGAVVAAVMWRRKSSDRKGGSYTQAASSDSAQGSDVSLTACKV'], 'masked_sequences': ['<mask>AVMAPRTLLLLLS<mask>ALALTQTWAGSHSMRY<mask>FTSVSRPGRGERRFIVVGYVDD<mask>QFVMFDSDAASQKM<mask>PRAPW<mask>EQEG<mask>E<mask>WDQE<mask>RNMK<mask>HSPT<mask>R<mask>NLGTLRGYYNQSEDGSHTIQIMYGCDVGPDGRFLRTYRQ<mask>AYDGKDYIALNEDLRSW<mask>AADMAAQITK<mask>KWEAVHAHEQRRVYLEGR<mask>VDG<mask>RRYLENGKETL<mask>RTDPPKT<mask>MTH<mask>PISD<mask>EATLRCWALG<mask>YPAEITLTWQRDGED<mask>T<mask><mask>TEL<mask>ETRPAG<mask>GTFNKWKAVVVPSGEEQRYTCHVQHEGLPKPLTLRWELSSQPTIP<mask>VGIIAG<mask><mask>LLGAVIT<mask>AV<mask>AAVMWRIKSSDRKGG<mask><mask><mask>QA<mask>SSDSAQGSDVSLTACKV', 'MAVMAPRTLVLLLSGALALTQTWAGSH<mask>MRYFFTS<mask>SRP<mask>RGEPRFI<mask>VCYVDDTQFVRFDSDAA<mask>QRMEPR<mask>PWIE<mask>EGPEYWDGETRKVKAH<mask>Q<mask>HRVDLGTLRGYYNQS<mask><mask>GSHT<mask>QRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLFS<mask>TAACMA<mask>Q<mask><mask><mask>HKWEA<mask>HVA<mask>QLRAYLEG<mask>CVEWLRRYLENGKETLQR<mask>DAPP<mask>HMTSHAVSD<mask><mask>ATLRCWALSFYP<mask><mask>I<mask>LTWQRSGEDQTQDTELVETRPAGD<mask>TFQKWAAV<mask><mask>PSGKEQRYTCHVQH<mask>GL<mask>KPLTLRWEPSSQP<mask>IPISGIIAGLVLFGAVITG<mask>VVAAVMWRRKSSDCKG<mask>SYSQAASSDSAQGSDVSLTAC<mask>V', 'MAVMAPWTL<mask>LLL<mask>GALALT<mask>TWACS<mask>SMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRF<mask>SDAAS<mask>KMEARA<mask>WI<mask>QEGPE<mask>WDQETRNMKAHSQTDRAELGTLRGY<mask>NQSEDGSHT<mask>QIMYGCDVGPDGRFLR<mask>YRQDAYDGKDYIAL<mask>EDGRSWTRA<mask>MAAQITKRKW<mask><mask>VH<mask>AEQRRVYLE<mask>RCVDGLRRYLENGKETLQRTD<mask>PKTH<mask>T<mask>HPISDHEATLRCWALG<mask>YPAEITLTNQRDG<mask>DQTQDT<mask>LVETRPAFDGTFQKWAA<mask>VVPSGEEQRYTCHVQHEHLPKPLTLRWELSSQPTIPIVG<mask><mask>AGLVL<mask>GAV<mask>TGA<mask>VAAVMWRRKS<mask>DR<mask>GGSYT<mask>A<mask>SSDSAQG<mask><mask>VSLTACKV', 'M<mask>VMAPRTLVLLLSGACALTQTWAGS<mask>SMRYDSTSVS<mask>PGRGEP<mask><mask>IAVG<mask><mask>DDTQ<mask>VHFDSDAASQ<mask>MERRAPWI<mask>QEGPEYWDE<mask>TGKVKAH<mask><mask>TDRENLRIALRYYNQ<mask>EAGSHTLQMMFGCDVGSDG<mask><mask>LCGYHQYAYDGKD<mask>IALKEDLR<mask>WTAADMAAQI<mask>KRKWEAAHVAEQQRAYLEGTCYDNLRRYLENGKETLQRTDPPKTHMTHHPISDHEATLRCWALGFYPAEI<mask>LTW<mask>R<mask>GEDQTQ<mask>TELVETRPAGDGTFQKWAAVVVPSGEE<mask>MYTCHVQHEGLP<mask>PLTLRWEPSS<mask>P<mask><mask>PIVGI<mask>AGLVLLGA<mask>ITGAVVAAVMWRRNSSDR<mask>G<mask><mask>YSQ<mask>ASSDSA<mask>GSD<mask>SLTACKV', 'MAVMAPRTLVLLL<mask>GALALTQTWA<mask>SHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRF<mask>SDA<mask>SQRMEPRAP<mask>IEQE<mask>PEYWDGETRKVKAHSQTHR<mask>DLGTLRGYTNQSEA<mask>SH<mask>VQRM<mask>GCD<mask>GSDWRFLR<mask>YHQ<mask>AYD<mask>KDYIALK<mask>DLR<mask><mask><mask>AADM<mask>AQTTKHKWEAAHVAEQ<mask>RAYLEMFC<mask>EYLRRYL<mask>N<mask>KETLHRTDAPKTHMTHHAVSD<mask>EATL<mask>CWALSFYPAEITLTW<mask>RDGEDQTQDTELVE<mask>R<mask>AGDGTFQKWA<mask>VVVPSGQEQRYTCHVQHKGLPKPLTTRWEPSSQPTIPIVGIIAG<mask>VLFLA<mask>I<mask>GAVVEAVMWRRKSS<mask>RKGG<mask>YSQA<mask>SS<mask>SAQGSDVSLTACKV', 'MTVMAP<mask><mask>V<mask>LLLSAAL<mask>LTET<mask>AGSHSMRYFDT<mask>MSRP<mask>RG<mask>PRFISVGYVDYT<mask>FVRFD<mask><mask>AA<mask>PREEPRAPWIEQE<mask>PEYWDRNTQIFKTNTQTDR<mask>SLRDLRGYYNQSEAGSHTLQ<mask>MYGCDVGPDGRLL<mask>GHNQYAYDGKDYIALNEDLR<mask>WTAA<mask>TAAQITQRKWEAARVA<mask>QDRA<mask>LEGT<mask>VEWMRRYLENG<mask>DT<mask>ERSDPPKTHV<mask>HHP<mask>WDHEVTLRCWALGFYPAEIT<mask>TWQRDGE<mask>QTQDTELVETRPAGDRTFQKWAAVVV<mask><mask>GEEQRY<mask><mask>HVQHEGLPKPLTLRWEPSSQSTVPIVGIVACLAVLAVVVIGA<mask>VAYV<mask>CRR<mask>SSG<mask>KGGS<mask>SQAACSDSAQGSDVSLTA', 'MEV<mask>APRTLLLLLSGA<mask>A<mask>TQTWAGSHSMRYFFT<mask>MSRMG<mask>GEPRFI<mask>VGYVDDTQF<mask>RFDSDAASQKMEPRAP<mask>PEQE<mask>PEYWDQETR<mask>MKAHSQTDRANL<mask>TLRGYYNQSEDGSHIIQIMYGCDVHPDGRFLRGYRTDAYD<mask>KD<mask>IALNEDWRSWTAADMAAQITLRKWEAVHAAEQ<mask>RVYLEGR<mask>VD<mask>LR<mask>YL<mask>NGKE<mask>LQRTDP<mask>KTHMTHHPISD<mask>EATLRCWAL<mask>F<mask>P<mask>EITLTWQRDGEDQT<mask>DTELVETRPAGDGTFQKWAAVVVPSGEEQR<mask><mask>C<mask><mask>Q<mask>EGLPKPLTLRWELSSQPTIPIVG<mask>IAGLVL<mask>GAVITGAVVAAVM<mask>RRKSSDRKGGSYTQAAS<mask>DSAQGS<mask>VSLTAC<mask>V', 'MAVM<mask>PRTLLLLLSGALALTQTWAGSH<mask>MRY<mask>FTSVSRPGRGE<mask>RFIAVGY<mask>DDTQFVRFDSDAASQKMEPRAMWIEQEGP<mask><mask>WDQ<mask>TRNMKAHSQTD<mask>RNLGTLR<mask>YYNQSEDGSHTIQIMYGCDVGPDGRFLRGYRQDAYDGKDYIALSEDLRSWTAADMAAQITKRKWEADHAAEQRRV<mask>LEGRC<mask>DGLRRYLENG<mask><mask>TLQRTDP<mask>KT<mask>MT<mask><mask>PISDHEA<mask>LSC<mask>A<mask>GF<mask>PA<mask>I<mask>L<mask>WQRDGE<mask>QTQDTEL<mask>ETRPAGDGTF<mask>KN<mask>AVVVPSGEEQRYTCHVQHEGLPKPL<mask>LRWELSSGPTKPIVGIIAGLVLL<mask>AVITGAVVAAV<mask>WRRKSSDR<mask>GG<mask>YTQ<mask>ASSDSAQGSDVSLTACK<mask>'], 'clean_sequences_ESMprotein': [ESMProtein(sequence='MAVMAPRTLLLLLSGALALTQTWAGSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQKMEPRAPWIEQEGPEYWDQETRNMKAHSQTDRANLGTLRGYYNQSEDGSHTIQIMYGCDVGPDGRFLRGYRQDAYDGKDYIALNEDLRSWTAADMAAQITKRKWEAVHAAEQRRVYLEGRCVDGLRRYLENGKETLQRTDPPKTHMTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWELSSQPTIPIVGIIAGLVLLGAVITGAVVAAVMWRRKSSDRKGGSYTQAASSDSAQGSDVSLTACKV', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MAVMAPRTLVLLLSGALALTQTWAGSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETLQRTDAPKTHMTHHAVSDHEATLRCWALSFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGQEQRYTCHVQHEGLPKPLTLRWEPSSQPTIPIVGIIAGLVLFGAVITGAVVAAVMWRRKSSDRKGGSYSQAASSDSAQGSDVSLTACKV', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MAVMAPRTLLLLLSGALALTQTWAGSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQKMEPRAPWIEQEGPEYWDQETRNMKAHSQTDRANLGTLRGYYNQSEDGSHTIQIMYGCDVGPDGRFLRGYRQDAYDGKDYIALNEDLRSWTAADMAAQITKRKWEAVHAAEQRRVYLEGRCVDGLRRYLENGKETLQRTDPPKTHMTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWELSSQPTIPIVGIIAGLVLLGAVITGAVVAAVMWRRKSSDRKGGSYTQAASSDSAQGSDVSLTACKV', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MAVMAPRTLVLLLSGALALTQTWAGSHSMRYFSTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDEETGKVKAHSQTDRENLRIALRYYNQSEAGSHTLQMMFGCDVGSDGRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQITKRKWEAAHVAEQQRAYLEGTCVDGLRRYLENGKETLQRTDPPKTHMTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWEPSSQPTVPIVGIIAGLVLLGAVITGAVVAAVMWRRNSSDRKGGSYSQAASSDSAQGSDVSLTACKV', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MAVMAPRTLVLLLSGALALTQTWAGSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETLQRTDAPKTHMTHHAVSDHEATLRCWALSFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGQEQRYTCHVQHEGLPKPLTLRWEPSSQPTIPIVGIIAGLVLFGAVITGAVVAAVMWRRKSSDRKGGSYSQAASSDSAQGSDVSLTACKV', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MLVMAPRTVLLLLSAALALTETWAGSHSMRYFDTAMSRPGRGEPRFISVGYVDDTQFVRFDSDAASPREEPRAPWIEQEGPEYWDRNTQIFKTNTQTDRESLRNLRGYYNQSEAGSHTLQSMYGCDVGPDGRLLRGHNQYAYDGKDYIALNEDLRSWTAADTAAQITQRKWEAARVAEQDRAYLEGTCVEWLRRYLENGKDTLERADPPKTHVTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDRTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWEPSSQSTVPIVGIVAGLAVLAVVVIGAVVAAVMCRRKSSGGKGGSYSQAACSDSAQGSDVSLTA', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MAVMAPRTLLLLLSGALALTQTWAGSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQKMEPRAPWIEQEGPEYWDQETRNMKAHSQTDRANLGTLRGYYNQSEDGSHTIQIMYGCDVGPDGRFLRGYRQDAYDGKDYIALNEDLRSWTAADMAAQITKRKWEAVHAAEQRRVYLEGRCVDGLRRYLENGKETLQRTDPPKTHMTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWELSSQPTIPIVGIIAGLVLLGAVITGAVVAAVMWRRKSSDRKGGSYTQAASSDSAQGSDVSLTACKV', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MAVMAPRTLLLLLSGALALTQTWAGSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQKMEPRAPWIEQEGPEYWDQETRNMKAHSQTDRANLGTLRGYYNQSEDGSHTIQIMYGCDVGPDGRFLRGYRQDAYDGKDYIALNEDLRSWTAADMAAQITKRKWEAVHAAEQRRVYLEGRCVDGLRRYLENGKETLQRTDPPKTHMTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWELSSQPTIPIVGIIAGLVLLGAVITGAVVAAVMWRRKSSDRKGGSYTQAASSDSAQGSDVSLTACKV', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False)], 'masked_sequences_ESMprotein': [ESMProtein(sequence='<mask>AVMAPRTLLLLLS<mask>ALALTQTWAGSHSMRY<mask>FTSVSRPGRGERRFIVVGYVDD<mask>QFVMFDSDAASQKM<mask>PRAPW<mask>EQEG<mask>E<mask>WDQE<mask>RNMK<mask>HSPT<mask>R<mask>NLGTLRGYYNQSEDGSHTIQIMYGCDVGPDGRFLRTYRQ<mask>AYDGKDYIALNEDLRSW<mask>AADMAAQITK<mask>KWEAVHAHEQRRVYLEGR<mask>VDG<mask>RRYLENGKETL<mask>RTDPPKT<mask>MTH<mask>PISD<mask>EATLRCWALG<mask>YPAEITLTWQRDGED<mask>T<mask><mask>TEL<mask>ETRPAG<mask>GTFNKWKAVVVPSGEEQRYTCHVQHEGLPKPLTLRWELSSQPTIP<mask>VGIIAG<mask><mask>LLGAVIT<mask>AV<mask>AAVMWRIKSSDRKGG<mask><mask><mask>QA<mask>SSDSAQGSDVSLTACKV', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MAVMAPRTLVLLLSGALALTQTWAGSH<mask>MRYFFTS<mask>SRP<mask>RGEPRFI<mask>VCYVDDTQFVRFDSDAA<mask>QRMEPR<mask>PWIE<mask>EGPEYWDGETRKVKAH<mask>Q<mask>HRVDLGTLRGYYNQS<mask><mask>GSHT<mask>QRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLFS<mask>TAACMA<mask>Q<mask><mask><mask>HKWEA<mask>HVA<mask>QLRAYLEG<mask>CVEWLRRYLENGKETLQR<mask>DAPP<mask>HMTSHAVSD<mask><mask>ATLRCWALSFYP<mask><mask>I<mask>LTWQRSGEDQTQDTELVETRPAGD<mask>TFQKWAAV<mask><mask>PSGKEQRYTCHVQH<mask>GL<mask>KPLTLRWEPSSQP<mask>IPISGIIAGLVLFGAVITG<mask>VVAAVMWRRKSSDCKG<mask>SYSQAASSDSAQGSDVSLTAC<mask>V', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MAVMAPWTL<mask>LLL<mask>GALALT<mask>TWACS<mask>SMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRF<mask>SDAAS<mask>KMEARA<mask>WI<mask>QEGPE<mask>WDQETRNMKAHSQTDRAELGTLRGY<mask>NQSEDGSHT<mask>QIMYGCDVGPDGRFLR<mask>YRQDAYDGKDYIAL<mask>EDGRSWTRA<mask>MAAQITKRKW<mask><mask>VH<mask>AEQRRVYLE<mask>RCVDGLRRYLENGKETLQRTD<mask>PKTH<mask>T<mask>HPISDHEATLRCWALG<mask>YPAEITLTNQRDG<mask>DQTQDT<mask>LVETRPAFDGTFQKWAA<mask>VVPSGEEQRYTCHVQHEHLPKPLTLRWELSSQPTIPIVG<mask><mask>AGLVL<mask>GAV<mask>TGA<mask>VAAVMWRRKS<mask>DR<mask>GGSYT<mask>A<mask>SSDSAQG<mask><mask>VSLTACKV', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='M<mask>VMAPRTLVLLLSGACALTQTWAGS<mask>SMRYDSTSVS<mask>PGRGEP<mask><mask>IAVG<mask><mask>DDTQ<mask>VHFDSDAASQ<mask>MERRAPWI<mask>QEGPEYWDE<mask>TGKVKAH<mask><mask>TDRENLRIALRYYNQ<mask>EAGSHTLQMMFGCDVGSDG<mask><mask>LCGYHQYAYDGKD<mask>IALKEDLR<mask>WTAADMAAQI<mask>KRKWEAAHVAEQQRAYLEGTCYDNLRRYLENGKETLQRTDPPKTHMTHHPISDHEATLRCWALGFYPAEI<mask>LTW<mask>R<mask>GEDQTQ<mask>TELVETRPAGDGTFQKWAAVVVPSGEE<mask>MYTCHVQHEGLP<mask>PLTLRWEPSS<mask>P<mask><mask>PIVGI<mask>AGLVLLGA<mask>ITGAVVAAVMWRRNSSDR<mask>G<mask><mask>YSQ<mask>ASSDSA<mask>GSD<mask>SLTACKV', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MAVMAPRTLVLLL<mask>GALALTQTWA<mask>SHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRF<mask>SDA<mask>SQRMEPRAP<mask>IEQE<mask>PEYWDGETRKVKAHSQTHR<mask>DLGTLRGYTNQSEA<mask>SH<mask>VQRM<mask>GCD<mask>GSDWRFLR<mask>YHQ<mask>AYD<mask>KDYIALK<mask>DLR<mask><mask><mask>AADM<mask>AQTTKHKWEAAHVAEQ<mask>RAYLEMFC<mask>EYLRRYL<mask>N<mask>KETLHRTDAPKTHMTHHAVSD<mask>EATL<mask>CWALSFYPAEITLTW<mask>RDGEDQTQDTELVE<mask>R<mask>AGDGTFQKWA<mask>VVVPSGQEQRYTCHVQHKGLPKPLTTRWEPSSQPTIPIVGIIAG<mask>VLFLA<mask>I<mask>GAVVEAVMWRRKSS<mask>RKGG<mask>YSQA<mask>SS<mask>SAQGSDVSLTACKV', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MTVMAP<mask><mask>V<mask>LLLSAAL<mask>LTET<mask>AGSHSMRYFDT<mask>MSRP<mask>RG<mask>PRFISVGYVDYT<mask>FVRFD<mask><mask>AA<mask>PREEPRAPWIEQE<mask>PEYWDRNTQIFKTNTQTDR<mask>SLRDLRGYYNQSEAGSHTLQ<mask>MYGCDVGPDGRLL<mask>GHNQYAYDGKDYIALNEDLR<mask>WTAA<mask>TAAQITQRKWEAARVA<mask>QDRA<mask>LEGT<mask>VEWMRRYLENG<mask>DT<mask>ERSDPPKTHV<mask>HHP<mask>WDHEVTLRCWALGFYPAEIT<mask>TWQRDGE<mask>QTQDTELVETRPAGDRTFQKWAAVVV<mask><mask>GEEQRY<mask><mask>HVQHEGLPKPLTLRWEPSSQSTVPIVGIVACLAVLAVVVIGA<mask>VAYV<mask>CRR<mask>SSG<mask>KGGS<mask>SQAACSDSAQGSDVSLTA', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MEV<mask>APRTLLLLLSGA<mask>A<mask>TQTWAGSHSMRYFFT<mask>MSRMG<mask>GEPRFI<mask>VGYVDDTQF<mask>RFDSDAASQKMEPRAP<mask>PEQE<mask>PEYWDQETR<mask>MKAHSQTDRANL<mask>TLRGYYNQSEDGSHIIQIMYGCDVHPDGRFLRGYRTDAYD<mask>KD<mask>IALNEDWRSWTAADMAAQITLRKWEAVHAAEQ<mask>RVYLEGR<mask>VD<mask>LR<mask>YL<mask>NGKE<mask>LQRTDP<mask>KTHMTHHPISD<mask>EATLRCWAL<mask>F<mask>P<mask>EITLTWQRDGEDQT<mask>DTELVETRPAGDGTFQKWAAVVVPSGEEQR<mask><mask>C<mask><mask>Q<mask>EGLPKPLTLRWELSSQPTIPIVG<mask>IAGLVL<mask>GAVITGAVVAAVM<mask>RRKSSDRKGGSYTQAAS<mask>DSAQGS<mask>VSLTAC<mask>V', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MAVM<mask>PRTLLLLLSGALALTQTWAGSH<mask>MRY<mask>FTSVSRPGRGE<mask>RFIAVGY<mask>DDTQFVRFDSDAASQKMEPRAMWIEQEGP<mask><mask>WDQ<mask>TRNMKAHSQTD<mask>RNLGTLR<mask>YYNQSEDGSHTIQIMYGCDVGPDGRFLRGYRQDAYDGKDYIALSEDLRSWTAADMAAQITKRKWEADHAAEQRRV<mask>LEGRC<mask>DGLRRYLENG<mask><mask>TLQRTDP<mask>KT<mask>MT<mask><mask>PISDHEA<mask>LSC<mask>A<mask>GF<mask>PA<mask>I<mask>L<mask>WQRDGE<mask>QTQDTEL<mask>ETRPAGDGTF<mask>KN<mask>AVVVPSGEEQRYTCHVQHEGLPKPL<mask>LRWELSSGPTKPIVGIIAGLVLL<mask>AVITGAVVAAV<mask>WRRKSSDR<mask>GG<mask>YTQ<mask>ASSDSAQGSDVSLTACK<mask>', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False)], 'masked_input_ids_ESMprotein_batched': _BatchedESMProteinTensor(sequence=tensor([[ 0, 32,  5,  ..., 15,  7,  2],\n",
      "        [ 0, 20,  5,  ..., 32,  7,  2],\n",
      "        [ 0, 20,  5,  ..., 15,  7,  2],\n",
      "        ...,\n",
      "        [ 0, 20, 11,  ...,  1,  1,  1],\n",
      "        [ 0, 20,  9,  ..., 32,  7,  2],\n",
      "        [ 0, 20,  5,  ..., 15, 32,  2]]), structure=None, secondary_structure=None, sasa=None, function=None, residue_annotations=None, coordinates=None, potential_sequence_of_concern=False), 'clean_input_ids_ESMprotein_batched': _BatchedESMProteinTensor(sequence=tensor([[ 0, 20,  5,  ..., 15,  7,  2],\n",
      "        [ 0, 20,  5,  ..., 15,  7,  2],\n",
      "        [ 0, 20,  5,  ..., 15,  7,  2],\n",
      "        ...,\n",
      "        [ 0, 20,  4,  ...,  1,  1,  1],\n",
      "        [ 0, 20,  5,  ..., 15,  7,  2],\n",
      "        [ 0, 20,  5,  ..., 15,  7,  2]]), structure=None, secondary_structure=None, sasa=None, function=None, residue_annotations=None, coordinates=None, potential_sequence_of_concern=False)}\n",
      "{'z': tensor([[[[-7.5787e+01, -5.1310e+01,  9.8781e+01,  ...,  2.0847e+02,\n",
      "            4.0586e+01, -1.0726e+02],\n",
      "          [-6.3666e+01, -5.9162e+01, -5.1830e+01,  ..., -3.2411e+01,\n",
      "            1.0520e+01, -1.9134e+01],\n",
      "          [-1.0415e+01, -2.2381e+01, -1.4173e+01,  ..., -5.0613e+01,\n",
      "            4.7942e+01, -5.3205e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-2.7920e+01,  2.8625e+00,  6.4688e+01,  ...,  4.4811e+01,\n",
      "            1.5642e+01,  3.4788e+01],\n",
      "          [-2.6895e+01, -2.3034e+01,  6.2510e+01,  ...,  8.8285e+01,\n",
      "           -1.9051e+00, -1.3137e+02],\n",
      "          [-1.4323e+01, -2.6959e+00,  2.3477e+01,  ...,  3.5719e+00,\n",
      "            3.3964e-02, -6.1911e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-9.7394e+00,  2.0081e+01,  3.4673e+01,  ..., -2.7890e-01,\n",
      "           -1.0967e+01,  3.4225e+01],\n",
      "          [ 9.4209e+00,  1.2335e+00,  9.0935e+01,  ...,  4.4095e+01,\n",
      "           -3.8400e+01,  7.3194e+01],\n",
      "          [ 1.7774e+01, -9.6788e+00, -1.2485e+01,  ...,  1.2586e+01,\n",
      "            4.0355e+01, -7.7395e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-9.0633e+01, -5.3944e+01,  9.9216e+01,  ...,  1.9452e+02,\n",
      "            2.9525e+01, -1.0949e+02],\n",
      "          [-6.1494e+01, -6.0051e+01, -5.9876e+01,  ..., -5.2594e+01,\n",
      "            4.1139e+00, -2.2814e+01],\n",
      "          [ 2.7097e-01, -2.5368e+01, -8.3957e+00,  ..., -4.8651e+01,\n",
      "            4.6714e+01, -5.9701e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-4.0599e+01,  1.4086e+01,  6.2028e+01,  ...,  4.9069e+01,\n",
      "            2.0663e+01,  3.6595e+01],\n",
      "          [-3.4037e+01, -6.5183e+00,  6.2519e+01,  ...,  8.9143e+01,\n",
      "           -9.1002e+00, -1.4977e+02],\n",
      "          [ 1.6794e+00, -8.7587e+00,  3.2249e+01,  ...,  1.3206e+01,\n",
      "            1.1681e+01, -6.6727e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-1.4467e+00,  1.5287e+01,  2.8315e+01,  ...,  3.0188e+00,\n",
      "           -9.7570e+00,  2.4901e+01],\n",
      "          [ 1.1826e+01, -3.1632e+00,  8.9367e+01,  ...,  4.4564e+01,\n",
      "           -3.8673e+01,  5.0349e+01],\n",
      "          [ 1.1039e+01, -1.1170e+01, -8.0234e+00,  ...,  7.8601e+00,\n",
      "            3.5962e+01, -8.0373e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-8.0584e+01, -4.7928e+01,  9.8558e+01,  ...,  2.0461e+02,\n",
      "            3.4531e+01, -1.0459e+02],\n",
      "          [-6.4622e+01, -5.1473e+01, -5.0964e+01,  ..., -4.0959e+01,\n",
      "            5.5560e+00, -1.5845e+01],\n",
      "          [-1.0542e+01, -2.5485e+01, -1.0878e+01,  ..., -5.0264e+01,\n",
      "            4.7069e+01, -5.0074e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-3.1890e+01,  6.5963e+00,  6.8057e+01,  ...,  3.9506e+01,\n",
      "            1.3026e+01,  3.8050e+01],\n",
      "          [-2.1317e+01, -7.6482e+00,  6.6364e+01,  ...,  9.9257e+01,\n",
      "            1.1724e+01, -1.4415e+02],\n",
      "          [-8.9612e-01, -1.0810e+01,  2.2784e+01,  ...,  1.2917e+00,\n",
      "            6.1515e+00, -6.3194e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-9.8199e+00,  1.7599e+01,  3.4084e+01,  ..., -1.8010e+00,\n",
      "           -1.0067e+01,  2.9578e+01],\n",
      "          [ 1.1699e+01,  4.0391e+00,  9.3571e+01,  ...,  3.9873e+01,\n",
      "           -3.6558e+01,  5.8486e+01],\n",
      "          [ 1.5462e+01, -7.5816e+00, -1.2345e+01,  ...,  8.0321e+00,\n",
      "            4.4006e+01, -7.7038e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-8.5675e+01, -5.6557e+01,  9.9903e+01,  ...,  1.9795e+02,\n",
      "            3.3628e+01, -1.0696e+02],\n",
      "          [-5.8795e+01, -6.0209e+01, -6.0639e+01,  ..., -4.3757e+01,\n",
      "            5.9186e+00, -2.0508e+01],\n",
      "          [-2.0193e+00, -2.8314e+01, -1.1540e+01,  ..., -5.0825e+01,\n",
      "            4.9439e+01, -5.7245e+01],\n",
      "          ...,\n",
      "          [-6.1280e+00,  1.0689e+01,  3.6608e+00,  ..., -3.1073e+00,\n",
      "            1.2708e+01, -4.7829e+00],\n",
      "          [-9.9795e+00,  3.1534e+00,  6.2991e+00,  ..., -2.0562e+00,\n",
      "            9.2929e+00,  2.7467e+00],\n",
      "          [-3.2073e+01,  2.9799e+01,  1.3938e+01,  ...,  2.2661e+01,\n",
      "            8.7887e+00,  3.1207e+00]],\n",
      "\n",
      "         [[-3.7015e+01,  1.0347e+01,  6.0538e+01,  ...,  4.7303e+01,\n",
      "            1.8857e+01,  4.0805e+01],\n",
      "          [-3.5532e+01, -1.1926e+01,  6.8037e+01,  ...,  8.6972e+01,\n",
      "           -4.3160e+00, -1.4841e+02],\n",
      "          [-3.9978e+00, -1.3038e+01,  3.0272e+01,  ...,  1.0824e+01,\n",
      "            1.4609e+01, -6.9711e+01],\n",
      "          ...,\n",
      "          [-6.2685e+00, -3.6390e+00, -1.1288e+01,  ...,  1.2923e+01,\n",
      "            4.2697e+00, -3.2927e+00],\n",
      "          [-1.3622e+01,  1.2895e-01, -1.2627e+01,  ...,  3.1730e+00,\n",
      "            9.8878e-01, -8.8053e+00],\n",
      "          [-1.4863e+01,  1.0134e+01, -8.7632e+00,  ..., -6.2454e+00,\n",
      "            1.1382e+01, -1.5462e+01]],\n",
      "\n",
      "         [[-3.0478e+00,  1.8920e+01,  2.8517e+01,  ...,  3.5334e+00,\n",
      "           -1.2132e+01,  2.8910e+01],\n",
      "          [ 1.1321e+01,  5.1602e-01,  8.7559e+01,  ...,  4.5745e+01,\n",
      "           -4.0812e+01,  5.9416e+01],\n",
      "          [ 1.5381e+01, -1.1881e+01, -1.0085e+01,  ...,  9.8215e+00,\n",
      "            4.1938e+01, -8.2825e+01],\n",
      "          ...,\n",
      "          [ 9.4441e-01, -7.1907e+00, -4.4117e-03,  ...,  1.6856e+01,\n",
      "            5.4601e+00, -5.5743e+00],\n",
      "          [-1.1009e+01,  2.2721e+00, -4.4464e+00,  ...,  7.7782e+00,\n",
      "            1.1162e+00, -8.7675e-01],\n",
      "          [-1.0276e+01,  7.9159e+00, -3.0885e+00,  ..., -3.2684e+00,\n",
      "            1.2607e+01, -7.3048e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1380e+01,  1.2231e+01, -7.9701e+00,  ..., -1.9197e+01,\n",
      "            9.8349e+00, -1.8719e+01],\n",
      "          [ 1.9333e+01, -2.7691e+00,  3.3259e+00,  ..., -9.1672e+00,\n",
      "            8.7885e+00, -7.6842e+00],\n",
      "          [ 1.1863e+01, -1.0818e+00,  2.3047e+00,  ..., -4.4312e+00,\n",
      "            2.4613e+00, -1.2729e+01],\n",
      "          ...,\n",
      "          [ 7.2059e+01,  9.8580e+00,  6.8542e+01,  ...,  1.9241e+01,\n",
      "           -2.8864e+01, -1.2759e+02],\n",
      "          [ 1.4003e+01, -1.3780e+01,  2.8244e+01,  ...,  1.6128e+01,\n",
      "           -3.6242e+01, -8.6507e+00],\n",
      "          [-1.1069e+01,  1.2709e+01,  1.1287e+01,  ...,  7.7137e+00,\n",
      "           -1.2927e+01, -1.0244e+01]],\n",
      "\n",
      "         [[-7.9335e+00,  5.6292e+00, -6.2357e+00,  ..., -2.5344e+01,\n",
      "            6.3168e+00, -2.6695e+01],\n",
      "          [ 1.8482e+01, -2.4724e+00,  1.2176e+00,  ..., -1.7628e+01,\n",
      "            2.0431e+00, -1.6093e+01],\n",
      "          [ 2.2905e+00, -1.3309e-01, -2.5560e+00,  ..., -1.2732e+01,\n",
      "            2.5616e-01, -9.5704e+00],\n",
      "          ...,\n",
      "          [ 1.1594e+01,  4.9203e+00,  5.3738e+01,  ...,  4.4892e+01,\n",
      "           -4.6698e+01,  2.8097e+01],\n",
      "          [ 3.2627e+01, -1.6924e+01, -2.7710e+01,  ...,  3.8002e+01,\n",
      "            8.4498e+00, -6.0959e+01],\n",
      "          [ 1.5938e+01,  5.5487e+00,  1.2822e+01,  ...,  4.1582e+00,\n",
      "           -4.0532e+01,  1.3745e+01]],\n",
      "\n",
      "         [[-1.4560e+01,  1.4676e+01,  4.2849e-01,  ...,  4.2711e+00,\n",
      "            1.1763e+01, -1.4445e+01],\n",
      "          [ 9.8684e+00,  6.2970e+00,  1.1367e+01,  ..., -2.2512e+01,\n",
      "            1.6250e+01, -9.4996e+00],\n",
      "          [ 3.8045e+00,  1.1691e+00,  5.9747e+00,  ..., -2.1614e+01,\n",
      "            4.2535e+00, -7.9023e+00],\n",
      "          ...,\n",
      "          [-9.2030e+00, -2.5783e+01,  3.0335e+01,  ...,  1.5717e+01,\n",
      "            8.2291e+00,  3.7036e+01],\n",
      "          [-2.8226e+01,  1.1563e+01,  4.6457e+01,  ...,  6.2266e+01,\n",
      "           -1.2297e+01,  4.5106e+01],\n",
      "          [-4.4053e+01, -6.8772e+01,  3.6863e+01,  ...,  1.2060e+02,\n",
      "           -6.0756e+01, -1.4963e+02]]],\n",
      "\n",
      "\n",
      "        [[[-8.6232e+01, -5.6776e+01,  9.8703e+01,  ...,  2.0233e+02,\n",
      "            3.5484e+01, -1.0922e+02],\n",
      "          [-5.9760e+01, -5.7313e+01, -5.6781e+01,  ..., -4.3028e+01,\n",
      "            5.1713e+00, -2.2068e+01],\n",
      "          [-1.6791e+00, -2.1682e+01, -1.2215e+01,  ..., -4.8484e+01,\n",
      "            4.4818e+01, -5.2936e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-3.3361e+01,  1.0509e+01,  6.3771e+01,  ...,  4.4376e+01,\n",
      "            1.7950e+01,  3.5115e+01],\n",
      "          [-2.7916e+01, -1.0028e+01,  6.4843e+01,  ...,  9.7112e+01,\n",
      "           -1.7003e+00, -1.5461e+02],\n",
      "          [-6.1033e+00, -6.4743e+00,  3.0683e+01,  ...,  6.1962e+00,\n",
      "            6.4106e+00, -6.6899e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-4.2062e+00,  1.6651e+01,  3.0384e+01,  ...,  2.7812e-01,\n",
      "           -1.1058e+01,  2.7071e+01],\n",
      "          [ 9.8915e+00, -5.1809e+00,  9.8344e+01,  ...,  4.8600e+01,\n",
      "           -4.2017e+01,  5.5031e+01],\n",
      "          [ 1.3458e+01, -1.1837e+01, -1.1488e+01,  ...,  2.6700e+00,\n",
      "            3.9189e+01, -7.9381e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-8.6405e+01, -5.5849e+01,  9.6350e+01,  ...,  1.9937e+02,\n",
      "            3.2435e+01, -1.1418e+02],\n",
      "          [-5.8095e+01, -6.2254e+01, -6.4914e+01,  ..., -4.6129e+01,\n",
      "            9.3389e+00, -2.1827e+01],\n",
      "          [ 1.3736e+00, -2.1307e+01, -1.1489e+01,  ..., -4.8070e+01,\n",
      "            4.7555e+01, -5.5138e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-3.5942e+01,  1.5130e+01,  5.5314e+01,  ...,  4.6797e+01,\n",
      "            2.0483e+01,  3.2103e+01],\n",
      "          [-2.4082e+01, -9.6169e+00,  5.7458e+01,  ...,  1.0017e+02,\n",
      "            1.3396e+01, -1.5304e+02],\n",
      "          [-1.5497e+00, -4.6722e+00,  2.6928e+01,  ...,  1.4799e+01,\n",
      "            8.3867e+00, -7.0396e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-1.3571e+00,  1.5344e+01,  2.7870e+01,  ...,  7.7299e-01,\n",
      "           -1.0111e+01,  2.4872e+01],\n",
      "          [ 8.1904e+00, -5.0364e+00,  9.4363e+01,  ...,  5.2095e+01,\n",
      "           -3.9099e+01,  4.8921e+01],\n",
      "          [ 1.2717e+01, -1.0721e+01, -1.2649e+01,  ...,  5.5267e+00,\n",
      "            4.1422e+01, -7.9576e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]]), 'pep_len': array([ 9,  9,  9, 10,  9,  9,  9, 10]), 'tcra_len': array([110, 112, 110, 115, 112, 117, 113, 113]), 'tcrb_len': array([114, 116, 115, 115, 115, 117, 115, 115]), 'hla_len': array([365, 362, 365, 362, 362, 365, 365, 365])}\n"
     ]
    }
   ],
   "source": [
    "# try for loop for one batch (break at the end)\n",
    "for tcr_batch, pep_batch, hla_batch, boltz_batch in zip(tcr_loader, pep_loader, hla_loader, boltz_loader):\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    break\n",
    "\n",
    "# to load one batch from each loader on CPU took 6.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08c16c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESM forwards (T+P+H) took: 17.520 s\n"
     ]
    }
   ],
   "source": [
    "# Testing on one batch so far\n",
    "# 1. Pick a batch and get token tensors and masks\n",
    "\n",
    "tcr_batch = next(iter(tcr_loader))\n",
    "pep_batch = next(iter(pep_loader))\n",
    "hla_batch = next(iter(hla_loader))\n",
    "\n",
    "tcr_ids = tcr_batch['clean_input_ids']\n",
    "tcr_mask = tcr_batch['attention_mask']\n",
    "\n",
    "pep_ids = pep_batch['clean_input_ids']\n",
    "pep_mask = pep_batch['attention_mask']\n",
    "\n",
    "hla_ids = hla_batch['clean_input_ids']\n",
    "hla_mask = hla_batch['attention_mask']\n",
    "\n",
    "# Unsqueze to make the right dims\n",
    "mT = tcr_mask.unsqueeze(-1).float()   # (B, L_T_pad, 1)\n",
    "mP = pep_mask.unsqueeze(-1).float()   # (B, L_P_pad, 1)\n",
    "mH = hla_mask.unsqueeze(-1).float()   # (B, L_H_pad, 1)\n",
    "\n",
    "\n",
    "#B = tcr_ids.size(0)\n",
    "\n",
    "# ---- TIME ESM FORWARDS ----\n",
    "t0 = time.perf_counter()\n",
    "# Call the BASE model inside the LoRA wrapper\n",
    "with torch.no_grad():\n",
    "    out_T = tcr_encoder.model.forward(sequence_tokens=tcr_ids)\n",
    "    out_P = peptide_encoder.model.forward(sequence_tokens=pep_ids)\n",
    "    out_H = hla_encoder.model.forward(sequence_tokens=hla_ids)\n",
    "t1 = time.perf_counter()\n",
    "print(f\"ESM forwards (T+P+H) took: {t1 - t0:.3f} s\")\n",
    "\n",
    "\n",
    "# Some ESMC builds expose .embeddings; otherwise take last hidden state\n",
    "emb_T = out_T.embeddings \n",
    "emb_P = out_P.embeddings \n",
    "emb_H = out_H.embeddings\n",
    "\n",
    "# it is true that the shape of the embeddings is (B, L_pad, D)\n",
    "# takes 18 seconds on CPU to do 24 embeddings\n",
    "\n",
    "# Shapes\n",
    "# is this B the same as the above B I defined?\n",
    "B, L_T_pad, D = emb_T.shape\n",
    "\n",
    "rL = 8\n",
    "rD = 16\n",
    "d  = 128      # shared d\n",
    "\n",
    "# true length estimates\n",
    "L_T_true = tcr_mask.sum(dim=1)\n",
    "L_T_max  = L_T_true.max()\n",
    "\n",
    "L_P_true = pep_mask.sum(dim=1)\n",
    "L_P_max  = L_P_true.max()\n",
    "\n",
    "L_H_true = hla_mask.sum(dim=1)\n",
    "L_H_max  = L_H_true.max()\n",
    "\n",
    "# Encoders\n",
    "tcr_encoder = ESMFactorisedEncoder(D, rL, rD, d, L_max=L_T_max).to(device)\n",
    "\n",
    "pmhc_encoder = PMHCFactorisedEncoder(\n",
    "    D, rL, rD, d,\n",
    "    L_P_max=L_P_max,\n",
    "    L_H_max=L_H_max,\n",
    "    R_PH=0.7\n",
    ").to(device)\n",
    "\n",
    "# Forward\n",
    "zT = tcr_encoder(emb_T, tcr_mask)                    # (B, d)\n",
    "zT = zT / (zT.norm(dim=-1, keepdim=True) + eps)          # normalise TCR\n",
    "\n",
    "zPH = pmhc_encoder(emb_P, pep_mask, emb_H, hla_mask) # (B, d)\n",
    "\n",
    "# Final fused representation\n",
    "e_hat = torch.cat([zT, zPH], dim=-1)                     # (B, 2d)\n",
    "\n",
    "\n",
    "# takes 18s for 1 batch :/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e3bfeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc8ab03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6818f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e789a3f",
   "metadata": {},
   "source": [
    "#### Code below is old code pre reorganisation and full loss function building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939b46f2",
   "metadata": {},
   "source": [
    "##### a) Factorisation/Projection for ESMC Encoders\n",
    "- N.B. From next cell onwards, need to reconfigure to do for loop for all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "561878ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pick a batch and get token tensors and masks\n",
    "\n",
    "tcr_batch = next(iter(tcr_loader))\n",
    "pep_batch = next(iter(pep_loader))\n",
    "hla_batch = next(iter(hla_loader))\n",
    "\n",
    "tcr_ids = tcr_batch['clean_input_ids']\n",
    "tcr_mask = tcr_batch['attention_mask']\n",
    "\n",
    "pep_ids = pep_batch['clean_input_ids']\n",
    "pep_mask = pep_batch['attention_mask']\n",
    "\n",
    "hla_ids = hla_batch['clean_input_ids']\n",
    "hla_mask = hla_batch['attention_mask']\n",
    "\n",
    "# Unsqueze to make the right dims\n",
    "mT = tcr_mask.unsqueeze(-1).float()   # (B, L_T_pad, 1)\n",
    "mP = pep_mask.unsqueeze(-1).float()   # (B, L_P_pad, 1)\n",
    "mH = hla_mask.unsqueeze(-1).float()   # (B, L_H_pad, 1)\n",
    "\n",
    "\n",
    "B = tcr_ids.size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "584ac9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the BASE model inside the LoRA wrapper\n",
    "with torch.no_grad():\n",
    "    out_T = tcr_encoder.model.forward(sequence_tokens=tcr_ids)\n",
    "    out_P = peptide_encoder.model.forward(sequence_tokens=pep_ids)\n",
    "    out_H = hla_encoder.model.forward(sequence_tokens=hla_ids)\n",
    "\n",
    "# Some ESMC builds expose .embeddings; otherwise take last hidden state\n",
    "emb_T = out_T.embeddings \n",
    "emb_P = out_P.embeddings \n",
    "emb_H = out_H.embeddings\n",
    "\n",
    "# it is true that the shape of the embeddings is (B, L_pad, D)\n",
    "# takes 18 seconds on CPU to do 24 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a921c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Think this cell is included in the below\n",
    "# # maximum true tcr length \n",
    "# L_T_true = tcr_mask.sum(dim=1)\n",
    "# L_T_max = L_T_true.max()\n",
    "\n",
    "# # maximum true peptide length \n",
    "# L_P_true = pep_mask.sum(dim=1)\n",
    "# L_P_max = L_P_true.max()\n",
    "\n",
    "# # maximum true HLA length \n",
    "# L_H_true = hla_mask.sum(dim=1)\n",
    "# L_H_max = L_H_true.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b2e111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorised Encoder to get z_T and Z_pMHC\n",
    "# z = vec(A^TXB)H\n",
    "# X - (B, L_pad, D)\n",
    "# B - (D, rD)\n",
    "# A - (L_pad, rL)\n",
    "# H - (rD * rL, d)\n",
    "\n",
    "\n",
    "eps = 1e-8\n",
    "\n",
    "class ESMFactorisedEncoder(nn.Module):\n",
    "    def __init__(self, D, rL, rD, d, L_max):\n",
    "        \"\"\"\n",
    "        D    : ESM embedding dim (e.g. 960)\n",
    "        rL   : positional rank\n",
    "        rD   : channel rank\n",
    "        d    : latent dim\n",
    "        L_max: max true length for this modality in the batch\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.D   = D\n",
    "        self.rL  = rL\n",
    "        self.rD  = rD\n",
    "        self.d   = d\n",
    "        self.L_max = L_max\n",
    "\n",
    "        # Channel mixing: D -> rD\n",
    "        self.B_c = nn.Parameter(torch.empty(D, rD))\n",
    "        nn.init.xavier_uniform_(self.B_c)\n",
    "\n",
    "        # Positional mixing: positions 0..L_max-1 -> rL\n",
    "        self.A_c = nn.Parameter(torch.empty(L_max, rL))\n",
    "        nn.init.xavier_uniform_(self.A_c)\n",
    "\n",
    "        # Final map: (rL * rD) -> d\n",
    "        self.H_c = nn.Parameter(torch.empty(rL * rD, d))\n",
    "        nn.init.xavier_uniform_(self.H_c)\n",
    "\n",
    "    def forward(self, emb, mask):\n",
    "        \"\"\"\n",
    "        emb  : (B, L_pad, D) token embeddings\n",
    "        mask : (B, L_pad)   1 = real token, 0 = pad\n",
    "        returns z : (B, d)\n",
    "        \"\"\"\n",
    "        device = emb.device\n",
    "        B, L_pad, D = emb.shape\n",
    "        assert D == self.D\n",
    "\n",
    "        # Compute true lengths\n",
    "        L_true = mask.sum(dim=1)            # (B,)\n",
    "        z_list = []\n",
    "\n",
    "        for b in range(B):\n",
    "            Lb = int(L_true[b].item())\n",
    "            if Lb == 0:\n",
    "                # Degenerate case: no tokens -> zero vector\n",
    "                z_b = torch.zeros(self.d, device=device)\n",
    "                z_list.append(z_b)\n",
    "                continue\n",
    "\n",
    "            Xb = emb[b, :Lb, :]                      # (Lb, D)\n",
    "            mb = mask[b, :Lb].unsqueeze(-1).float()  # (Lb, 1)\n",
    "            Xb = Xb * mb                             # (Lb, D)\n",
    "\n",
    "            # 1) Channel compression: D -> rD\n",
    "            Yb = Xb @ self.B_c                       # (Lb, rD)\n",
    "\n",
    "            # 2) Positional compression: Lb -> rL\n",
    "            A_pos = self.A_c[:Lb, :]                 # (Lb, rL)\n",
    "            Ub = A_pos.T @ Yb                        # (rL, rD)\n",
    "\n",
    "            # 3) Flatten and map to latent d\n",
    "            Ub_flat = Ub.reshape(-1)                 # (rL * rD,)\n",
    "            z_b = Ub_flat @ self.H_c                 # (d,)\n",
    "\n",
    "            # 4) Normalise (optional; you can drop this if you want magnitude to carry info)\n",
    "            #z_b = z_b / (z_b.norm() + eps)\n",
    "            #normalise after function because need to combine p and hla first\n",
    "\n",
    "            z_list.append(z_b)\n",
    "\n",
    "        z = torch.stack(z_list, dim=0)               # (B, d)\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bb7b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape\n",
    "B, L_T_pad, D = emb_T.shape\n",
    "\n",
    "# Latent ranks and final dimension (hyperparameters)\n",
    "rL = 8      # positional rank for TCR (tunable)\n",
    "rD = 16     # channel rank for TCR (tunable)\n",
    "d    = 128    # final latent dimension (same d as in Z*)\n",
    "\n",
    "# ratio of peptide to HLA (hyperparameter)\n",
    "#R = 0.7\n",
    "# Epsilon for numerical stability\n",
    "eps=1e-8\n",
    "\n",
    "# maximum true lenghts of the sequences\n",
    "# maximum true tcr length \n",
    "L_T_true = tcr_mask.sum(dim=1)\n",
    "L_T_max = L_T_true.max()\n",
    "# maximum true peptide length \n",
    "L_P_true = pep_mask.sum(dim=1)\n",
    "L_P_max = L_P_true.max()\n",
    "# maximum true HLA length \n",
    "L_H_true = hla_mask.sum(dim=1)\n",
    "L_H_max = L_H_true.max()\n",
    "\n",
    "\n",
    "tcr_encoder_new = ESMFactorisedEncoder(D, rL, rD, d, L_max=L_T_max).to(device)\n",
    "pep_encoder_new = ESMFactorisedEncoder(D, rL, rD, d, L_max=L_P_max).to(device)\n",
    "hla_encoder_new = ESMFactorisedEncoder(D, rL, rD, d, L_max=L_H_max).to(device)\n",
    "\n",
    "# when you call module(), you are calling the forward method (PyTorch convention)\n",
    "zT = tcr_encoder_new(emb_T, tcr_mask)\n",
    "zP = pep_encoder_new(emb_P, pep_mask)\n",
    "zH = hla_encoder_new(emb_H, hla_mask)\n",
    "\n",
    "# normalise T as we are not gating this\n",
    "zT = zT / (zT.norm(dim=-1, keepdim=True) + eps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e58c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to what Barbara suggested in meeting - scale within the projection learning?\n",
    "\n",
    "R_PH = 0.7  # peptide gets more weight\n",
    "\n",
    "gP = (R_PH ** 0.5)          # scalar\n",
    "gH = ((1.0 - R_PH) ** 0.5)  # scalar\n",
    "\n",
    "gP_t = torch.tensor(gP, device=device)\n",
    "gH_t = torch.tensor(gH, device=device)\n",
    "\n",
    "# zP, zH: (B, d)\n",
    "zPH = gP_t * zP + gH_t * zH      # (B, d)\n",
    "# Optionally normalise:\n",
    "zPH = zPH / (zPH.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "# concatenate into e_hat\n",
    "e_hat = torch.cat([zT, zPH], dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "584682df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next step to apply non linear layers (potentially ReLU as middle layers, as long as the end is linear transformation and can learn negative values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9e7eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf91b7",
   "metadata": {},
   "source": [
    "##### Get Boltz Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca6b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get Boltz embeddings\n",
    "# file_path = '/home/natasha/multimodal_model/outputs/boltz_runs/positives/pair_000/boltz_results_pair_000/predictions/pair_000/embeddings_pair_000.npz'\n",
    "# manifest_path = '/home/natasha/multimodal_model/data/manifests/boltz_100_manifest.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c488fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/natasha/multimodal_model/scripts/train\n",
      "/home/natasha/multimodal_model/data/manifests/boltz_100_manifest.csv\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "\n",
    "home = '/home/natasha/multimodal_model'\n",
    "manifest_path = os.path.join(home, 'data', 'manifests', 'boltz_100_manifest.csv')\n",
    "# positive_manifest_path = os.path.join(home, 'data', 'manifests', 'boltz_100_manifest.csv')\n",
    "# negative_manifest_path = os.path.join(home, 'data', 'negative_manifests', 'boltz_100_manifest.csv')\n",
    "#print(manifest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a64ea0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch z shapes: [torch.Size([604, 604, 128]), torch.Size([604, 604, 128]), torch.Size([604, 604, 128]), torch.Size([604, 604, 128]), torch.Size([604, 604, 128]), torch.Size([604, 604, 128]), torch.Size([604, 604, 128]), torch.Size([604, 604, 128])]\n",
      "Pep lengths: [ 9  9 10  9  9 10  9 10]\n"
     ]
    }
   ],
   "source": [
    "class BoltzDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for loading Boltz z-embeddings one by one,\n",
    "    with chain lengths from the manifest.\n",
    "    Each pair has its own .npz file.\n",
    "    ORIGINAL VERSION - returns numpy arrays\n",
    "    \"\"\"\n",
    "    def __init__(self, manifest_path, base_path):\n",
    "        self.manifest = pd.read_csv(manifest_path)\n",
    "        self.base_path = base_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.manifest)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        yaml_rel_path = self.manifest.iloc[idx]['yaml_path']\n",
    "        pair_id = os.path.splitext(os.path.basename(yaml_rel_path))[0]\n",
    "        emb_path = os.path.join(\n",
    "            self.base_path,\n",
    "            'outputs',\n",
    "            'boltz_runs',\n",
    "            'positives',\n",
    "            pair_id,\n",
    "            f'boltz_results_{pair_id}',\n",
    "            'predictions',\n",
    "            pair_id,\n",
    "            f'embeddings_{pair_id}.npz'\n",
    "        )\n",
    "        with np.load(emb_path) as arr:\n",
    "            z = arr['z']  # Returns numpy array as-is\n",
    "        pep_len = self.manifest.iloc[idx]['pep_len']\n",
    "        tcra_len = self.manifest.iloc[idx]['tcra_len']\n",
    "        tcrb_len = self.manifest.iloc[idx]['tcrb_len']\n",
    "        hla_len = self.manifest.iloc[idx]['hla_len']\n",
    "        return z, pep_len, tcra_len, tcrb_len, hla_len\n",
    "\n",
    "\n",
    "def boltz_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Uses numpy for padding, then converts to torch at the end\n",
    "    \"\"\"\n",
    "    zs, pep_lens, tcra_lens, tcrb_lens, hla_lens = zip(*batch)\n",
    "    # Each z has shape [sum_of_lengths, sum_of_lengths, dim] or [1, sum_of_lengths, sum_of_lengths, dim]\n",
    "    # Pad zs to max shape with zeros and stack into tensor\n",
    "    zs = [np.squeeze(z, axis=0) for z in zs]\n",
    "    max_len = max(z.shape[0] for z in zs)\n",
    "    # get channel dimension number from first z\n",
    "    dim = zs[0].shape[-1]\n",
    "    padded_zs = np.zeros((len(zs), max_len, max_len, dim), dtype=zs[0].dtype)\n",
    "    for i, z in enumerate(zs):\n",
    "        l = z.shape[0]\n",
    "        padded_zs[i, :l, :l, :] = z\n",
    "    zs = torch.from_numpy(padded_zs).float()  # or .to(device)\n",
    "    \n",
    "    return {\n",
    "        \"z\": zs,  # batch of z arrays, each possibly of different shape\n",
    "        \"pep_len\": np.array(pep_lens),\n",
    "        \"tcra_len\": np.array(tcra_lens),\n",
    "        \"tcrb_len\": np.array(tcrb_lens),\n",
    "        \"hla_len\": np.array(hla_lens),\n",
    "    }\n",
    "\n",
    "# Example usage with original version:\n",
    "dataset_original = BoltzDataset(manifest_path, home)\n",
    "dataloader_original = DataLoader(\n",
    "    dataset_original, batch_size=8, shuffle=True, collate_fn=boltz_collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "batch = next(iter(dataloader_original))\n",
    "print(\"Batch z shapes:\", [z.shape for z in batch[\"z\"]])\n",
    "print(\"Pep lengths:\", batch[\"pep_len\"])\n",
    "\n",
    "# 5.6s to load 100 pairs\n",
    "# 56 s to load 1000 pairs\n",
    "# 560 s to load 10000 pairs (9 mins)\n",
    "# 5600 s to load 100000 pairs (16 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9717bdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230 10 365 [112 114 112 110 113 112 113 112] [116 116 113 114 115 114 115 117] [ 9  9 10  9  9 10  9 10] [362 365 365 365 365 365 365 365]\n"
     ]
    }
   ],
   "source": [
    "L_T = batch[\"tcra_len\"] + batch[\"tcrb_len\"]\n",
    "L_P = batch[\"pep_len\"]\n",
    "L_H = batch[\"hla_len\"]\n",
    "\n",
    "L_T_max = max(L_T)\n",
    "L_P_max = max(L_P)\n",
    "L_H_max = max(L_H)\n",
    "\n",
    "print(L_T_max, L_P_max, L_H_max, batch['tcra_len'], batch['tcrb_len'], batch['pep_len'], batch['hla_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f9e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorisation Z\n",
    "\n",
    "for i in range(len(batch[\"z\"])):\n",
    "    L_T_max = batch[\"tcra_len\"][i]\n",
    "    L_P_max = batch[\"pep_len\"][i]\n",
    "    L_H_max = batch[\"hla_len\"][i]\n",
    "\n",
    "\n",
    "\n",
    "dB  = 128 # dimension of Boltz embeddings\n",
    "rB  = 16 # Boltz channel rank\n",
    "rT  = 8 # TCR positional rank\n",
    "rPH = 8 # pMHC positional rank\n",
    "\n",
    "B_Z = torch.nn.Parameter(torch.empty(dB, rB))\n",
    "nn.init.xavier_uniform_(B_Z)\n",
    "\n",
    "\n",
    "L_T_max = int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ccef78",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mBoltzFactorised\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Factorised Boltz embeddings for projection into latent shared space before NC loss\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    - Zstar_batch: (B, 2d, 2d) operator acting on e_hat_t, e_hat_pmc in R^2d\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dB, rB, rT, rPH, d, L_max, L_PH_max):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class BoltzFactorised(nn.Module):\n",
    "    \"\"\"\n",
    "    Factorised Boltz embeddings for projection into latent shared space before NC loss\n",
    "\n",
    "    Inputs:\n",
    "    - z_boltz: (B, L_pad, L_pad, d_boltz) full Boltz z for the batch\n",
    "    - L_alpha, L_beta, L_p, L_h: (B,) lengths of the TCR alpha, TCR beta, peptide, HLA\n",
    "    - gP, gH: (B,) scalar or (B,) peptide/HLA gates in [0,1], norm-preserving in quadrature???? As in, gP**2 + gH**2 = 1\n",
    "\n",
    "    Outputs:\n",
    "    - Zstar_batch: (B, 2d, 2d) operator acting on e_hat_t, e_hat_pmc in R^2d\n",
    "    \"\"\"\n",
    "    def __init__(self, dB, rB, rT, rPH, d, L_max, L_PH_max):\n",
    "        \"\"\"\n",
    "        dB      : channel dimension of Boltz embeddings\n",
    "        rB      : rank of Boltz channel factorisation\n",
    "        rT      : rank of TCR positional encoding\n",
    "        rPH     : rank of pMHC positional encoding\n",
    "        d       : latent dimension of shared space\n",
    "        L_T_max   : maximum length of any sequence in the batch\n",
    "        L_PH_max: maximum length of any pMHC sequence in the batch\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dB     = dB\n",
    "        self.rB     = rB\n",
    "        self.rT     = rT\n",
    "        self.rPH    = rPH\n",
    "        self.d      = d\n",
    "        self.L_T_max  = L_max\n",
    "        self.L_PH_max = L_PH_max\n",
    "\n",
    "        # ---- 1) Channel mixing: dB -> rB ---- \n",
    "        self.B_Z = torch.nn.Parameter(torch.empty(dB, rB))\n",
    "        nn.init.xavier_uniform_(self.B_Z)\n",
    "\n",
    "        # ---- 2)a) TCR positional encoding: rT -> rT ---- \n",
    "        self.A_T = torch.nn.Parameter(torch.empty(L_T_max, rT))\n",
    "        nn.init.xavier_uniform_(self.A_T)\n",
    "\n",
    "        # ---- 2)b) pMHC positional encoding: rPH -> rPH ---- \n",
    "        self.A_PH = torch.nn.Parameter(torch.empty(L_PH_max, rPH))\n",
    "        nn.init.xavier_uniform_(self.A_PH)\n",
    "\n",
    "        # ---- 3) Learnable maps from factorised z (r* x r* x rB) -> d x d ---- \n",
    "        # flatten sizes for each block\n",
    "        n_TT   = rT  * rT  * rB\n",
    "        n_TPH  = rT  * rPH * rB\n",
    "        n_PHT  = rPH * rT  * rB\n",
    "        n_PHPH = rPH * rPH * rB\n",
    "        dd     = d * d\n",
    "\n",
    "        self.H_TT   = nn.Parameter(torch.empty(n_TT,   dd))\n",
    "        self.H_TPH  = nn.Parameter(torch.empty(n_TPH,  dd))\n",
    "        self.H_PHT  = nn.Parameter(torch.empty(n_PHT,  dd))\n",
    "        self.H_PHPH = nn.Parameter(torch.empty(n_PHPH, dd))\n",
    "\n",
    "        nn.init.xavier_uniform_(self.H_TT)\n",
    "        nn.init.xavier_uniform_(self.H_TPH)\n",
    "        nn.init.xavier_uniform_(self.H_PHT)\n",
    "        nn.init.xavier_uniform_(self.H_PHPH)\n",
    "\n",
    "        # ---- 4) Final linear layer: d -> d ---- \n",
    "        self.W_out = nn.Parameter(torch.empty(d, d))\n",
    "        nn.init.xavier_uniform_(self.W_out)\n",
    "    \n",
    "    def _get_gate_scalar(self, g, b):\n",
    "        \"\"\"\n",
    "        Helper: allow g to be a scalar tensor () or per-sample tensor (B,).\n",
    "        Returns a Python float for sample b.\n",
    "        \"\"\"\n",
    "        if g.dim() == 0:\n",
    "            return float(g.item())\n",
    "        else:\n",
    "            return float(g[b].item())\n",
    "\n",
    "    def forward(self, z_boltz, L_alpha, L_beta, L_p, L_h, gP, gH):\n",
    "        \"\"\"\n",
    "        Z_boltz : (B, L_pad, L_pad, dB)\n",
    "        L_alpha : (B,) true alpha lengths\n",
    "        L_beta  : (B,) true beta lengths\n",
    "        L_p     : (B,) true peptide lengths\n",
    "        L_h     : (B,) true HLA lengths\n",
    "        gP      : scalar () or (B,) peptide gate (already sqrt(R_PH))\n",
    "        gH      : scalar () or (B,) HLA gate (already sqrt(1-R_PH))\n",
    "\n",
    "        Returns:\n",
    "          Zstar_batch: (B, 2d, 2d)\n",
    "        \"\"\"\n",
    "\n",
    "        device = z_boltz.device\n",
    "        B, L_pad, _, dB = z_boltz.shape\n",
    "        assert dB == self.dB\n",
    "\n",
    "        Zstar_list = []\n",
    "\n",
    "        for b in range(B):\n",
    "            La  = int(L_alpha[b].item())\n",
    "            Lb  = int(L_beta[b].item())\n",
    "            Lp_ = int(L_p[b].item())\n",
    "            Lh_ = int(L_h[b].item())\n",
    "\n",
    "            L_T     = La + Lb\n",
    "            L_PH    = Lp_ + Lh_\n",
    "            L       = L_T + L_PH\n",
    "\n",
    "            # if we have missing z it just returns identity\n",
    "            if L == 0:\n",
    "                I_2d = torch.eye(2* self.d, device=device)\n",
    "                Zstar_list.append(I_2d)\n",
    "                continue\n",
    "            \n",
    "\n",
    "            # restrict to true tokens for the sample\n",
    "            Z = z_boltz[b, :L, :L, :] # (L, L, dB)\n",
    "\n",
    "            # ---- 2) Per channel normalisation (right now omit this step) ----\n",
    "            # another potential here is to use Adam optimiser to learn the normalisation\n",
    "            # mu = Z.mean(dim=(0,1), keepdim=True) # (1, 1, dB)\n",
    "            # std = Z.std(dim=(0,1), keepdim=True) # (1, 1, dB)\n",
    "            # Zc = (Z - mu) / std            # (L, L, dB)\n",
    "            # Zc = Zc / (math.sqrt(L) + eps) # scale by sqrt(L)\n",
    "            Zc = Z.clone()\n",
    "\n",
    "            # ----- 3) Gating ----\n",
    "            # TCR gates\n",
    "            if La > 0 and Lb > 0:\n",
    "                gA_b = 2**-0.5\n",
    "                gB_b = 2**-0.5\n",
    "            elif La > 0 and Lb == 0:\n",
    "                gA_b = 1\n",
    "                gB_b = 0\n",
    "            elif La == 0 and Lb > 0:\n",
    "                gA_b = 0\n",
    "                gB_b = 1\n",
    "            else:\n",
    "                gA_b = 0\n",
    "                gB_b = 0\n",
    "\n",
    "            # Peptide/HLA gates (from R set in encoder part)\n",
    "            gP_b = self._get_gate_scalar(gP, b)\n",
    "            gH_b = self._get_gate_scalar(gH, b)\n",
    "\n",
    "            # ---- 4) Build token-level gate vector over [alpha | beta | p | h] ----\n",
    "            gate = torch.zeros(L, device=device) # (L,)\n",
    "\n",
    "            idx0 = 0\n",
    "            idx1 = idx0 + La\n",
    "            idx2 = idx1 + Lb\n",
    "            idx3 = idx2 + Lp_\n",
    "            idx4 = idx3 + Lh_\n",
    "\n",
    "            if La  > 0: gate[idx0:idx1] = gA_b\n",
    "            if Lb  > 0: gate[idx1:idx2] = gB_b\n",
    "            if Lp_ > 0: gate[idx2:idx3] = gP_b\n",
    "            if Lh_ > 0: gate[idx3:idx4] = gH_b\n",
    "\n",
    "            gate_row = gate.view(L, 1, 1) # (L, 1, 1)\n",
    "            gate_col = gate.view(1, L, 1) # (1, L, 1)\n",
    "\n",
    "            Zg = Zc * gate_row * gate_col # (L, L, dB)\n",
    "\n",
    "            # ---- 5) Get TCR/pMHC blocks ----\n",
    "            sT = slice(0, L_T)           # [0, L_T) -> TCR (alpha + beta)\n",
    "            sPH = slice(L_T, L_T + L_PH) # [L_T, L] -> pMHC (P+H)\n",
    "\n",
    "            Z_TT  = Zg[sT, sT, :]    # (L_T, L_T, dB)\n",
    "            Z_TPH = Zg[sT, sPH, :]   # (L_T, L_PH, dB)\n",
    "            Z_PHT = Zg[sPH, sT, :]   # (L_PH, L_T, dB)\n",
    "            Z_PHPH = Zg[sPH, sPH, :] # (L_PH, L_PH, dB)\n",
    "\n",
    "            # ---- 6) channel/dimension compression ----\n",
    "            B_Z = self.B_Z # (dB, rB) operator across channels\n",
    "            Y_TT   = torch.einsum('ijc,cr->ijr', Z_TT,   B_Z)   # (L_T,  L_T,  rB)\n",
    "            Y_TPH  = torch.einsum('ijc,cr->ijr', Z_TPH,  B_Z)   # (L_T,  L_PH, rB)\n",
    "            Y_PHT  = torch.einsum('ijc,cr->ijr', Z_PHT,  B_Z)   # (L_PH, L_T,  rB)\n",
    "            Y_PHPH = torch.einsum('ijc,cr->ijr', Z_PHPH, B_Z)   # (L_PH, L_PH, rB)\n",
    "\n",
    "            # ---- 7) TCR positional compression with A_T / A_PH ----\n",
    "            # Sample-specific rows for the correct lengths for the per-sample positional tensors\n",
    "            if L_T > 0:\n",
    "                A_T_b = self.A_T[:L_T, :] # (L_T, rT)\n",
    "            else: \n",
    "                # no TCRs, so treat as zero contribution (or could be noise, decide later)\n",
    "                A_T_b = self.A_T[:1, :] * 0.0 # (1, rT) dummy\n",
    "\n",
    "            if L_PH > 0:\n",
    "                A_PH_b = self.A_PH[:L_PH, :] # (L_PH, rPH)\n",
    "            else:\n",
    "                # no pMHCs, so treat as zero contribution (or could be noise, decide later)\n",
    "                A_PH_b = self.A_PH[:1, :] * 0.0 # (1, rPH) dummy\n",
    "            \n",
    "            rT  = self.rT\n",
    "            rPH = self.rPH\n",
    "            rB  = self.rB\n",
    "            d   = self.d\n",
    "\n",
    "            # N.B. U tensors are not learned, they are discarded as intermediary steps for compression\n",
    "            # TCR-TCR (L_T, L_T, rB) -> (rT, rT, rB)\n",
    "            if L_T > 0:\n",
    "                U_TT = torch.einsum('ip,ijr->pjr', A_T_b, Y_TT) # (rT, L_T, rB)\n",
    "                V_TT = torch.einsum('pjr,jq->pqr', U_TT, A_T_b) # (rT, rT, rB)\n",
    "            else:\n",
    "                V_TT = torch.zeros(rT, rT, rB, device=device) # (rT, rT, rB)\n",
    "\n",
    "            # TCR–pMHC: (L_T, L_PH, rB) -> (rT, rPH, rB)\n",
    "            if L_T > 0 and L_PH > 0:\n",
    "                U_TPH = torch.einsum('ip,ijr->pjr', A_T_b,  Y_TPH)   # (rT,  L_PH, rB)\n",
    "                V_TPH = torch.einsum('pjr,jq->pqr', U_TPH, A_PH_b)   # (rT,  rPH, rB)\n",
    "            else:\n",
    "                V_TPH = torch.zeros(rT, rPH, rB, device=device)\n",
    "\n",
    "            # pMHC–TCR: (L_PH, L_T, rB) -> (rPH, rT, rB)\n",
    "            if L_T > 0 and L_PH > 0:\n",
    "                U_PHT = torch.einsum('ip,ijr->pjr', A_PH_b, Y_PHT)   # (rPH, L_T,  rB)\n",
    "                V_PHT = torch.einsum('pjr,jq->pqr', U_PHT, A_T_b)    # (rPH, rT,  rB)\n",
    "            else:\n",
    "                V_PHT = torch.zeros(rPH, rT, rB, device=device)\n",
    "\n",
    "            # pMHC–pMHC: (L_PH, L_PH, rB) -> (rPH, rPH, rB)\n",
    "            if L_PH > 0:\n",
    "                U_PHPH = torch.einsum('ip,ijr->pjr', A_PH_b, Y_PHPH) # (rPH, L_PH, rB)\n",
    "                V_PHPH = torch.einsum('pjr,jq->pqr', U_PHPH, A_PH_b) # (rPH, rPH, rB)\n",
    "            else:\n",
    "                V_PHPH = torch.zeros(rPH, rPH, rB, device=device)\n",
    "\n",
    "            # ---- 8) Flatten factorised blocks and map to d×d via H_* ----\n",
    "            v_TT_flat   = V_TT.reshape(-1)    # (rT*rT*rB,)\n",
    "            v_TPH_flat  = V_TPH.reshape(-1)   # (rT*rPH*rB,)\n",
    "            v_PHT_flat  = V_PHT.reshape(-1)   # (rPH*rT*rB,)\n",
    "            v_PHPH_flat = V_PHPH.reshape(-1)  # (rPH*rPH*rB,)\n",
    "\n",
    "            k_TT_flat   = v_TT_flat   @ self.H_TT   # (d*d,)\n",
    "            k_TPH_flat  = v_TPH_flat  @ self.H_TPH  # (d*d,)\n",
    "            k_PHT_flat  = v_PHT_flat  @ self.H_PHT  # (d*d,)\n",
    "            k_PHPH_flat = v_PHPH_flat @ self.H_PHPH # (d*d,)\n",
    "\n",
    "            K_TT   = k_TT_flat.view(d, d)\n",
    "            K_TPH  = k_TPH_flat.view(d, d)\n",
    "            K_PHT  = k_PHT_flat.view(d, d)\n",
    "            K_PHPH = k_PHPH_flat.view(d, d)\n",
    "\n",
    "            # Optional: enforce symmetry on diagonal blocks\n",
    "            # Enforce symmetry on all blocks?\n",
    "            K_TT   = 0.5 * (K_TT   + K_TT.t())\n",
    "            K_PHPH = 0.5 * (K_PHPH + K_PHPH.t())\n",
    "\n",
    "            # ---- 9) Assemble 2d x 2d operator for this sample ----\n",
    "            I_d = torch.eye(d, device=device)\n",
    "            Zstar_b = torch.zeros(2*d, 2*d, device=device)\n",
    "\n",
    "            Zstar_b[:d,  :d]  = I_d + K_TT\n",
    "            Zstar_b[:d,  d:]  = I_d + K_TPH\n",
    "            Zstar_b[d:,  :d]  = I_d + K_PHT\n",
    "            Zstar_b[d:,  d:]  = I_d + K_PHPH\n",
    "\n",
    "            Zstar_list.append(Zstar_b)\n",
    "\n",
    "        Zstar_batch = torch.stack(Zstar_list, dim=0)  # (B, 2d, 2d)\n",
    "        return Zstar_batch \n",
    "\n",
    "\n",
    "# index convention for the einsum\n",
    "# i = row token position, “i” is historically “index” or “first axis”\n",
    "# j\t= column token position, second positional axis (matrix-like)\n",
    "# c\t= channel\n",
    "# p, q\t= latent positional modes, P = projection / latent position\n",
    "# r = latent channel modes, R = rank / channel rank\n",
    "# b\t= batch index, B = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d7b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting it all together\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcr-multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
