{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e2b02a3",
   "metadata": {},
   "source": [
    "##### This script defines the encoders for the multimodal model\n",
    "##### Overview:\n",
    "- The inputs are the ESMC and Boltz embeddings \n",
    "- 3 encoders from PEFT of ESMC\n",
    "- Two projection heads: one for ESM embeddings and one for Boltz embeddings\n",
    "- Non contrastive learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91aae1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/natasha/miniconda3/envs/tcr-multimodal/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "# Streamlined imports - removing duplicates\n",
    "import time\n",
    "import esm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "import copy\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import subprocess\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, update_display\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ESM imports\n",
    "from esm.models.esmc import ESMC\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig, ESMProteinTensor\n",
    "from esm.models.esmc import _BatchedESMProteinTensor\n",
    "\n",
    "# Tokenizer imports\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import decoders\n",
    "\n",
    "# PEFT imports\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from peft.tuners.lora import LoraConfig, LoraModel\n",
    "\n",
    "# Set environment variable\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31bb77ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/natasha/multimodal_model/scripts/train\n",
      "Project root: /home/natasha/multimodal_model\n",
      "Models directory at: /home/natasha/multimodal_model/models\n",
      "Checkpoints directory at: /home/natasha/multimodal_model/models/checkpoints\n"
     ]
    }
   ],
   "source": [
    "# Get current working directory and create models folder\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "# Get the project root (go up one level from scripts/train)\n",
    "project_root = Path(current_dir).parent.parent\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Create models directory in project root\n",
    "models_dir = project_root / \"models\"\n",
    "if not models_dir.exists():\n",
    "    print(f\"Models directory does not exist, creating it at: {models_dir}\")\n",
    "    models_dir.mkdir(exist_ok=True)\n",
    "print(f\"Models directory at: {models_dir}\")\n",
    "\n",
    "# Also create a checkpoints subdirectory for saving model checkpoints\n",
    "checkpoints_dir = models_dir / \"checkpoints\"\n",
    "if not checkpoints_dir.exists():\n",
    "    print(f\"Checkpoints directory does not exist, creating it at: {checkpoints_dir}\")\n",
    "    checkpoints_dir.mkdir(exist_ok=True)\n",
    "print(f\"Checkpoints directory at: {checkpoints_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07943542",
   "metadata": {},
   "source": [
    "##### Get ESM Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d270e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to load ESM C with LM head enabled \n",
    "# expose final token embeddings before the logits head (is logits head the LM head, LM head=language modelling head)?\n",
    "# collator returns: input_ids, attention_mask\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# load model and allow lora (rather than eval mode?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17ffa245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size = [1,7,960], always the case?\n",
    "# size = [1, 12, 960]\n",
    "# size is I think batch number, sequence length, embedding dimension\n",
    "\n",
    "df = pd.read_csv('/home/natasha/multimodal_model/data/raw/HLA/boltz_100_runs.csv')\n",
    "# Fill empty/nan values with <unk> token\n",
    "df['TCRa'] = df['TCRa'].fillna('X')\n",
    "df['TCRb'] = df['TCRb'].fillna('X')\n",
    "\n",
    "# Replace empty strings with <unk>\n",
    "df.loc[df['TCRa'] == '', 'TCRa'] = 'X'\n",
    "df.loc[df['TCRb'] == '', 'TCRb'] = 'X'\n",
    "\n",
    "df['TCR_full'] = df['TCRa'] + df['TCRb']\n",
    "df['m_alpha'] = 1\n",
    "df['m_beta'] = 1\n",
    "df.loc[df['TCRa'] == 'X', 'm_alpha'] = 0\n",
    "df.loc[df['TCRb'] == 'X', 'm_beta'] = 0\n",
    "#df.to_csv('/home/natasha/multimodal_model/data/raw/HLA/boltz_100_runs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9631825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read full train dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ca97902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hugging face dataset?\n",
    "class TCR_dataset(Dataset):\n",
    "    \"\"\"Dataset for TCR data, for use in encoder training to propagate through to NC model\"\"\"\n",
    "    def __init__(self, data_path, column_name='TCR_full', include_label=False):\n",
    "        self.data_path = data_path\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.column_name = column_name  # Store column name here\n",
    "        self.include_label = include_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):  # Remove column_name parameter\n",
    "        row = self.data.iloc[idx]  # Fix: self.data, not self.csv\n",
    "        protein = row[self.column_name]  # Use stored column name\n",
    "        protein_idx = f'TCR_{idx}'\n",
    "        if self.include_label:\n",
    "            return protein_idx, protein, row.get('Binding', -1)\n",
    "        #return protein_idx, protein\n",
    "        return protein\n",
    "\n",
    "class peptide_dataset(Dataset):\n",
    "    def __init__(self, data_path, column_name='Peptide', include_label=False):\n",
    "        self.data_path = data_path\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.column_name = column_name\n",
    "        self.include_label = include_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        peptide = row[self.column_name]\n",
    "        peptide_idx = f'peptide_{idx}'\n",
    "        if self.include_label:\n",
    "            return peptide_idx, peptide, row.get('Binding', -1)\n",
    "        #return peptide_idx, peptide\n",
    "        return peptide\n",
    "\n",
    "class HLA_dataset(Dataset):\n",
    "    def __init__(self, data_path, column_name='HLA', include_label=False):\n",
    "        self.data_path = data_path\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.column_name = column_name\n",
    "        self.include_label = include_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        hla = row[self.column_name]\n",
    "        hla_idx = f'hla_{idx}'\n",
    "        if self.include_label:\n",
    "            return hla_idx, hla, row.get('Binding', -1)\n",
    "        #return hla_idx, hla\n",
    "        return hla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b671bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcr = TCR_dataset(data_path='/home/natasha/multimodal_model/data/raw/HLA/boltz_100_runs.csv', column_name='TCR_full')\n",
    "peptide = peptide_dataset(data_path='/home/natasha/multimodal_model/data/raw/HLA/boltz_100_runs.csv', column_name='Peptide')\n",
    "hla = HLA_dataset(data_path='/home/natasha/multimodal_model/data/raw/HLA/boltz_100_runs.csv', column_name='HLA_sequence')\n",
    "\n",
    "# not sure I reallt need these classes??? Hmmmmmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94e53763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a78f9c74a243aa93712de22ccc8a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tcrs = [ESMProtein(sequence=s) for s in tcr.data['TCR_full']]\n",
    "peptides = [ESMProtein(sequence=s) for s in peptide.data['Peptide']]\n",
    "hlas = [ESMProtein(sequence=s) for s in hla.data['HLA_sequence']]\n",
    "\n",
    "# can batch at the forward step, not the encoding step\n",
    "\n",
    "#model = ESMC.from_pretrained(\"esmc_300m\").to(device).eval()\n",
    "model = ESMC.from_pretrained(\"esmc_300m\").eval()\n",
    "\n",
    "tcrs_data = [seq for seq in tcr]\n",
    "peptides_data = [seq for seq in peptide]\n",
    "hlas_data = [seq for seq in hla]\n",
    "\n",
    "encoded_tcrs = [model.encode(p) for p in tcrs]\n",
    "encoded_peptides = [model.encode(p) for p in peptides]\n",
    "encoded_hlas = [model.encode(p) for p in hlas]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b45d13e",
   "metadata": {},
   "source": [
    "##### Mask Data and Collate for MLM for Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eaf8da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do for entire dataset\n",
    "# do we also want to output attention_mask from the tokenizer?\n",
    "\n",
    "tok = model.tokenizer\n",
    "CLS_ID = tok.cls_token_id\n",
    "EOS_ID = tok.eos_token_id\n",
    "PAD_ID = tok.pad_token_id\n",
    "MASK_ID = tok.mask_token_id\n",
    "\n",
    "AA_IDS =  [5,10,17,13,23,16,9,6,21,12,4,15,20,18,14,8,11,22,19,7]\n",
    "\n",
    "\n",
    "class EncodedSeqDataset(Dataset):\n",
    "    def __init__(self, sequences, enc):     # ← now takes two arguments\n",
    "        self.sequences = sequences          # list[str]\n",
    "        self.input_ids = enc['input_ids']\n",
    "        self.attention_mask = enc['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"sequence\": self.sequences[idx],  # raw sequence string\n",
    "            \"input_ids\": torch.as_tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            \"attention_mask\": torch.as_tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0865bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMProteinCollator:\n",
    "    def __init__(self, *, cls_id, eos_id, pad_id, mask_id, amino_acids,\n",
    "                 p=0.15, min_per_seq=2, max_per_seq=45, aa_frac=0.20):\n",
    "        self.CLS = cls_id\n",
    "        self.EOS = eos_id\n",
    "        self.PAD = pad_id\n",
    "        self.MASK = mask_id\n",
    "        self.aa = torch.as_tensor(amino_acids, dtype=torch.long)\n",
    "        self.p = p\n",
    "        self.min_per_seq = min_per_seq\n",
    "        self.max_per_seq = max_per_seq\n",
    "        self.aa_frac = aa_frac\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def mask_batch(self, input_ids, attention_mask):\n",
    "        device = input_ids.device\n",
    "        aa = self.aa.to(device)\n",
    "\n",
    "        B, L = input_ids.shape\n",
    "        valid_mask = attention_mask.bool() \\\n",
    "                   & (input_ids != self.PAD) \\\n",
    "                   & (input_ids != self.CLS) \\\n",
    "                   & (input_ids != self.EOS)\n",
    "\n",
    "        masked_input_ids = input_ids.clone()\n",
    "        labels = torch.full_like(input_ids, -100)\n",
    "\n",
    "        for i in range(B):\n",
    "            vmask = valid_mask[i]\n",
    "            if not vmask.any():\n",
    "                continue\n",
    "\n",
    "            valid_idx = vmask.nonzero(as_tuple=False).squeeze(1)  # (L_valid,)\n",
    "            L_valid = valid_idx.numel()\n",
    "\n",
    "            # how many to mask: floor(p*L_valid), clamped to [2, 45] but never > L_valid\n",
    "            n = torch.floor(self.p * torch.tensor(L_valid, device=device, dtype=torch.float32)).to(torch.int64)\n",
    "            n = torch.clamp(n, min=self.min_per_seq, max=min(self.max_per_seq, L_valid))\n",
    "            if n.item() == 0:\n",
    "                continue\n",
    "\n",
    "            # choose n distinct valid positions\n",
    "            chosen = valid_idx[torch.randperm(L_valid, device=device)[:n]]\n",
    "\n",
    "            # split into AA vs MASK; ensure >=1 AA if n>=2\n",
    "            n_amino = torch.floor(self.aa_frac * n).to(torch.int64)\n",
    "            if n.item() >= 2:\n",
    "                n_amino = torch.clamp(n_amino, min=1)\n",
    "            n_mask = n - n_amino\n",
    "\n",
    "            order = torch.randperm(n.item(), device=device)\n",
    "            mask_pos  = chosen[order[:n_mask]]\n",
    "            amino_pos = chosen[order[n_mask:]]\n",
    "\n",
    "            # labels only at supervised positions\n",
    "            labels[i, chosen] = input_ids[i, chosen]\n",
    "\n",
    "            # apply edits\n",
    "            if n_mask.item() > 0:\n",
    "                masked_input_ids[i, mask_pos] = self.MASK\n",
    "            if n_amino.item() > 0:\n",
    "                r_idx = torch.randint(high=aa.numel(), size=(n_amino.item(),), device=device)\n",
    "                masked_input_ids[i, amino_pos] = aa[r_idx]\n",
    "\n",
    "        return masked_input_ids, labels\n",
    "\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_ids = torch.stack([f[\"input_ids\"] for f in features], dim=0)\n",
    "        attention_mask = torch.stack([f[\"attention_mask\"] for f in features], dim=0)\n",
    "        sequences = [f[\"sequence\"] for f in features]\n",
    "        proteins = [ESMProtein(sequence=f[\"sequence\"]) for f in features]\n",
    "        batched_clean = _BatchedESMProteinTensor(sequence=input_ids)\n",
    "\n",
    "\n",
    "        masked_input_ids, labels = self.mask_batch(input_ids, attention_mask)\n",
    "\n",
    "        # build masked sequences as strings (keep <mask>, drop CLS/EOS/PAD)\n",
    "        masked_sequences = []\n",
    "        for row in masked_input_ids.tolist():\n",
    "            toks = collator.tokenizer.convert_ids_to_tokens(row, skip_special_tokens=False)\n",
    "            aa = []\n",
    "            for t in toks:\n",
    "                if t in (collator.tokenizer.cls_token, collator.tokenizer.eos_token, collator.tokenizer.pad_token):\n",
    "                    continue\n",
    "                aa.append(t)  # AA tokens are single letters; keep \"<mask>\" as is\n",
    "            masked_sequences.append(\"\".join(aa))\n",
    "\n",
    "        proteins_masked = [ESMProtein(sequence=s) for s in masked_sequences]\n",
    "        batched_masked = _BatchedESMProteinTensor(sequence=masked_input_ids)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"masked_input_ids\": masked_input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"clean_input_ids\": input_ids.clone(),\n",
    "            \"clean_sequences\": sequences,                 # clean strings\n",
    "            \"masked_sequences\": masked_sequences,   # masked strings  ← NEW\n",
    "            \"clean_sequences_ESMprotein\": proteins,\n",
    "            \"masked_sequences_ESMprotein\": proteins_masked,\n",
    "            \"masked_input_ids_ESMprotein_batched\": batched_masked,\n",
    "            \"clean_input_ids_ESMprotein_batched\": batched_clean,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "146109ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your existing BatchEncodings:\n",
    "# clean_tcrs_tokenized, clean_peptides_tokenized, clean_hlas_tokenized\n",
    "clean_tcrs_tokenized = model.tokenizer(tcrs_data, return_tensors='pt', padding=True)\n",
    "clean_peptides_tokenized = model.tokenizer(peptides_data, return_tensors='pt', padding=True)\n",
    "clean_hlas_tokenized = model.tokenizer(hlas_data, return_tensors='pt', padding=True)\n",
    "\n",
    "tcr_ds = EncodedSeqDataset(tcrs_data,clean_tcrs_tokenized)\n",
    "pep_ds = EncodedSeqDataset(peptides_data, clean_peptides_tokenized)\n",
    "hla_ds = EncodedSeqDataset(hlas_data, clean_hlas_tokenized)\n",
    "\n",
    "collator = MLMProteinCollator(\n",
    "    cls_id=CLS_ID, eos_id=EOS_ID, pad_id=PAD_ID, mask_id=MASK_ID,\n",
    "    amino_acids=AA_IDS, p=0.15, min_per_seq=2, max_per_seq=45, aa_frac=0.20\n",
    ")\n",
    "collator.tokenizer = model.tokenizer\n",
    "\n",
    "\n",
    "tcr_loader = DataLoader(tcr_ds, batch_size=8, shuffle=True, num_workers=4, collate_fn=collator)\n",
    "pep_loader = DataLoader(pep_ds, batch_size=8, shuffle=True, num_workers=4, collate_fn=collator)\n",
    "hla_loader = DataLoader(hla_ds, batch_size=8, shuffle=True, num_workers=4, collate_fn=collator)\n",
    "\n",
    "\n",
    "# gives a batch dict from the collator with 4 keys\n",
    "# input_ids, labels (original tokens only at masked positions, -100 everywhere else)\n",
    "# attention_mask (0,1 for padding), clean_input_ids (clean copy of the input for clean forward pass using boltz for NC loss)\n",
    "\n",
    "# model.to(\"cpu\")\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "def optimizer_to_cpu(optim):\n",
    "    for st in optim.state.values():\n",
    "        for k, v in list(st.items()):\n",
    "            if torch.is_tensor(v):\n",
    "                st[k] = v.detach().to(\"cpu\")\n",
    "\n",
    "# move model to CPU and delete, as now we have the correct inputs \n",
    "model.to(\"cpu\")\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d9497b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_784697/627727058.py:33: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler  = GradScaler(enabled=False)                     # <-- no scaler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to tcr_encoder_checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "use_amp = torch.cuda.is_available()\n",
    "\n",
    "#base = ESMC.from_pretrained(\"esmc_300m\").to(device)\n",
    "#base = ESMC.from_pretrained(\"esmc_300m\")\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "    target_modules=[\"out_proj\", \"layernorm_qkv.1\"],  # inner Linear inside the Sequential\n",
    ")\n",
    "\n",
    "model_tcr = LoraModel(ESMC.from_pretrained(\"esmc_300m\"), lora_cfg, adapter_name=\"tcr\")\n",
    "\n",
    "\n",
    "#model_tcr = LoraModel(base, lora_cfg, adapter_name=\"tcr\")\n",
    "\n",
    "# freeze everything; unfreeze only LoRA params\n",
    "for p in model_tcr.parameters():\n",
    "    p.requires_grad = False\n",
    "for name, p in model_tcr.named_parameters():\n",
    "    if \"lora_A\" in name or \"lora_B\" in name:\n",
    "        p.requires_grad = True\n",
    "\n",
    "model_tcr.to(\"cuda\")\n",
    "model_tcr.train()\n",
    "\n",
    "optim_tcr = torch.optim.AdamW(\n",
    "    (p for p in model_tcr.parameters() if p.requires_grad),\n",
    "    lr=1e-3, weight_decay=0.01\n",
    ")\n",
    "\n",
    "\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler  = GradScaler(enabled=False)                     # <-- no scaler\n",
    "\n",
    "for batch in tcr_loader:\n",
    "    input_ids = batch[\"masked_input_ids\"].to(device, dtype=torch.long)\n",
    "    labels    = batch[\"labels\"].to(device)\n",
    "\n",
    "    with autocast(\"cuda\", enabled=use_amp, dtype=torch.bfloat16):  # <-- bf16\n",
    "        out    = model_tcr(input_ids)\n",
    "        logits = out.sequence_logits\n",
    "        #print(type(logits), logits.shape, logits.requires_grad)  # expect: Tensor, [B,L,V], True\n",
    "\n",
    "        loss   = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            labels.view(-1),\n",
    "            ignore_index=-100\n",
    "        )\n",
    "\n",
    "    loss.backward()                                     # <-- standard backward\n",
    "    optim_tcr.step(); optim_tcr.zero_grad(set_to_none=True)\n",
    "\n",
    "    del out, logits, loss, input_ids, labels, batch\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "# 1.1 seconds for 100\n",
    "# 11 for 1000\n",
    "# 110 for 10000\n",
    "# 350 for 35000 - 5.8 minutes to train one encoder on full dataset\n",
    "\n",
    "optimizer_to_cpu(optim_tcr)\n",
    "\n",
    "# save tcr model\n",
    "\n",
    "checkpoint_filename = 'tcr_encoder_checkpoint.pth'\n",
    "\n",
    "checkpoint_dict = {\n",
    "    #'epoch': num_epochs,\n",
    "    'tcr_model_state_dict': model_tcr.state_dict(),\n",
    "    #'pep_model_state_dict': peptide_model_nc.state_dict(), \n",
    "    'optimizer_state_dict': optim_tcr.state_dict(),\n",
    "    #'final_loss': avg_epoch_loss,\n",
    "    #'binding_threshold': threshold  # Add threshold to saved state\n",
    "}\n",
    "\n",
    "torch.save(checkpoint_dict, checkpoints_dir/checkpoint_filename)\n",
    "\n",
    "print(f\"Checkpoint saved to {checkpoint_filename}\")\n",
    "\n",
    "model_tcr.to(\"cpu\")\n",
    "\n",
    "del optim_tcr, model_tcr, checkpoint_dict, checkpoint_filename\n",
    "\n",
    "torch.cuda.empty_cache()  # Free up GPU memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bd0beba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_784697/363416641.py:32: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler  = GradScaler(enabled=False)                     # <-- no scaler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to peptide_encoder_checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "# peptide encoder\n",
    "\n",
    "use_amp = torch.cuda.is_available()\n",
    "\n",
    "#base = ESMC.from_pretrained(\"esmc_300m\").to(device)\n",
    "\n",
    "## use the same lora config for peptide and HLA - modify later\n",
    "# lora_cfg = LoraConfig(\n",
    "#     r=8, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "#     target_modules=[\"out_proj\", \"layernorm_qkv.1\"],  # inner Linear inside the Sequential\n",
    "# )\n",
    "\n",
    "model_pep = LoraModel(ESMC.from_pretrained(\"esmc_300m\"), lora_cfg, adapter_name=\"pep\")\n",
    "#model_pep = LoraModel(base, lora_cfg, adapter_name=\"peptide\")\n",
    "\n",
    "# freeze everything; unfreeze only LoRA params\n",
    "for p in model_pep.parameters():\n",
    "    p.requires_grad = False\n",
    "for name, p in model_pep.named_parameters():\n",
    "    if \"lora_A\" in name or \"lora_B\" in name:\n",
    "        p.requires_grad = True\n",
    "\n",
    "model_pep.to(\"cuda\")\n",
    "model_pep.train()\n",
    "\n",
    "optim_pep = torch.optim.AdamW(\n",
    "    (p for p in model_pep.parameters() if p.requires_grad),\n",
    "    lr=1e-3, weight_decay=0.01\n",
    ")\n",
    "\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler  = GradScaler(enabled=False)                     # <-- no scaler\n",
    "\n",
    "for batch in pep_loader:\n",
    "    input_ids = batch[\"masked_input_ids\"].to(device, dtype=torch.long)\n",
    "    labels    = batch[\"labels\"].to(device)\n",
    "\n",
    "    with autocast(\"cuda\", enabled=use_amp, dtype=torch.bfloat16):  # <-- bf16\n",
    "        out    = model_pep(input_ids)\n",
    "        logits = out.sequence_logits\n",
    "        #print(type(logits), logits.shape, logits.requires_grad)  # expect: Tensor, [B,L,V], True\n",
    "\n",
    "        loss   = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            labels.view(-1),\n",
    "            ignore_index=-100\n",
    "        )\n",
    "\n",
    "    loss.backward()                                     # <-- standard backward\n",
    "    optim_pep.step(); optim_pep.zero_grad(set_to_none=True)\n",
    "\n",
    "    del out, logits, loss, input_ids, labels, batch\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "optimizer_to_cpu(optim_pep)\n",
    "\n",
    "# save peptide model\n",
    "\n",
    "checkpoint_filename = 'peptide_encoder_checkpoint.pth'\n",
    "\n",
    "checkpoint_dict = {\n",
    "    #'epoch': num_epochs,\n",
    "    'peptide_model_state_dict': model_pep.state_dict(),\n",
    "    'optimizer_state_dict': optim_pep.state_dict(),\n",
    "    #'final_loss': avg_epoch_loss,\n",
    "    #'binding_threshold': threshold  # Add threshold to saved state\n",
    "}\n",
    "\n",
    "torch.save(checkpoint_dict, checkpoints_dir/checkpoint_filename)\n",
    "\n",
    "print(f\"Checkpoint saved to {checkpoint_filename}\")\n",
    "\n",
    "model_pep.to(\"cpu\")\n",
    "del optim_pep, model_pep, checkpoint_dict, checkpoint_filename\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "#print(torch.cuda.memory_summary())           # “Active” bytes should drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b319d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_784697/2523016605.py:33: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler  = GradScaler(enabled=False)                     # <-- no scaler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to hla_encoder_checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "# HLA encoder\n",
    "\n",
    "use_amp = torch.cuda.is_available()\n",
    "\n",
    "#base = ESMC.from_pretrained(\"esmc_300m\").to(device)\n",
    "\n",
    "## use the same lora config for peptide and HLA - modify later\n",
    "# lora_cfg = LoraConfig(\n",
    "#     r=8, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "#     target_modules=[\"out_proj\", \"layernorm_qkv.1\"],  # inner Linear inside the Sequential\n",
    "# )\n",
    "\n",
    "#model_hla = LoraModel(base, lora_cfg, adapter_name=\"hla\")\n",
    "model_hla = LoraModel(ESMC.from_pretrained(\"esmc_300m\"), lora_cfg, adapter_name=\"hla\")\n",
    "\n",
    "\n",
    "# freeze everything; unfreeze only LoRA params\n",
    "for p in model_hla.parameters():\n",
    "    p.requires_grad = False\n",
    "for name, p in model_hla.named_parameters():\n",
    "    if \"lora_A\" in name or \"lora_B\" in name:\n",
    "        p.requires_grad = True\n",
    "\n",
    "model_hla.to(\"cuda\")\n",
    "model_hla.train()\n",
    "\n",
    "optim_hla = torch.optim.AdamW(\n",
    "    (p for p in model_hla.parameters() if p.requires_grad),\n",
    "    lr=1e-3, weight_decay=0.01\n",
    ")\n",
    "\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler  = GradScaler(enabled=False)                     # <-- no scaler\n",
    "\n",
    "for batch in hla_loader:\n",
    "    input_ids = batch[\"masked_input_ids\"].to(device, dtype=torch.long)\n",
    "    labels    = batch[\"labels\"].to(device)\n",
    "\n",
    "    with autocast(\"cuda\", enabled=use_amp, dtype=torch.bfloat16):  # <-- bf16\n",
    "        out    = model_hla(input_ids)\n",
    "        logits = out.sequence_logits\n",
    "        #print(type(logits), logits.shape, logits.requires_grad)  # expect: Tensor, [B,L,V], True\n",
    "\n",
    "        loss   = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            labels.view(-1),\n",
    "            ignore_index=-100\n",
    "        )\n",
    "\n",
    "    loss.backward()                                     # <-- standard backward\n",
    "    optim_hla.step()\n",
    "    optim_hla.zero_grad(set_to_none=True)\n",
    "\n",
    "    # clear memory\n",
    "    del out, logits, loss, input_ids, labels, batch\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "optimizer_to_cpu(optim_hla)\n",
    "\n",
    "# save HLA model\n",
    "\n",
    "checkpoint_filename = 'hla_encoder_checkpoint.pth'\n",
    "\n",
    "checkpoint_dict = {\n",
    "    #'epoch': num_epochs,\n",
    "    'hla_model_state_dict': model_hla.state_dict(),\n",
    "    'optimizer_state_dict': optim_hla.state_dict(),\n",
    "    #'final_loss': avg_epoch_loss,\n",
    "    #'binding_threshold': threshold  # Add threshold to saved state\n",
    "}\n",
    "\n",
    "torch.save(checkpoint_dict, checkpoints_dir/checkpoint_filename)\n",
    "\n",
    "print(f\"Checkpoint saved to {checkpoint_filename}\")\n",
    "\n",
    "model_hla.to(\"cpu\")\n",
    "del optim_hla, model_hla, checkpoint_dict, checkpoint_filename\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a9ef0",
   "metadata": {},
   "source": [
    "#### Load Encoders and Boltz Embeddings\n",
    "\n",
    "a) Load encoders and create boltz loader function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3939828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f38124f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Encoders loaded successfully as fixed feature extractors!\n",
      "  (Optimizer state skipped - not needed for this use case)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LOADING ENCODERS FOR USE AS FIXED FEATURE EXTRACTORS\n",
    "\n",
    "Use case: You're NOT continuing to train the encoders. Instead, you'll use them\n",
    "as fixed/pre-trained components in the next part of your training pipeline (likely\n",
    "the multimodal model that combines TCR, peptide, and HLA embeddings).\n",
    "\n",
    "WHAT YOU NEED:\n",
    "✅ Model weights (to get the fine-tuned LoRA adapters)\n",
    "✅ Set to .eval() mode (to disable dropout, use batch norm stats, etc.)\n",
    "\n",
    "WHAT YOU DON'T NEED:\n",
    "❌ Optimizer state - only needed if you were continuing to train the encoders\n",
    "\n",
    "NOTE: The checkpoint contains 'optimizer_state_dict', but we're ignoring it\n",
    "since we're using these models as frozen feature extractors.\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Load the checkpoint dictionaries (just Python dicts from disk)\n",
    "# checkpoint_tcr = torch.load(checkpoints_dir/'tcr_encoder_checkpoint.pth', map_location=device)\n",
    "# checkpoint_pep = torch.load(checkpoints_dir/'peptide_encoder_checkpoint.pth', map_location=device)\n",
    "# checkpoint_hla = torch.load(checkpoints_dir/'hla_encoder_checkpoint.pth', map_location=device)\n",
    "checkpoint_tcr = torch.load(checkpoints_dir/'tcr_encoder_checkpoint.pth', map_location='cpu')\n",
    "checkpoint_pep = torch.load(checkpoints_dir/'peptide_encoder_checkpoint.pth', map_location='cpu')\n",
    "checkpoint_hla = torch.load(checkpoints_dir/'hla_encoder_checkpoint.pth', map_location='cpu')\n",
    "\n",
    "# Step 2: Recreate the model architectures (same as during training)\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "    target_modules=[\"out_proj\", \"layernorm_qkv.1\"],\n",
    ")\n",
    "\n",
    "# Create models with same architecture as during training\n",
    "tcr_encoder = LoraModel(ESMC.from_pretrained(\"esmc_300m\"), lora_cfg, adapter_name=\"tcr\")\n",
    "peptide_encoder = LoraModel(ESMC.from_pretrained(\"esmc_300m\"), lora_cfg, adapter_name=\"pep\")\n",
    "hla_encoder = LoraModel(ESMC.from_pretrained(\"esmc_300m\"), lora_cfg, adapter_name=\"hla\")\n",
    "\n",
    "# Step 3: Load the model state dictionaries (this loads base weights + LoRA adapters)\n",
    "# NOTE: We're NOT loading optimizer_state_dict - not needed for inference/feature extraction\n",
    "tcr_encoder.load_state_dict(checkpoint_tcr['tcr_model_state_dict'])\n",
    "peptide_encoder.load_state_dict(checkpoint_pep['peptide_model_state_dict'])\n",
    "hla_encoder.load_state_dict(checkpoint_hla['hla_model_state_dict'])\n",
    "\n",
    "# Step 4: Set to evaluation mode and move to CPU\n",
    "# .eval() mode ensures: no dropout, batch norm uses running stats, etc.\n",
    "# tcr_encoder.to('cpu').eval()\n",
    "# peptide_encoder.to('cpu').eval()\n",
    "# hla_encoder.to('cpu').eval()\n",
    "\n",
    "# Load to GPU becuase we need to get the embeddings in the forward pass\n",
    "tcr_encoder.to(device).eval()\n",
    "peptide_encoder.to(device).eval()\n",
    "hla_encoder.to(device).eval()\n",
    "\n",
    "print(\"✓ Encoders loaded successfully as fixed feature extractors!\")\n",
    "print(\"  (Optimizer state skipped - not needed for this use case)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fb951ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/natasha/multimodal_model/scripts/train\n",
      "456 377\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "\n",
    "home = '/home/natasha/multimodal_model'\n",
    "#manifest_path = os.path.join(home, 'data', 'manifests', 'boltz_100_manifest.csv')\n",
    "positive_manifest_path = os.path.join(home, 'data', 'manifests', 'boltz_100_manifest.csv')\n",
    "negative_manifest_path = os.path.join(home, 'data', 'negative_manifests', 'boltz_100_manifest.csv')\n",
    "#print(manifest_path)\n",
    "\n",
    "#manifest_data = pd.read_csv(manifest_path)\n",
    "positive_manifest_data = pd.read_csv(positive_manifest_path)\n",
    "negative_manifest_data = pd.read_csv(negative_manifest_path)\n",
    "manifest_data = pd.concat([positive_manifest_data, negative_manifest_data])\n",
    "\n",
    "L_T_max_boltz = max(manifest_data['tcra_len']+manifest_data['tcrb_len'])\n",
    "L_PH_max_boltz = max(manifest_data['pep_len']+manifest_data['hla_len'])\n",
    "\n",
    "print(L_T_max_boltz, L_PH_max_boltz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04c05801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get Boltz dataset\n",
    "# # old version\n",
    "\n",
    "# class BoltzDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     Dataset for loading Boltz z-embeddings one by one,\n",
    "#     with chain lengths from the manifest.\n",
    "#     Each pair has its own .npz file.\n",
    "#     ORIGINAL VERSION - returns numpy arrays\n",
    "#     \"\"\"\n",
    "#     def __init__(self, manifest_path, base_path):\n",
    "#         self.manifest = pd.read_csv(manifest_path)\n",
    "#         self.base_path = base_path\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.manifest)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         yaml_rel_path = self.manifest.iloc[idx]['yaml_path']\n",
    "#         pair_id = os.path.splitext(os.path.basename(yaml_rel_path))[0]\n",
    "#         emb_path = os.path.join(\n",
    "#             self.base_path,\n",
    "#             'outputs',\n",
    "#             'boltz_runs',\n",
    "#             'positives',\n",
    "#             pair_id,\n",
    "#             f'boltz_results_{pair_id}',\n",
    "#             'predictions',\n",
    "#             pair_id,\n",
    "#             f'embeddings_{pair_id}.npz'\n",
    "#         )\n",
    "#         with np.load(emb_path) as arr:\n",
    "#             z = arr['z']  # Returns numpy array as-is\n",
    "#         pep_len = self.manifest.iloc[idx]['pep_len']\n",
    "#         tcra_len = self.manifest.iloc[idx]['tcra_len']\n",
    "#         tcrb_len = self.manifest.iloc[idx]['tcrb_len']\n",
    "#         hla_len = self.manifest.iloc[idx]['hla_len']\n",
    "#         return z, pep_len, tcra_len, tcrb_len, hla_len\n",
    "\n",
    "\n",
    "\n",
    "# def boltz_collate_fn(batch):\n",
    "#     \"\"\"\n",
    "#     Pad Boltz z to max L in batch; return torch tensors\n",
    "#     \"\"\"\n",
    "#     zs, pep_lens, tcra_lens, tcrb_lens, hla_lens = zip(*batch)\n",
    "\n",
    "#     # Each z: [L, L, dim] or [1, L, L, dim]\n",
    "#     zs = [np.squeeze(z, axis=0) for z in zs]  # ensure (L, L, dB)\n",
    "\n",
    "#     max_len = max(z.shape[0] for z in zs)\n",
    "#     dim = zs[0].shape[-1]\n",
    "\n",
    "#     padded_zs = np.zeros((len(zs), max_len, max_len, dim), dtype=zs[0].dtype)\n",
    "#     for i, z in enumerate(zs):\n",
    "#         L = z.shape[0]\n",
    "#         padded_zs[i, :L, :L, :] = z\n",
    "\n",
    "#     zs_torch = torch.from_numpy(padded_zs).float()\n",
    "\n",
    "#     pep_lens_t  = torch.as_tensor(pep_lens,  dtype=torch.long)\n",
    "#     tcra_lens_t = torch.as_tensor(tcra_lens, dtype=torch.long)\n",
    "#     tcrb_lens_t = torch.as_tensor(tcrb_lens, dtype=torch.long)\n",
    "#     hla_lens_t  = torch.as_tensor(hla_lens,  dtype=torch.long)\n",
    "\n",
    "#     return {\n",
    "#         \"z\": zs_torch,          # (B, L_pad, L_pad, dB)\n",
    "#         \"pep_len\":  pep_lens_t, # (B,)\n",
    "#         \"tcra_len\": tcra_lens_t,\n",
    "#         \"tcrb_len\": tcrb_lens_t,\n",
    "#         \"hla_len\":  hla_lens_t,\n",
    "#     }\n",
    "\n",
    "\n",
    "# # # Usage:\n",
    "# # dataset_original = BoltzDataset(manifest_path, home)\n",
    "# # dataloader_original = DataLoader(\n",
    "# #     dataset_original, batch_size=8, shuffle=True, collate_fn=boltz_collate_fn\n",
    "# # )\n",
    "\n",
    "\n",
    "# # batch = next(iter(dataloader_original))\n",
    "# # print(\"Batch z shapes:\", [z.shape for z in batch[\"z\"]])\n",
    "# # print(\"Pep lengths:\", batch[\"pep_len\"])\n",
    "\n",
    "# # 5.6s to load 100 pairs\n",
    "# # 56 s to load 1000 pairs\n",
    "# # 560 s to load 10000 pairs (9 mins)\n",
    "# # 5600 s to load 100000 pairs (16 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "089c2fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoltzDataset(Dataset):\n",
    "    def __init__(self, manifest_path, base_path, split_dir, strict=False):\n",
    "        \"\"\"\n",
    "        split_dir: e.g. 'positives' or 'negatives'\n",
    "        strict=False: if True, raise when a file is missing; else filter it out.\n",
    "        \"\"\"\n",
    "        self.base_path = base_path\n",
    "        self.split_dir = split_dir\n",
    "        manifest = pd.read_csv(manifest_path)\n",
    "\n",
    "        valid_rows = []\n",
    "        missing = 0\n",
    "\n",
    "        for i in range(len(manifest)):\n",
    "            yaml_rel_path = manifest.iloc[i]['yaml_path']\n",
    "            pair_id = os.path.splitext(os.path.basename(yaml_rel_path))[0]\n",
    "            emb_path = os.path.join(\n",
    "                self.base_path,\n",
    "                'outputs', 'boltz_runs',\n",
    "                self.split_dir,\n",
    "                pair_id,\n",
    "                f'boltz_results_{pair_id}',\n",
    "                'predictions',\n",
    "                pair_id,\n",
    "                f'embeddings_{pair_id}.npz'\n",
    "            )\n",
    "            if os.path.exists(emb_path):\n",
    "                row = manifest.iloc[i].copy()\n",
    "                row[\"pair_id\"] = pair_id\n",
    "                row[\"emb_path\"] = emb_path\n",
    "                valid_rows.append(row)\n",
    "            else:\n",
    "                missing += 1\n",
    "                if strict:\n",
    "                    raise FileNotFoundError(f\"Missing: {emb_path}\")\n",
    "\n",
    "        self.manifest = pd.DataFrame(valid_rows).reset_index(drop=True)\n",
    "        print(f\"[BoltzDataset:{split_dir}] kept {len(self.manifest)} rows, filtered {missing} missing files.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.manifest)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.manifest.iloc[idx]\n",
    "        emb_path = row[\"emb_path\"]\n",
    "\n",
    "        with np.load(emb_path) as arr:\n",
    "            z = arr[\"z\"]\n",
    "\n",
    "        z = np.squeeze(z, axis=0) if z.ndim == 4 else z  # (L,L,dB)\n",
    "        L_pad = z.shape[0]\n",
    "\n",
    "        pep_len  = int(row[\"pep_len\"])\n",
    "        tcra_len = int(row[\"tcra_len\"])\n",
    "        tcrb_len = int(row[\"tcrb_len\"])\n",
    "        hla_len  = int(row[\"hla_len\"])\n",
    "\n",
    "        # --- clamp lengths so (tcra+tcrb+pep+hla) never exceeds what z contains ---\n",
    "        L_T  = tcra_len + tcrb_len\n",
    "        L_PH = pep_len + hla_len\n",
    "        L_total = L_T + L_PH\n",
    "\n",
    "        if L_total > L_pad:\n",
    "            L_T_new = min(L_T, L_pad)\n",
    "            remaining = L_pad - L_T_new\n",
    "            L_PH_new = min(L_PH, remaining)\n",
    "\n",
    "            tcra_new = min(tcra_len, L_T_new)\n",
    "            tcrb_new = min(tcrb_len, L_T_new - tcra_new)\n",
    "\n",
    "            pep_new = min(pep_len, L_PH_new)\n",
    "            hla_new = min(hla_len, L_PH_new - pep_new)\n",
    "\n",
    "            tcra_len, tcrb_len, pep_len, hla_len = tcra_new, tcrb_new, pep_new, hla_new\n",
    "\n",
    "        return z, pep_len, tcra_len, tcrb_len, hla_len\n",
    "\n",
    "\n",
    "def boltz_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Pad Boltz z to max L in batch; return torch tensors.\n",
    "    Handles z stored as either (L,L,dB) or (1,L,L,dB).\n",
    "    \"\"\"\n",
    "    zs, pep_lens, tcra_lens, tcrb_lens, hla_lens = zip(*batch)\n",
    "\n",
    "    zs_fixed = []\n",
    "    for z in zs:\n",
    "        # z can be (L,L,dB) or (1,L,L,dB)\n",
    "        if z.ndim == 4:\n",
    "            # only squeeze if leading dim is actually 1\n",
    "            if z.shape[0] == 1:\n",
    "                z = z[0]  # -> (L,L,dB)\n",
    "            else:\n",
    "                # unexpected but handle: take first element\n",
    "                z = z[0]\n",
    "        elif z.ndim != 3:\n",
    "            raise ValueError(f\"Unexpected z ndim={z.ndim}, shape={z.shape}\")\n",
    "\n",
    "        zs_fixed.append(z)\n",
    "\n",
    "    max_len = max(z.shape[0] for z in zs_fixed)\n",
    "    dim = zs_fixed[0].shape[-1]\n",
    "\n",
    "    padded_zs = np.zeros((len(zs_fixed), max_len, max_len, dim), dtype=zs_fixed[0].dtype)\n",
    "    for i, z in enumerate(zs_fixed):\n",
    "        L = z.shape[0]\n",
    "        padded_zs[i, :L, :L, :] = z\n",
    "\n",
    "    zs_torch = torch.from_numpy(padded_zs).float()\n",
    "\n",
    "    return {\n",
    "        \"z\": zs_torch,  # (B, L_pad, L_pad, dB)\n",
    "        \"pep_len\":  torch.as_tensor(pep_lens,  dtype=torch.long),\n",
    "        \"tcra_len\": torch.as_tensor(tcra_lens, dtype=torch.long),\n",
    "        \"tcrb_len\": torch.as_tensor(tcrb_lens, dtype=torch.long),\n",
    "        \"hla_len\":  torch.as_tensor(hla_lens,  dtype=torch.long),\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a3c69c",
   "metadata": {},
   "source": [
    "b) Load Embeddings: Pre-compute embeddings so we don't have to do it at the forward step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f3b60ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b75ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESM embeddings \n",
    "\n",
    "all_emb_T, all_emb_P, all_emb_H = [], [], []\n",
    "all_mask_T, all_mask_P, all_mask_H = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for tcr_batch, pep_batch, hla_batch in zip(tcr_loader, pep_loader, hla_loader):\n",
    "        # move to device\n",
    "        # tcr_ids = torch.stack(tcr_batch[\"clean_input_ids\"]).to(device)\n",
    "        # pep_ids = torch.stack(pep_batch[\"clean_input_ids\"]).to(device)\n",
    "        # hla_ids = torch.stack(hla_batch[\"clean_input_ids\"]).to(device)\n",
    "\n",
    "        tcr_ids  = tcr_batch[\"clean_input_ids\"].to(device)\n",
    "        pep_ids  = pep_batch[\"clean_input_ids\"].to(device)\n",
    "        hla_ids  = hla_batch[\"clean_input_ids\"].to(device)\n",
    "\n",
    "        tcr_mask = tcr_batch[\"attention_mask\"]\n",
    "        pep_mask = pep_batch[\"attention_mask\"]\n",
    "        hla_mask = hla_batch[\"attention_mask\"]\n",
    "\n",
    "        out_T = tcr_encoder.model(sequence_tokens=tcr_ids)\n",
    "        out_P = peptide_encoder.model(sequence_tokens=pep_ids)\n",
    "        out_H = hla_encoder.model(sequence_tokens=hla_ids)\n",
    "\n",
    "        all_emb_T.append(out_T.embeddings.cpu())\n",
    "        all_emb_P.append(out_P.embeddings.cpu())\n",
    "        all_emb_H.append(out_H.embeddings.cpu())\n",
    "\n",
    "        all_mask_T.append(tcr_mask)\n",
    "        all_mask_P.append(pep_mask)\n",
    "        all_mask_H.append(hla_mask)\n",
    "\n",
    "emb_T = torch.cat(all_emb_T, dim=0)\n",
    "emb_P = torch.cat(all_emb_P, dim=0)\n",
    "emb_H = torch.cat(all_emb_H, dim=0)\n",
    "\n",
    "mask_T = torch.cat(all_mask_T, dim=0)\n",
    "mask_P = torch.cat(all_mask_P, dim=0)\n",
    "mask_H = torch.cat(all_mask_H, dim=0)\n",
    "\n",
    "torch.save(\n",
    "    {\"emb_T\": emb_T, \"emb_P\": emb_P, \"emb_H\": emb_H,\n",
    "     \"mask_T\": mask_T, \"mask_P\": mask_P, \"mask_H\": mask_H},\n",
    "    \"train_embeddings.pt\"\n",
    ")\n",
    "\n",
    "tcr_encoder.to('cpu')\n",
    "peptide_encoder.to('cpu')\n",
    "hla_encoder.to('cpu')\n",
    "\n",
    "# Clear GPU cache to ensure nothing is hanging around\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# took 1.8s to load 100 full pairs on GPU\n",
    "# 18s to load 1,000 full pairs on GPU\n",
    "# 3m to load 10,000 full pairs on GPU\n",
    "# 9m to load 30,000 full pairs on GPU \n",
    "# so we're good to go for training purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ce6fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Boltz Embeddings\n",
    "# # Old - trying to fix to plot histograms\n",
    "\n",
    "# # Before defining positives and negaitves for comparison\n",
    "# # dataset_original = BoltzDataset(manifest_path, home)\n",
    "# # boltz_loader = DataLoader(\n",
    "# #     dataset_original, batch_size=8, shuffle=True, collate_fn=boltz_collate_fn\n",
    "# # )\n",
    "\n",
    "# dataset_original = BoltzDataset(positive_manifest_path, home)\n",
    "# boltz_loader = DataLoader(\n",
    "#     dataset_original, batch_size=8, shuffle=True, collate_fn=boltz_collate_fn\n",
    "# )\n",
    "\n",
    "# # load negatives\n",
    "\n",
    "# dataset_negatives = BoltzDataset(negative_manifest_path, home)\n",
    "# negatives_loader = DataLoader(\n",
    "#     dataset_negatives, batch_size=8, shuffle=True, collate_fn=boltz_collate_fn\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0e09630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BoltzDataset:positives] kept 100 rows, filtered 0 missing files.\n",
      "[BoltzDataset:negatives] kept 32 rows, filtered 68 missing files.\n"
     ]
    }
   ],
   "source": [
    "dataset_original  = BoltzDataset(positive_manifest_path, home, split_dir=\"positives\")\n",
    "boltz_loader      = DataLoader(dataset_original, batch_size=8, shuffle=True, collate_fn=boltz_collate_fn)\n",
    "\n",
    "dataset_negatives = BoltzDataset(negative_manifest_path, home, split_dir=\"negatives\")\n",
    "negatives_loader  = DataLoader(dataset_negatives, batch_size=8, shuffle=True, collate_fn=boltz_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5617bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for correct padding lengths\n",
    "\n",
    "# b = next(iter(negatives_loader))\n",
    "# print(\"neg batch z pad:\", b[\"z\"].shape[1])\n",
    "# print(\"neg max L (after clamping):\", int((b[\"tcra_len\"]+b[\"tcrb_len\"]+b[\"pep_len\"]+b[\"hla_len\"]).max().item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01f6fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ PLOTTING FUNCTIONS ------------\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_zstar_metrics(Zstar: torch.Tensor, eps: float = 1e-12):\n",
    "    \"\"\"\n",
    "    Zstar: (B, n, n) assumed symmetric (or close).\n",
    "    Returns dict of per-sample tensors (B,) for:\n",
    "      - diag_mean_mean\n",
    "      - offdiag_mean_mean\n",
    "      - z_eig_top1_share\n",
    "      - z_eig_top2_share\n",
    "      - z_trace\n",
    "    \"\"\"\n",
    "    # force symmetry for safety\n",
    "    Z = 0.5 * (Zstar + Zstar.transpose(-1, -2))  # (B,n,n)\n",
    "    B, n, _ = Z.shape\n",
    "\n",
    "    # diag / offdiag means\n",
    "    diag = torch.diagonal(Z, dim1=-2, dim2=-1)          # (B,n)\n",
    "    diag_mean_mean = diag.mean(dim=-1)                  # (B,)\n",
    "\n",
    "    total_sum = Z.sum(dim=(-2, -1))                     # (B,)\n",
    "    diag_sum  = diag.sum(dim=-1)                        # (B,)\n",
    "    off_sum   = total_sum - diag_sum                    # (B,)\n",
    "    off_count = n * (n - 1)\n",
    "    offdiag_mean_mean = off_sum / (off_count + eps)     # (B,)\n",
    "\n",
    "    # trace\n",
    "    z_trace = diag_sum                                  # (B,)\n",
    "\n",
    "    # eigenvalue shares (use abs so “dominance” is scale-invariant under sign flips)\n",
    "    evals = torch.linalg.eigvalsh(Z)                    # (B,n), ascending\n",
    "    abs_evals = evals.abs()\n",
    "\n",
    "    # sort descending for top shares\n",
    "    abs_sorted, _ = torch.sort(abs_evals, dim=-1, descending=True)  # (B,n)\n",
    "    denom = abs_sorted.sum(dim=-1) + eps                             # (B,)\n",
    "\n",
    "    z_eig_top1_share = abs_sorted[:, 0] / denom                      # (B,)\n",
    "    z_eig_top2_share = (abs_sorted[:, 0] + abs_sorted[:, 1]) / denom # (B,)\n",
    "\n",
    "    return {\n",
    "        \"diag_mean_mean\": diag_mean_mean.detach().cpu(),\n",
    "        \"offdiag_mean_mean\": offdiag_mean_mean.detach().cpu(),\n",
    "        \"z_eig_top1_share\": z_eig_top1_share.detach().cpu(),\n",
    "        \"z_eig_top2_share\": z_eig_top2_share.detach().cpu(),\n",
    "        \"z_trace\": z_trace.detach().cpu(),\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_zstar_histograms(metrics_pos: dict, metrics_neg: dict, bins: int = 30, suptitle: str = \"\"):\n",
    "    \"\"\"\n",
    "    metrics_* : dict[str, Tensor(B,)] from compute_zstar_metrics\n",
    "    Plots 5 histograms: diag_mean_mean, offdiag_mean_mean, z_eig_top1_share, z_eig_top2_share, z_trace\n",
    "    \"\"\"\n",
    "    keys = [\"diag_mean_mean\", \"offdiag_mean_mean\", \"z_eig_top1_share\", \"z_eig_top2_share\", \"z_trace\"]\n",
    "\n",
    "    plt.figure(figsize=(18, 4))\n",
    "    if suptitle:\n",
    "        plt.suptitle(suptitle)\n",
    "\n",
    "    for i, k in enumerate(keys, 1):\n",
    "        plt.subplot(1, 5, i)\n",
    "        x_pos = metrics_pos[k].numpy()\n",
    "        x_neg = metrics_neg[k].numpy()\n",
    "        plt.hist(x_pos, bins=bins, alpha=0.6, label=\"Positives\")\n",
    "        plt.hist(x_neg, bins=bins, alpha=0.6, label=\"Negatives\")\n",
    "        plt.title(k)\n",
    "        plt.xlabel(k)\n",
    "        plt.ylabel(\"Pairs\")\n",
    "        if i == 1:\n",
    "            plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def forward_to_Zstar_only(\n",
    "    batch_seq: dict,\n",
    "    batch_boltz: dict,\n",
    "    tcr_factorised,\n",
    "    pmhc_factorised,\n",
    "    boltz_factoriser,\n",
    "    gP,\n",
    "    gH,\n",
    "    device,\n",
    "    eps: float = 1e-8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs *inference-only* forward for a batch and returns Zstar (B,2d,2d).\n",
    "    Assumes batch_seq has emb/mask keys and batch_boltz has z + length keys.\n",
    "    \"\"\"\n",
    "    # sequence side\n",
    "    emb_T  = batch_seq[\"emb_T\"].to(device)\n",
    "    mask_T = batch_seq[\"mask_T\"].to(device)\n",
    "    emb_P  = batch_seq[\"emb_P\"].to(device)\n",
    "    mask_P = batch_seq[\"mask_P\"].to(device)\n",
    "    emb_H  = batch_seq[\"emb_H\"].to(device)\n",
    "    mask_H = batch_seq[\"mask_H\"].to(device)\n",
    "\n",
    "    zT  = tcr_factorised(emb_T, mask_T)\n",
    "    zT  = zT / (zT.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "    zPH = pmhc_factorised(emb_P, mask_P, emb_H, mask_H)\n",
    "    zPH = zPH / (zPH.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "    e_hat = torch.cat([zT, zPH], dim=-1)\n",
    "\n",
    "    # boltz side\n",
    "    z_boltz = batch_boltz[\"z\"].to(device)\n",
    "    L_p     = batch_boltz[\"pep_len\"].to(device)\n",
    "    L_alpha = batch_boltz[\"tcra_len\"].to(device)\n",
    "    L_beta  = batch_boltz[\"tcrb_len\"].to(device)\n",
    "    L_h     = batch_boltz[\"hla_len\"].to(device)\n",
    "\n",
    "    # Zstar\n",
    "    _, _, Zstar = non_contrastive_hamiltonian_loss(\n",
    "        zT=zT,\n",
    "        zPH=zPH,\n",
    "        e_hat=e_hat,\n",
    "        z_boltz_batch=z_boltz,\n",
    "        L_alpha=L_alpha,\n",
    "        L_beta=L_beta,\n",
    "        L_p=L_p,\n",
    "        L_h=L_h,\n",
    "        gP=gP,\n",
    "        gH=gH,\n",
    "        boltz_factoriser=boltz_factoriser,\n",
    "        alpha=1.0,\n",
    "        beta=1.0,\n",
    "        delta=1.0,\n",
    "        gamma_var=1.0,\n",
    "        eps=1e-4,\n",
    "        return_Zstar=True,\n",
    "        use_limit_Zstar=False,\n",
    "    )\n",
    "\n",
    "    return Zstar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5143b90",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab3504",
   "metadata": {},
   "source": [
    "##### a) Projection heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca830729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorised Encoder to get z_T and Z_pMHC\n",
    "# z = vec(A^TXB)H\n",
    "# X - (B, L_pad, D)\n",
    "# B - (D, rD)\n",
    "# A - (L_pad, rL)\n",
    "# H - (rD * rL, d)\n",
    "\n",
    "\n",
    "eps = 1e-8\n",
    "\n",
    "class ESMFactorisedEncoder(nn.Module):\n",
    "    def __init__(self, D, rL, rD, d, L_max):\n",
    "        \"\"\"\n",
    "        D    : ESM embedding dim (e.g. 960)\n",
    "        rL   : positional rank\n",
    "        rD   : channel rank\n",
    "        d    : latent dim\n",
    "        L_max: max true length for this modality in the batch\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.D   = D\n",
    "        self.rL  = rL\n",
    "        self.rD  = rD\n",
    "        self.d   = d\n",
    "        self.L_max = L_max\n",
    "\n",
    "        # Channel mixing: D -> rD\n",
    "        self.B_c = nn.Parameter(torch.empty(D, rD))\n",
    "        nn.init.xavier_uniform_(self.B_c)\n",
    "\n",
    "        # Positional mixing: positions 0..L_max-1 -> rL\n",
    "        self.A_c = nn.Parameter(torch.empty(L_max, rL))\n",
    "        nn.init.xavier_uniform_(self.A_c)\n",
    "\n",
    "        # Final map: (rL * rD) -> d\n",
    "        self.H_c = nn.Parameter(torch.empty(rL * rD, d))\n",
    "        nn.init.xavier_uniform_(self.H_c)\n",
    "\n",
    "    def forward(self, emb, mask):\n",
    "        \"\"\"\n",
    "        emb  : (B, L_pad, D) token embeddings\n",
    "        mask : (B, L_pad)   1 = real token, 0 = pad\n",
    "        returns z : (B, d)\n",
    "        \"\"\"\n",
    "        device = emb.device\n",
    "        B, L_pad, D = emb.shape\n",
    "        assert D == self.D\n",
    "\n",
    "        # Compute true lengths\n",
    "        L_true = mask.sum(dim=1)            # (B,)\n",
    "        z_list = []\n",
    "\n",
    "        for b in range(B):\n",
    "            Lb = int(L_true[b].item())\n",
    "            if Lb == 0:\n",
    "                # Degenerate case: no tokens -> zero vector\n",
    "                z_b = torch.zeros(self.d, device=device)\n",
    "                z_list.append(z_b)\n",
    "                continue\n",
    "\n",
    "            Xb = emb[b, :Lb, :]                      # (Lb, D)\n",
    "            mb = mask[b, :Lb].unsqueeze(-1).float()  # (Lb, 1)\n",
    "            Xb = Xb * mb                             # (Lb, D)\n",
    "\n",
    "            # 1) Channel compression: D -> rD\n",
    "            Yb = Xb @ self.B_c                       # (Lb, rD)\n",
    "\n",
    "            # 2) Positional compression: Lb -> rL\n",
    "            A_pos = self.A_c[:Lb, :]                 # (Lb, rL)\n",
    "            Ub = A_pos.T @ Yb                        # (rL, rD)\n",
    "\n",
    "            # 3) Flatten and map to latent d\n",
    "            Ub_flat = Ub.reshape(-1)                 # (rL * rD,)\n",
    "            z_b = Ub_flat @ self.H_c                 # (d,)\n",
    "\n",
    "            # 4) Normalise (optional; you can drop this if you want magnitude to carry info)\n",
    "            #z_b = z_b / (z_b.norm() + eps)\n",
    "            #normalise after function because need to combine p and hla first\n",
    "\n",
    "            z_list.append(z_b)\n",
    "\n",
    "        z = torch.stack(z_list, dim=0)               # (B, d)\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "670d3f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorised Encoder to get Z_pMHC with restrictions on pMHC dimensions \n",
    "# Calls ESMFactorisedEncoder for peptide and HLA separately\n",
    "\n",
    "class PMHCFactorisedEncoder(nn.Module):\n",
    "    def __init__(self, D, rL, rD, d, L_P_max, L_H_max, R_PH=0.7):\n",
    "        \"\"\"\n",
    "        D      : ESM embedding dim\n",
    "        rL     : positional rank\n",
    "        rD     : channel rank\n",
    "        d      : total latent dim for pMHC\n",
    "        L_P_max: max true peptide length\n",
    "        L_H_max: max true HLA length\n",
    "        R_PH   : fraction of d reserved for peptide (e.g. 0.7)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.D    = D\n",
    "        self.rL   = rL\n",
    "        self.rD   = rD\n",
    "        self.d    = d\n",
    "        self.R_PH = R_PH\n",
    "\n",
    "        # Split d into peptide and HLA sub-dims\n",
    "        d_P = int(round(R_PH * d))\n",
    "        d_H = d - d_P\n",
    "        assert d_P > 0 and d_H > 0, \"Choose d and R_PH so both > 0\"\n",
    "\n",
    "        self.d_P = d_P\n",
    "        self.d_H = d_H\n",
    "\n",
    "        # Separate factorised encoders for peptide and HLA\n",
    "        self.pep_encoder = ESMFactorisedEncoder(D, rL, rD, d_P, L_P_max)\n",
    "        self.hla_encoder = ESMFactorisedEncoder(D, rL, rD, d_H, L_H_max)\n",
    "\n",
    "    def forward(self, emb_P, mask_P, emb_H, mask_H):\n",
    "        \"\"\"\n",
    "        emb_P: (B, L_P_pad, D)\n",
    "        mask_P: (B, L_P_pad)\n",
    "        emb_H: (B, L_H_pad, D)\n",
    "        mask_H: (B, L_H_pad)\n",
    "        returns zPH: (B, d) with first d_P dims peptide, last d_H dims HLA\n",
    "        \"\"\"\n",
    "        eps = 1e-8\n",
    "        device = emb_P.device\n",
    "\n",
    "        zP = self.pep_encoder(emb_P, mask_P)  # (B, d_P)\n",
    "        zH = self.hla_encoder(emb_H, mask_H)  # (B, d_H)\n",
    "\n",
    "        # Optional: normalise segments separately so one can't trivially dominate\n",
    "        zP = zP / (zP.norm(dim=-1, keepdim=True) + eps)\n",
    "        zH = zH / (zH.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "        # Concatenate: 70% dims peptide, 30% dims HLA\n",
    "        zPH = torch.cat([zP, zH], dim=-1)     # (B, d)\n",
    "\n",
    "        # Optional: global normalisation\n",
    "        zPH = zPH / (zPH.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "        return zPH\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01874950",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoltzFactorised(nn.Module):\n",
    "    \"\"\"\n",
    "    Factorised Boltz embeddings for projection into latent shared space before NC loss\n",
    "\n",
    "    Inputs:\n",
    "    - z_boltz: (B, L_pad, L_pad, d_boltz) full Boltz z for the batch\n",
    "    - L_alpha, L_beta, L_p, L_h: (B,) lengths of the TCR alpha, TCR beta, peptide, HLA\n",
    "    - gP, gH: (B,) scalar or (B,) peptide/HLA gates in [0,1], norm-preserving in quadrature???? As in, gP**2 + gH**2 = 1\n",
    "\n",
    "    Outputs:\n",
    "    - Zstar_batch: (B, 2d, 2d) operator acting on e_hat_t, e_hat_pmc in R^2d\n",
    "    \"\"\"\n",
    "    def __init__(self, dB, rB, rT, rPH, d, L_max, L_PH_max):\n",
    "        \"\"\"\n",
    "        dB      : channel dimension of Boltz embeddings\n",
    "        rB      : rank of Boltz channel factorisation\n",
    "        rT      : rank of TCR positional encoding\n",
    "        rPH     : rank of pMHC positional encoding\n",
    "        d       : latent dimension of shared space\n",
    "        L_T_max   : maximum length of any sequence in the batch\n",
    "        L_PH_max: maximum length of any pMHC sequence in the batch\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dB     = dB\n",
    "        self.rB     = rB\n",
    "        self.rT     = rT\n",
    "        self.rPH    = rPH\n",
    "        self.d      = d\n",
    "        self.L_T_max  = L_max\n",
    "        self.L_PH_max = L_PH_max\n",
    "\n",
    "        # ---- 1) Channel mixing: dB -> rB ---- \n",
    "        self.B_Z = torch.nn.Parameter(torch.empty(dB, rB))\n",
    "        nn.init.xavier_uniform_(self.B_Z)\n",
    "\n",
    "        # ---- 2)a) TCR positional encoding: rT -> rT ---- \n",
    "        #self.A_T = torch.nn.Parameter(torch.empty(L_T_max, rT))\n",
    "        self.A_T  = torch.nn.Parameter(torch.empty(self.L_T_max,  rT))\n",
    "        nn.init.xavier_uniform_(self.A_T)\n",
    "\n",
    "        # ---- 2)b) pMHC positional encoding: rPH -> rPH ---- \n",
    "        #self.A_PH = torch.nn.Parameter(torch.empty(L_PH_max, rPH))\n",
    "        self.A_PH = torch.nn.Parameter(torch.empty(self.L_PH_max, rPH))\n",
    "        nn.init.xavier_uniform_(self.A_PH)\n",
    "\n",
    "        # ---- 3) Learnable maps from factorised z (r* x r* x rB) -> d x d ---- \n",
    "        # flatten sizes for each block\n",
    "        n_TT   = rT  * rT  * rB\n",
    "        n_TPH  = rT  * rPH * rB\n",
    "        n_PHT  = rPH * rT  * rB\n",
    "        n_PHPH = rPH * rPH * rB\n",
    "        dd     = d * d\n",
    "\n",
    "        self.H_TT   = nn.Parameter(torch.empty(n_TT,   dd))\n",
    "        self.H_TPH  = nn.Parameter(torch.empty(n_TPH,  dd))\n",
    "        self.H_PHT  = nn.Parameter(torch.empty(n_PHT,  dd))\n",
    "        self.H_PHPH = nn.Parameter(torch.empty(n_PHPH, dd))\n",
    "\n",
    "        nn.init.xavier_uniform_(self.H_TT)\n",
    "        nn.init.xavier_uniform_(self.H_TPH)\n",
    "        nn.init.xavier_uniform_(self.H_PHT)\n",
    "        nn.init.xavier_uniform_(self.H_PHPH)\n",
    "\n",
    "        # ---- 4) Final linear layer: d -> d ---- \n",
    "        self.W_out = nn.Parameter(torch.empty(d, d))\n",
    "        nn.init.xavier_uniform_(self.W_out)\n",
    "    \n",
    "    def _get_gate_scalar(self, g, b):\n",
    "        \"\"\"\n",
    "        Helper: allow g to be a scalar tensor () or per-sample tensor (B,).\n",
    "        Returns a Python float for sample b.\n",
    "        \"\"\"\n",
    "        if g.dim() == 0:\n",
    "            return float(g.item())\n",
    "        else:\n",
    "            return float(g[b].item())\n",
    "\n",
    "    def forward(self, z_boltz, L_alpha, L_beta, L_p, L_h, gP, gH):\n",
    "        \"\"\"\n",
    "        Z_boltz : (B, L_pad, L_pad, dB)\n",
    "        L_alpha : (B,) true alpha lengths\n",
    "        L_beta  : (B,) true beta lengths\n",
    "        L_p     : (B,) true peptide lengths\n",
    "        L_h     : (B,) true HLA lengths\n",
    "        gP      : scalar () or (B,) peptide gate (already sqrt(R_PH))\n",
    "        gH      : scalar () or (B,) HLA gate (already sqrt(1-R_PH))\n",
    "\n",
    "        Returns:\n",
    "          Zstar_batch: (B, 2d, 2d)\n",
    "        \"\"\"\n",
    "\n",
    "        device = z_boltz.device\n",
    "        B, L_pad, _, dB = z_boltz.shape\n",
    "        assert dB == self.dB\n",
    "\n",
    "        Zstar_list = []\n",
    "\n",
    "        for b in range(B):\n",
    "            La  = int(L_alpha[b].item())\n",
    "            Lb  = int(L_beta[b].item())\n",
    "            Lp_ = int(L_p[b].item())\n",
    "            Lh_ = int(L_h[b].item())\n",
    "\n",
    "            L_T     = La + Lb\n",
    "            L_PH    = Lp_ + Lh_\n",
    "            L       = L_T + L_PH\n",
    "\n",
    "            # if we have missing z it just returns identity\n",
    "            if L == 0:\n",
    "                I_2d = torch.eye(2* self.d, device=device)\n",
    "                Zstar_list.append(I_2d)\n",
    "                continue\n",
    "            \n",
    "\n",
    "            # restrict to true tokens for the sample\n",
    "            Z = z_boltz[b, :L, :L, :] # (L, L, dB)\n",
    "\n",
    "            \n",
    "            Zc = Z.clone()\n",
    "\n",
    "            # ----- 3) Gating ----\n",
    "            # TCR gates\n",
    "            if La > 0 and Lb > 0:\n",
    "                gA_b = 2**-0.5\n",
    "                gB_b = 2**-0.5\n",
    "            elif La > 0 and Lb == 0:\n",
    "                gA_b = 1\n",
    "                gB_b = 0\n",
    "            elif La == 0 and Lb > 0:\n",
    "                gA_b = 0\n",
    "                gB_b = 1\n",
    "            else:\n",
    "                gA_b = 0\n",
    "                gB_b = 0\n",
    "\n",
    "            # Peptide/HLA gates (from R set in encoder part)\n",
    "            gP_b = self._get_gate_scalar(gP, b)\n",
    "            gH_b = self._get_gate_scalar(gH, b)\n",
    "\n",
    "            # ---- 4) Build token-level gate vector over [alpha | beta | p | h] ----\n",
    "            gate = torch.zeros(L, device=device) # (L,)\n",
    "\n",
    "            idx0 = 0\n",
    "            idx1 = idx0 + La\n",
    "            idx2 = idx1 + Lb\n",
    "            idx3 = idx2 + Lp_\n",
    "            idx4 = idx3 + Lh_\n",
    "\n",
    "            if La  > 0: gate[idx0:idx1] = gA_b\n",
    "            if Lb  > 0: gate[idx1:idx2] = gB_b\n",
    "            if Lp_ > 0: gate[idx2:idx3] = gP_b\n",
    "            if Lh_ > 0: gate[idx3:idx4] = gH_b\n",
    "\n",
    "            gate_row = gate.view(L, 1, 1) # (L, 1, 1)\n",
    "            gate_col = gate.view(1, L, 1) # (1, L, 1)\n",
    "\n",
    "            Zg = Zc * gate_row * gate_col # (L, L, dB)\n",
    "\n",
    "            # ---- 5) Get TCR/pMHC blocks ----\n",
    "            sT = slice(0, L_T)           # [0, L_T) -> TCR (alpha + beta)\n",
    "            sPH = slice(L_T, L_T + L_PH) # [L_T, L] -> pMHC (P+H)\n",
    "\n",
    "            Z_TT  = Zg[sT, sT, :]    # (L_T, L_T, dB)\n",
    "            Z_TPH = Zg[sT, sPH, :]   # (L_T, L_PH, dB)\n",
    "            Z_PHT = Zg[sPH, sT, :]   # (L_PH, L_T, dB)\n",
    "            Z_PHPH = Zg[sPH, sPH, :] # (L_PH, L_PH, dB)\n",
    "\n",
    "            # ---- 6) channel/dimension compression ----\n",
    "            B_Z = self.B_Z # (dB, rB) operator across channels\n",
    "            Y_TT   = torch.einsum('ijc,cr->ijr', Z_TT,   B_Z)   # (L_T,  L_T,  rB)\n",
    "            Y_TPH  = torch.einsum('ijc,cr->ijr', Z_TPH,  B_Z)   # (L_T,  L_PH, rB)\n",
    "            Y_PHT  = torch.einsum('ijc,cr->ijr', Z_PHT,  B_Z)   # (L_PH, L_T,  rB)\n",
    "            Y_PHPH = torch.einsum('ijc,cr->ijr', Z_PHPH, B_Z)   # (L_PH, L_PH, rB)\n",
    "\n",
    "            # ---- 7) TCR positional compression with A_T / A_PH ----\n",
    "            # Sample-specific rows for the correct lengths for the per-sample positional tensors\n",
    "            if L_T > 0:\n",
    "                A_T_b = self.A_T[:L_T, :] # (L_T, rT)\n",
    "            else: \n",
    "                # no TCRs, so treat as zero contribution (or could be noise, decide later)\n",
    "                A_T_b = self.A_T[:1, :] * 0.0 # (1, rT) dummy\n",
    "\n",
    "            if L_PH > 0:\n",
    "                A_PH_b = self.A_PH[:L_PH, :] # (L_PH, rPH)\n",
    "            else:\n",
    "                # no pMHCs, so treat as zero contribution (or could be noise, decide later)\n",
    "                A_PH_b = self.A_PH[:1, :] * 0.0 # (1, rPH) dummy\n",
    "            \n",
    "            rT  = self.rT\n",
    "            rPH = self.rPH\n",
    "            rB  = self.rB\n",
    "            d   = self.d\n",
    "\n",
    "            # N.B. U tensors are not learned, they are discarded as intermediary steps for compression\n",
    "            # TCR-TCR (L_T, L_T, rB) -> (rT, rT, rB)\n",
    "            if L_T > 0:\n",
    "                U_TT = torch.einsum('ip,ijr->pjr', A_T_b, Y_TT) # (rT, L_T, rB)\n",
    "                V_TT = torch.einsum('pjr,jq->pqr', U_TT, A_T_b) # (rT, rT, rB)\n",
    "            else:\n",
    "                V_TT = torch.zeros(rT, rT, rB, device=device) # (rT, rT, rB)\n",
    "\n",
    "            # TCR–pMHC: (L_T, L_PH, rB) -> (rT, rPH, rB)\n",
    "            if L_T > 0 and L_PH > 0:\n",
    "                U_TPH = torch.einsum('ip,ijr->pjr', A_T_b,  Y_TPH)   # (rT,  L_PH, rB)\n",
    "                V_TPH = torch.einsum('pjr,jq->pqr', U_TPH, A_PH_b)   # (rT,  rPH, rB)\n",
    "            else:\n",
    "                V_TPH = torch.zeros(rT, rPH, rB, device=device)\n",
    "\n",
    "            # pMHC–TCR: (L_PH, L_T, rB) -> (rPH, rT, rB)\n",
    "            if L_T > 0 and L_PH > 0:\n",
    "                U_PHT = torch.einsum('ip,ijr->pjr', A_PH_b, Y_PHT)   # (rPH, L_T,  rB)\n",
    "                V_PHT = torch.einsum('pjr,jq->pqr', U_PHT, A_T_b)    # (rPH, rT,  rB)\n",
    "            else:\n",
    "                V_PHT = torch.zeros(rPH, rT, rB, device=device)\n",
    "\n",
    "            # pMHC–pMHC: (L_PH, L_PH, rB) -> (rPH, rPH, rB)\n",
    "            if L_PH > 0:\n",
    "                U_PHPH = torch.einsum('ip,ijr->pjr', A_PH_b, Y_PHPH) # (rPH, L_PH, rB)\n",
    "                V_PHPH = torch.einsum('pjr,jq->pqr', U_PHPH, A_PH_b) # (rPH, rPH, rB)\n",
    "            else:\n",
    "                V_PHPH = torch.zeros(rPH, rPH, rB, device=device)\n",
    "\n",
    "            # ---- 8) Flatten factorised blocks and map to d×d via H_* ----\n",
    "            v_TT_flat   = V_TT.reshape(-1)    # (rT*rT*rB,)\n",
    "            v_TPH_flat  = V_TPH.reshape(-1)   # (rT*rPH*rB,)\n",
    "            v_PHT_flat  = V_PHT.reshape(-1)   # (rPH*rT*rB,)\n",
    "            v_PHPH_flat = V_PHPH.reshape(-1)  # (rPH*rPH*rB,)\n",
    "\n",
    "            k_TT_flat   = v_TT_flat   @ self.H_TT   # (d*d,)\n",
    "            k_TPH_flat  = v_TPH_flat  @ self.H_TPH  # (d*d,)\n",
    "            k_PHT_flat  = v_PHT_flat  @ self.H_PHT  # (d*d,)\n",
    "            k_PHPH_flat = v_PHPH_flat @ self.H_PHPH # (d*d,)\n",
    "\n",
    "            K_TT   = k_TT_flat.view(d, d)\n",
    "            K_TPH  = k_TPH_flat.view(d, d)\n",
    "            K_PHT  = k_PHT_flat.view(d, d)\n",
    "            K_PHPH = k_PHPH_flat.view(d, d)\n",
    "\n",
    "            # ---- 9) Assemble 2d x 2d operator for this sample ----\n",
    "            I_d = torch.eye(d, device=device)\n",
    "            Zstar_b = torch.zeros(2*d, 2*d, device=device)\n",
    "\n",
    "            # keeping original - not manually trying to scale the blocks to control for blow up in loss\n",
    "            Zstar_b[:d,  :d]  = I_d + K_TT\n",
    "            Zstar_b[:d,  d:]  = I_d + K_TPH\n",
    "            Zstar_b[d:,  :d]  = I_d + K_PHT\n",
    "            Zstar_b[d:,  d:]  = I_d + K_PHPH\n",
    "\n",
    "\n",
    "            Zstar_list.append(Zstar_b)\n",
    "\n",
    "        Zstar_batch = torch.stack(Zstar_list, dim=0)  # (B, 2d, 2d)\n",
    "        # symmetrise whole matrix\n",
    "        Zstar_batch = 0.5 * (Zstar_batch + Zstar_batch.transpose(1, 2)) # or at beginning\n",
    "        # also enforce within the learning too\n",
    "        return Zstar_batch \n",
    "\n",
    "\n",
    "# index convention for the einsum\n",
    "# i = row token position, “i” is historically “index” or “first axis”\n",
    "# j\t= column token position, second positional axis (matrix-like)\n",
    "# c\t= channel\n",
    "# p, q\t= latent positional modes, P = projection / latent position\n",
    "# r = latent channel modes, R = rank / channel rank\n",
    "# b\t= batch index, B = batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d5161",
   "metadata": {},
   "source": [
    "##### b) Batch Loop with Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d75f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingTripletDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path):\n",
    "        data = torch.load(path)\n",
    "        self.emb_T = data[\"emb_T\"]   # (N, L_T_pad, D)\n",
    "        self.emb_P = data[\"emb_P\"]   # (N, L_P_pad, D)\n",
    "        self.emb_H = data[\"emb_H\"]   # (N, L_H_pad, D)\n",
    "\n",
    "        self.mask_T = data[\"mask_T\"] # (N, L_T_pad)\n",
    "        self.mask_P = data[\"mask_P\"]\n",
    "        self.mask_H = data[\"mask_H\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.emb_T.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"emb_T\":  self.emb_T[idx],\n",
    "            \"mask_T\": self.mask_T[idx],\n",
    "            \"emb_P\":  self.emb_P[idx],\n",
    "            \"mask_P\": self.mask_P[idx],\n",
    "            \"emb_H\":  self.emb_H[idx],\n",
    "            \"mask_H\": self.mask_H[idx],\n",
    "        }\n",
    "\n",
    "train_dataset = EmbeddingTripletDataset(\"train_embeddings.pt\")\n",
    "train_loader  = DataLoader(train_dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d75cfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for recreating original boltz \n",
    "\n",
    "def _get_gate_scalar(g, b):\n",
    "    # g can be scalar () or per-sample (B,)\n",
    "    return float(g.item()) if g.dim() == 0 else float(g[b].item())\n",
    "\n",
    "\n",
    "def build_token_gate(La, Lb, Lp, Lh, gP_b, gH_b, device):\n",
    "    \"\"\"\n",
    "    Build token-level gate vector over [alpha | beta | peptide | hla] with your exact logic.\n",
    "    Returns gate: (L,)\n",
    "    \"\"\"\n",
    "    L = La + Lb + Lp + Lh\n",
    "    gate = torch.zeros(L, device=device)\n",
    "\n",
    "    # TCR gates\n",
    "    if La > 0 and Lb > 0:\n",
    "        gA_b = 2**-0.5\n",
    "        gB_b = 2**-0.5\n",
    "    elif La > 0 and Lb == 0:\n",
    "        gA_b, gB_b = 1.0, 0.0\n",
    "    elif La == 0 and Lb > 0:\n",
    "        gA_b, gB_b = 0.0, 1.0\n",
    "    else:\n",
    "        gA_b, gB_b = 0.0, 0.0\n",
    "\n",
    "    idx0 = 0\n",
    "    idx1 = idx0 + La\n",
    "    idx2 = idx1 + Lb\n",
    "    idx3 = idx2 + Lp\n",
    "    idx4 = idx3 + Lh\n",
    "\n",
    "    if La > 0: gate[idx0:idx1] = gA_b\n",
    "    if Lb > 0: gate[idx1:idx2] = gB_b\n",
    "    if Lp > 0: gate[idx2:idx3] = gP_b\n",
    "    if Lh > 0: gate[idx3:idx4] = gH_b\n",
    "\n",
    "    return gate\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def raw_boltz_sym_hist_metrics(\n",
    "    z_boltz, L_alpha, L_beta, L_p, L_h, gP, gH, eps=1e-8\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the raw (gated) symmetrised metrics you care about, in Torch.\n",
    "\n",
    "    z_boltz: (B, L_pad, L_pad, dB)\n",
    "    returns dict of (B,) tensors:\n",
    "      raw_diag_mean_mean\n",
    "      raw_offdiag_mean_mean\n",
    "      raw_trace_over_L   (trace normalised by L for length stability)\n",
    "    \"\"\"\n",
    "    device = z_boltz.device\n",
    "    B, L_pad, _, dB = z_boltz.shape\n",
    "\n",
    "    diag_list  = []\n",
    "    off_list   = []\n",
    "    tr_list    = []\n",
    "\n",
    "    for b in range(B):\n",
    "        La = int(L_alpha[b].item())\n",
    "        Lb = int(L_beta[b].item())\n",
    "        Lp_ = int(L_p[b].item())\n",
    "        Lh_ = int(L_h[b].item())\n",
    "        L = La + Lb + Lp_ + Lh_\n",
    "\n",
    "        if L <= 1:\n",
    "            diag_list.append(torch.tensor(0.0, device=device))\n",
    "            off_list.append(torch.tensor(0.0, device=device))\n",
    "            tr_list.append(torch.tensor(0.0, device=device))\n",
    "            continue\n",
    "\n",
    "        gP_b = _get_gate_scalar(gP, b)\n",
    "        gH_b = _get_gate_scalar(gH, b)\n",
    "\n",
    "        Z = z_boltz[b, :L, :L, :]  # (L,L,dB)\n",
    "\n",
    "        gate = build_token_gate(La, Lb, Lp_, Lh_, gP_b, gH_b, device=device)  # (L,)\n",
    "        Zg = Z * gate.view(L, 1, 1) * gate.view(1, L, 1)  # (L,L,dB)\n",
    "\n",
    "        # symmetrise (per channel, same effect as your numpy loop)\n",
    "        Zsym = 0.5 * (Zg + Zg.transpose(0, 1))  # (L,L,dB)\n",
    "\n",
    "        # diagonal entries (L,dB)\n",
    "        diag = torch.diagonal(Zsym, dim1=0, dim2=1)\n",
    "\n",
    "        # metrics (averaged across positions and channels)\n",
    "        diag_mean_mean = diag.mean()                            # scalar\n",
    "        trace_over_L   = diag.sum(dim=0).mean() / (L + eps)      # scalar\n",
    "\n",
    "        # off diagonal mean across all entries and channels\n",
    "        mask = ~torch.eye(L, device=device, dtype=torch.bool)\n",
    "        offdiag_mean_mean = Zsym[mask, :].mean()                 # scalar\n",
    "\n",
    "        diag_list.append(diag_mean_mean)\n",
    "        off_list.append(offdiag_mean_mean)\n",
    "        tr_list.append(trace_over_L)\n",
    "\n",
    "    return {\n",
    "        \"raw_diag_mean_mean\": torch.stack(diag_list),         # (B,)\n",
    "        \"raw_offdiag_mean_mean\": torch.stack(off_list),       # (B,)\n",
    "        \"raw_trace_over_L\": torch.stack(tr_list),             # (B,)\n",
    "    }\n",
    "\n",
    "\n",
    "def residual_zstar_hist_metrics(Zstar, d, kappa_scaffold=1.0, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Compute the same style of metrics on ΔZ* = Z* - scaffold.\n",
    "\n",
    "    Zstar: (B, 2d, 2d) (will be symmetrised internally)\n",
    "    returns dict of (B,) tensors:\n",
    "      res_diag_mean_mean\n",
    "      res_offdiag_mean_mean\n",
    "      res_trace_over_n   (trace normalised by 2d)\n",
    "    \"\"\"\n",
    "    Z = 0.5 * (Zstar + Zstar.transpose(-1, -2))\n",
    "    B, n, _ = Z.shape\n",
    "    assert n == 2*d\n",
    "\n",
    "    I = torch.eye(d, device=Z.device).unsqueeze(0).expand(B, d, d)\n",
    "    Z_scaffold = torch.zeros(B, 2*d, 2*d, device=Z.device)\n",
    "    Z_scaffold[:, :d, :d] = I\n",
    "    Z_scaffold[:, d:, d:] = I\n",
    "    Z_scaffold[:, :d, d:] = kappa_scaffold * I\n",
    "    Z_scaffold[:, d:, :d] = kappa_scaffold * I\n",
    "\n",
    "    dZ = Z - Z_scaffold\n",
    "\n",
    "    diag = torch.diagonal(dZ, dim1=-2, dim2=-1)         # (B,2d)\n",
    "    res_diag_mean_mean = diag.mean(dim=-1)              # (B,)\n",
    "    res_trace_over_n   = diag.sum(dim=-1) / (2*d + eps) # (B,)\n",
    "\n",
    "    off = dZ.clone()\n",
    "    off.diagonal(dim1=-2, dim2=-1).zero_()\n",
    "    res_offdiag_mean_mean = off.sum(dim=(-1, -2)) / (n*(n-1) + eps)\n",
    "\n",
    "    return {\n",
    "        \"res_diag_mean_mean\": res_diag_mean_mean,\n",
    "        \"res_offdiag_mean_mean\": res_offdiag_mean_mean,\n",
    "        \"res_trace_over_n\": res_trace_over_n,\n",
    "    }\n",
    "\n",
    "\n",
    "def batch_mean_std(x, eps=1e-6):\n",
    "    \"\"\"\n",
    "    x: (B,) tensor\n",
    "    returns mean, std (unbiased=False)\n",
    "    \"\"\"\n",
    "    mu = x.mean()\n",
    "    std = x.std(unbiased=False) + eps\n",
    "    return mu, std\n",
    "\n",
    "\n",
    "def hist_anchor_loss_batch_moments(raw_metrics, res_metrics, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Distribution-level (batch) anchoring: match mean and std of each metric.\n",
    "    Returns scalar loss and a dict of diagnostics.\n",
    "    \"\"\"\n",
    "    losses = {}\n",
    "    total = 0.0\n",
    "\n",
    "    pairs = [\n",
    "        (\"diag\",  raw_metrics[\"raw_diag_mean_mean\"],   res_metrics[\"res_diag_mean_mean\"]),\n",
    "        (\"off\",   raw_metrics[\"raw_offdiag_mean_mean\"],res_metrics[\"res_offdiag_mean_mean\"]),\n",
    "        (\"trace\", raw_metrics[\"raw_trace_over_L\"],     res_metrics[\"res_trace_over_n\"]),\n",
    "    ]\n",
    "\n",
    "    diag_out = {}\n",
    "\n",
    "    for name, t_raw, s_res in pairs:\n",
    "        mu_t, sd_t = batch_mean_std(t_raw, eps=eps)\n",
    "        mu_s, sd_s = batch_mean_std(s_res, eps=eps)\n",
    "\n",
    "        l_mu = (mu_s - mu_t).pow(2)\n",
    "        l_sd = (sd_s - sd_t).pow(2)\n",
    "\n",
    "        l = l_mu + l_sd\n",
    "        total = total + l\n",
    "\n",
    "        diag_out[f\"{name}_mu_t\"] = mu_t.item()\n",
    "        diag_out[f\"{name}_sd_t\"] = sd_t.item()\n",
    "        diag_out[f\"{name}_mu_s\"] = mu_s.item()\n",
    "        diag_out[f\"{name}_sd_s\"] = sd_s.item()\n",
    "        diag_out[f\"L_{name}_mu\"] = l_mu.item()\n",
    "        diag_out[f\"L_{name}_sd\"] = l_sd.item()\n",
    "\n",
    "    return total, diag_out\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def raw_boltz_sym_frob_per_sample(\n",
    "    z_boltz, L_alpha, L_beta, L_p, L_h, gP, gH,\n",
    "    length_norm=\"sqrt\",  # \"none\" | \"sqrt\" | \"L\"\n",
    "    eps=1e-8\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns raw Frobenius norm per sample of gated+symmetrised Boltz z:\n",
    "      s_raw: (B,)\n",
    "    \"\"\"\n",
    "    device = z_boltz.device\n",
    "    B, _, _, dB = z_boltz.shape\n",
    "    out = []\n",
    "\n",
    "    for b in range(B):\n",
    "        La = int(L_alpha[b].item())\n",
    "        Lb = int(L_beta[b].item())\n",
    "        Lp_ = int(L_p[b].item())\n",
    "        Lh_ = int(L_h[b].item())\n",
    "        L = La + Lb + Lp_ + Lh_\n",
    "\n",
    "        if L <= 0:\n",
    "            out.append(torch.tensor(0.0, device=device))\n",
    "            continue\n",
    "\n",
    "        gP_b = _get_gate_scalar(gP, b)\n",
    "        gH_b = _get_gate_scalar(gH, b)\n",
    "\n",
    "        Z = z_boltz[b, :L, :L, :]  # (L,L,dB)\n",
    "        gate = build_token_gate(La, Lb, Lp_, Lh_, gP_b, gH_b, device=device)  # (L,)\n",
    "\n",
    "        Zg = Z * gate.view(L,1,1) * gate.view(1,L,1)\n",
    "        Zsym = 0.5 * (Zg + Zg.transpose(0,1))\n",
    "\n",
    "        frob = Zsym.flatten().norm(p=2)  # scalar\n",
    "\n",
    "        if length_norm == \"sqrt\":\n",
    "            frob = frob / (L**0.5 + eps)\n",
    "        elif length_norm == \"L\":\n",
    "            frob = frob / (L + eps)\n",
    "        # else \"none\": do nothing\n",
    "\n",
    "        out.append(frob)\n",
    "\n",
    "    return torch.stack(out, dim=0)  # (B,)\n",
    "\n",
    "\n",
    "def residual_zstar_frob_per_sample(Zstar, d, kappa_scaffold=1.0, length_norm=\"sqrt\", eps=1e-8):\n",
    "    \"\"\"\n",
    "    Returns Frobenius norm per sample of residual operator:\n",
    "      s_star: (B,)\n",
    "    \"\"\"\n",
    "    Z = 0.5 * (Zstar + Zstar.transpose(-1, -2))\n",
    "    B, n, _ = Z.shape\n",
    "    assert n == 2*d\n",
    "\n",
    "    I = torch.eye(d, device=Z.device).unsqueeze(0).expand(B, d, d)\n",
    "    Z_scaffold = torch.zeros(B, 2*d, 2*d, device=Z.device)\n",
    "    Z_scaffold[:, :d, :d] = I\n",
    "    Z_scaffold[:, d:, d:] = I\n",
    "    Z_scaffold[:, :d, d:] = kappa_scaffold * I\n",
    "    Z_scaffold[:, d:, :d] = kappa_scaffold * I\n",
    "\n",
    "    dZ = Z - Z_scaffold\n",
    "    frob = dZ.flatten(1).norm(dim=1)  # (B,)\n",
    "\n",
    "    if length_norm == \"sqrt\":\n",
    "        frob = frob / ((2*d)**0.5 + eps)\n",
    "    elif length_norm == \"L\":\n",
    "        frob = frob / (2*d + eps)\n",
    "\n",
    "    return frob\n",
    "\n",
    "\n",
    "def boltz_norm_anchor_loss(\n",
    "    z_boltz, Zstar, L_alpha, L_beta, L_p, L_h, gP, gH,\n",
    "    d, kappa_scaffold=1.0,\n",
    "    length_norm_raw=\"sqrt\", length_norm_star=\"sqrt\",\n",
    "    eps=1e-8\n",
    "):\n",
    "    \"\"\"\n",
    "    True anchoring: match per-sample residual Zstar norm to raw Boltz gated+sym norm.\n",
    "    Uses log-space MSE for stability.\n",
    "\n",
    "    Returns:\n",
    "      L_norm (scalar), diagnostics dict\n",
    "    \"\"\"\n",
    "    s_raw  = raw_boltz_sym_frob_per_sample(\n",
    "        z_boltz, L_alpha, L_beta, L_p, L_h, gP, gH,\n",
    "        length_norm=length_norm_raw, eps=eps\n",
    "    )  # (B,)\n",
    "\n",
    "    s_star = residual_zstar_frob_per_sample(\n",
    "        Zstar, d=d, kappa_scaffold=kappa_scaffold,\n",
    "        length_norm=length_norm_star, eps=eps\n",
    "    )  # (B,)\n",
    "\n",
    "    # avoid log(0)\n",
    "    s_raw  = s_raw.clamp_min(1e-8)\n",
    "    s_star = s_star.clamp_min(1e-8)\n",
    "\n",
    "    L_norm = (s_star.log() - s_raw.log()).pow(2).mean()\n",
    "\n",
    "    diag = {\n",
    "        \"raw_frob_mean\": float(s_raw.mean().item()),\n",
    "        \"star_frob_mean\": float(s_star.mean().item()),\n",
    "        \"raw_frob_p95\": float(s_raw.quantile(0.95).item()),\n",
    "        \"star_frob_p95\": float(s_star.quantile(0.95).item()),\n",
    "        \"log_frob_absdiff_mean\": float((s_star.log() - s_raw.log()).abs().mean().item()),\n",
    "    }\n",
    "    return L_norm, diag\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79caf85",
   "metadata": {},
   "source": [
    "c) Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f97dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularisers\n",
    "\n",
    "def vicreg_variance(u, gamma=1.0, eps=1e-4):\n",
    "    \"\"\"\n",
    "    u: (B, d) embeddings for one modality (TCR or pMHC)\n",
    "    gamma: minimum desired std per dimension (VICReg uses gamma=1.0)\n",
    "    \"\"\"\n",
    "    B, d = u.shape\n",
    "    u_centered = u - u.mean(dim=0, keepdim=True)            # (B, d), make mean 0\n",
    "    #std = torch.sqrt(u_centered.var(dim=0) + eps)           # (d,)\n",
    "    std = torch.sqrt(u_centered.var(dim=0, unbiased=False) + eps)\n",
    "\n",
    "    # (1/d) * sum_j ReLU(gamma - std_j)\n",
    "    var_loss = F.relu(gamma - std).mean()\n",
    "    return var_loss\n",
    "\n",
    "\n",
    "def vicreg_covariance(u, eps=1e-4):\n",
    "    \"\"\"\n",
    "    u: (B, d) embeddings for one modality (TCR or pMHC)\n",
    "    Returns (1/d^2) * sum_{j!=k} Cov(u_j, u_k)^2\n",
    "    \"\"\"\n",
    "    B, d = u.shape\n",
    "    u_centered = u - u.mean(dim=0, keepdim=True)            # (B, d)\n",
    "\n",
    "    # covariance matrix C = (u^T u) / (B-1)\n",
    "    cov = (u_centered.T @ u_centered) / (B - 1)             # (d, d)\n",
    "\n",
    "    # zero diag, keep off-diagonals, as we don't want diagonal terms (variances)\n",
    "    diag = torch.diag(cov)\n",
    "    cov_off = cov - torch.diag_embed(diag)\n",
    "\n",
    "    cov_loss = (cov_off ** 2).sum() / (d * d)\n",
    "    return cov_loss\n",
    "\n",
    "\n",
    "def Z_eig_floor_loss(Z, tau=-5.0, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Penalise Z if its minimum eigenvalue drops below tau.\n",
    "    For symmetric Z, eigvalsh is stable and returns real eigenvalues.\n",
    "\n",
    "    Z: (B, n, n), assumed (approximately) symmetric.\n",
    "    tau: floor. If lambda_min < tau => penalty (tau - lambda_min)^2.\n",
    "    Returns: (loss_scalar, lambda_min_mean, lambda_min_min)\n",
    "    \"\"\"\n",
    "    # Force symmetry for numerical safety (cheap)\n",
    "    Zsym = 0.5 * (Z + Z.transpose(-1, -2))\n",
    "\n",
    "    # Eigenvalues for each sample (B, n)\n",
    "    evals = torch.linalg.eigvalsh(Zsym)\n",
    "\n",
    "    # Minimum eigenvalue per sample (B,)\n",
    "    lam_min = evals[:, 0]\n",
    "\n",
    "    # Penalise if below floor\n",
    "    penalty = torch.relu(tau - lam_min + eps).pow(2).mean()\n",
    "\n",
    "    return penalty, lam_min.mean().item(), lam_min.min().item()\n",
    "\n",
    "\n",
    "def Z_spikiness_loss(Z, target_ratio=2.0, iters=6, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Penalise if |lambda_max| dominates relative to Frobenius-scaled mean eigen magnitude.\n",
    "    Uses power iteration (cheap) on symmetric Z.\n",
    "\n",
    "    ratio = |λ_max| / (||Z||_F / sqrt(n))\n",
    "    Penalise ReLU(ratio - target_ratio)^2\n",
    "    \"\"\"\n",
    "    Zsym = 0.5 * (Z + Z.transpose(-1, -2))\n",
    "    B, n, _ = Zsym.shape\n",
    "\n",
    "    # scale proxy for \"mean eigen magnitude\"\n",
    "    frob = Zsym.flatten(1).norm(dim=1).clamp_min(eps)      # (B,)\n",
    "    mean_mag = frob / (n**0.5 + eps)                       # (B,)\n",
    "\n",
    "    # power iteration for largest magnitude eigenvalue direction\n",
    "    v = torch.randn(B, n, 1, device=Zsym.device)\n",
    "    v = v / (v.norm(dim=1, keepdim=True) + eps)\n",
    "\n",
    "    for _ in range(iters):\n",
    "        v = torch.bmm(Zsym, v)\n",
    "        v = v / (v.norm(dim=1, keepdim=True) + eps)\n",
    "\n",
    "    # Rayleigh quotient gives eigenvalue estimate\n",
    "    Zv = torch.bmm(Zsym, v)\n",
    "    lam = torch.bmm(v.transpose(1,2), Zv).squeeze(-1).squeeze(-1)  # (B,)\n",
    "    lam = lam.abs()\n",
    "\n",
    "    ratio = lam / (mean_mag + eps)\n",
    "    return torch.relu(ratio - target_ratio).pow(2).mean(), {\n",
    "        \"spike_ratio_mean\": float(ratio.mean().item()),\n",
    "        \"spike_ratio_p95\": float(ratio.quantile(0.95).item()),\n",
    "        \"lam_max_abs_mean\": float(lam.mean().item()),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "748d6f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_contrastive_hamiltonian_loss(\n",
    "    zT,\n",
    "    zPH,\n",
    "    e_hat,\n",
    "    z_boltz_batch,\n",
    "    L_alpha,\n",
    "    L_beta,\n",
    "    L_p,\n",
    "    L_h,\n",
    "    gP,\n",
    "    gH,\n",
    "    boltz_factoriser,\n",
    "    alpha=1.0,\n",
    "    beta=1.0,\n",
    "    delta=1.0,\n",
    "    gamma_var=1.0,\n",
    "    eps=1e-4,\n",
    "    return_Zstar=False,\n",
    "    use_limit_Zstar=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Base loss only:\n",
    "      L_total_base = alpha * L_inv + beta * (L_var_T + L_var_PH) + delta * (L_cov_T + L_cov_PH)\n",
    "\n",
    "    Z-regularisers (anchor/spectral/isotropy) should be added in the training loop\n",
    "    because anchor needs state across steps (Z0_norm).\n",
    "    \"\"\"\n",
    "\n",
    "    device = e_hat.device\n",
    "    B, two_d = e_hat.shape\n",
    "    d = two_d // 2\n",
    "\n",
    "    # ---- 1) Boltz → Z★(n) ----\n",
    "    if use_limit_Zstar:\n",
    "        I = torch.eye(d, device=device).unsqueeze(0).expand(B, d, d)\n",
    "        Zstar = torch.zeros(B, 2*d, 2*d, device=device)\n",
    "        Zstar[:, :d, :d] = I\n",
    "        Zstar[:, :d, d:] = I\n",
    "        Zstar[:, d:, :d] = I\n",
    "        Zstar[:, d:, d:] = I\n",
    "    else:\n",
    "        Zstar = boltz_factoriser(\n",
    "            z_boltz_batch.to(device),\n",
    "            L_alpha.to(device),\n",
    "            L_beta.to(device),\n",
    "            L_p.to(device),\n",
    "            L_h.to(device),\n",
    "            gP.to(device),\n",
    "            gH.to(device),\n",
    "        )  # (B, 2d, 2d)\n",
    "\n",
    "        # enforce symmetry at loss level\n",
    "        Zstar = 0.5 * (Zstar + Zstar.transpose(-1, -2))\n",
    "\n",
    "    # # ---- 2) Hamiltonian proxy ----\n",
    "    # quad = torch.einsum(\"bi,bij,bj->b\", e_hat, Zstar, e_hat)   # (B,)\n",
    "    # H = -0.5 * quad\n",
    "    # L_inv = H.mean()\n",
    "\n",
    "    quad = torch.einsum(\"bi,bij,bj->b\", e_hat, Zstar, e_hat)  # (B,)\n",
    "    e2   = (e_hat * e_hat).sum(dim=-1).clamp_min(1e-8)       # (B,)\n",
    "    Zf   = Zstar.flatten(1).norm(dim=1).clamp_min(1e-8)      # (B,)\n",
    "\n",
    "    score = quad / (e2 * Zf)   # (B,)\n",
    "    H = -0.5 * score\n",
    "    L_inv = H.mean()\n",
    "          \n",
    "    # diagnostics (unchanged)\n",
    "    quad_mean = quad.mean()\n",
    "    quad_max  = quad.max()\n",
    "    quad_min  = quad.min()\n",
    "\n",
    "    \n",
    "    # ---- 3) VICReg variance/covariance ----\n",
    "    L_var_T  = vicreg_variance(zT,  gamma=gamma_var, eps=eps)\n",
    "    L_var_PH = vicreg_variance(zPH, gamma=gamma_var, eps=eps)\n",
    "\n",
    "    L_cov_T  = vicreg_covariance(zT,  eps=eps)\n",
    "    L_cov_PH = vicreg_covariance(zPH, eps=eps)\n",
    "\n",
    "    L_var_total = L_var_T + L_var_PH\n",
    "    L_cov_total = L_cov_T + L_cov_PH\n",
    "\n",
    "    # ---- 4) Base total ----\n",
    "    L_total_base = alpha * L_inv + beta * L_var_total + delta * L_cov_total\n",
    "\n",
    "\n",
    "    # ---- 5) Logging ----\n",
    "    alpha_L_inv = (alpha * L_inv).item()\n",
    "    beta_var    = (beta * (L_var_T + L_var_PH)).item()\n",
    "    delta_cov   = (delta * (L_cov_T + L_cov_PH)).item()\n",
    "\n",
    "    components = {\n",
    "        \"L_total_base\": L_total_base.item(),\n",
    "        \"L_inv\":        L_inv.item(),\n",
    "        \"L_var_T\":      L_var_T.item(),\n",
    "        \"L_var_PH\":     L_var_PH.item(),\n",
    "        \"L_cov_T\":      L_cov_T.item(),\n",
    "        \"L_cov_PH\":     L_cov_PH.item(),\n",
    "        \"quad_mean\": quad_mean.item(),\n",
    "        \"quad_min\":  quad_min.item(),\n",
    "        \"quad_max\":  quad_max.item(),\n",
    "        \"alpha_L_inv\": alpha_L_inv,\n",
    "        \"beta_var\": beta_var,\n",
    "        \"delta_cov\": delta_cov,\n",
    "    }\n",
    "\n",
    "    # limit-case diagnostics\n",
    "    if use_limit_Zstar:\n",
    "        cos = (zT * zPH).sum(dim=-1)  # (B,)\n",
    "        H_expected = -1.0 - cos\n",
    "        components[\"H_expected_mean\"] = H_expected.mean().item()\n",
    "        components[\"H_actual_mean\"]   = H.mean().item()\n",
    "        components[\"H_max_abs_diff\"]  = (H - H_expected).abs().max().item()\n",
    "        components[\"cos_mean\"]        = cos.mean().item()\n",
    "        components[\"H_min\"]           = H.min().item()\n",
    "        components[\"H_max\"]           = H.max().item()\n",
    "\n",
    "    if return_Zstar:\n",
    "        return L_total_base, components, Zstar\n",
    "    else:\n",
    "        return L_total_base, components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9633f782",
   "metadata": {},
   "source": [
    "Training Loop with Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "86520919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared CUDA cache.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(\"Cleared CUDA cache.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a56e9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Boltz factoriser capacity check ===\n",
      "A_T rows (model TCR capacity): 456\n",
      "A_PH rows (model pMHC capacity): 377\n",
      "Expected max L_T from manifest: 456\n",
      "Expected max L_PH from manifest: 377\n",
      "{'L_total_base': 1.9275261163711548, 'L_inv': -0.0032326322980225086, 'L_var_T': 0.9602407217025757, 'L_var_PH': 0.9705167412757874, 'L_cov_T': 6.927645017640316e-07, 'L_cov_PH': 5.437342451841687e-07, 'quad_mean': 19.354930877685547, 'quad_min': 11.658121109008789, 'quad_max': 26.75594711303711, 'alpha_L_inv': -0.0032326322980225086, 'beta_var': 1.9307575225830078, 'delta_cov': 1.2364987469482003e-06, 'L_norm_anchor': 10.66529655456543, 'norm_raw_frob_mean': 2449.7412109375, 'norm_star_frob_mean': 93.50626373291016, 'norm_raw_frob_p95': 2504.975830078125, 'norm_star_frob_p95': 95.3263931274414, 'norm_log_frob_absdiff_mean': 3.2656993865966797, 'L_spike': 0.0, 'spike_spike_ratio_mean': 0.595345139503479, 'spike_spike_ratio_p95': 0.9302937388420105, 'spike_lam_max_abs_mean': 55.880828857421875, 'L_hist_anchor': 5.758762836456299, 'hist_diag_mu_t': -1.8726768493652344, 'hist_diag_sd_t': 0.03885278105735779, 'hist_diag_mu_s': 0.3767637610435486, 'hist_diag_sd_s': 0.13841016590595245, 'hist_L_diag_mu': 5.059983253479004, 'hist_L_diag_sd': 0.00991167314350605, 'hist_off_mu_t': -0.235177144408226, 'hist_off_sd_t': 0.026954587548971176, 'hist_off_mu_s': 0.03024788573384285, 'hist_off_sd_s': 0.007754296530038118, 'hist_L_off_mu': 0.07045044749975204, 'hist_L_off_sd': 0.00036865120637230575, 'hist_trace_mu_t': -0.3987964987754822, 'hist_trace_sd_t': 0.009741906076669693, 'hist_trace_mu_s': 0.3767637610435486, 'hist_trace_sd_s': 0.13841016590595245, 'hist_L_trace_mu': 0.6014937162399292, 'hist_L_trace_sd': 0.01655552163720131, 'L_total': 18.778379440307617, 'L_eigfloor': 0.42679452896118164, 'lam_min_mean': -185.28817749023438, 'lam_min_min': -188.22817993164062, 'Znorm_max': 1529.24755859375, 'Zstar_abs_mean': 4.387029647827148, 'Zstar_max_abs': 36.97342300415039}\n",
      "{'L_total_base': 1.9026765823364258, 'L_inv': -0.010013682767748833, 'L_var_T': 0.9512358903884888, 'L_var_PH': 0.9614475965499878, 'L_cov_T': 2.189929773521726e-06, 'L_cov_PH': 4.560319666779833e-06, 'quad_mean': 60.32759475708008, 'quad_min': 39.43248748779297, 'quad_max': 78.35426330566406, 'alpha_L_inv': -0.010013682767748833, 'beta_var': 1.9126834869384766, 'delta_cov': 6.750249667675234e-06, 'L_norm_anchor': 10.570795059204102, 'norm_raw_frob_mean': 2431.609375, 'norm_star_frob_mean': 94.17643737792969, 'norm_raw_frob_p95': 2480.82080078125, 'norm_star_frob_p95': 96.69510650634766, 'norm_log_frob_absdiff_mean': 3.251222848892212, 'L_spike': 0.0, 'spike_spike_ratio_mean': 0.4831518232822418, 'spike_spike_ratio_p95': 0.9050132632255554, 'spike_lam_max_abs_mean': 45.807838439941406, 'L_hist_anchor': 5.622329235076904, 'hist_diag_mu_t': -1.8460776805877686, 'hist_diag_sd_t': 0.02105865068733692, 'hist_diag_mu_s': 0.36776313185691833, 'hist_diag_sd_s': 0.21842214465141296, 'hist_L_diag_mu': 4.901090621948242, 'hist_L_diag_sd': 0.03895235061645508, 'hist_off_mu_t': -0.24166862666606903, 'hist_off_sd_t': 0.01728798635303974, 'hist_off_mu_s': 0.003950427286326885, 'hist_off_sd_s': 0.011429759673774242, 'hist_L_off_mu': 0.06032872200012207, 'hist_L_off_sd': 3.431881850701757e-05, 'hist_trace_mu_t': -0.39147865772247314, 'hist_trace_sd_t': 0.005172803066670895, 'hist_trace_mu_s': 0.36776313185691833, 'hist_trace_sd_s': 0.21842214465141296, 'hist_L_trace_mu': 0.5764481425285339, 'hist_L_trace_sd': 0.04547528177499771, 'L_total': 18.4549560546875, 'L_eigfloor': 0.3591541051864624, 'lam_min_mean': -179.74114990234375, 'lam_min_min': -189.29896545410156, 'Znorm_max': 1555.413818359375, 'Zstar_abs_mean': 4.401063442230225, 'Zstar_max_abs': 36.122798919677734}\n",
      "{'L_total_base': 1.8854074478149414, 'L_inv': -0.011250175535678864, 'L_var_T': 0.9387168288230896, 'L_var_PH': 0.957932710647583, 'L_cov_T': 3.740755801118212e-06, 'L_cov_PH': 4.222706593282055e-06, 'quad_mean': 69.19501495361328, 'quad_min': 44.05255126953125, 'quad_max': 101.04750061035156, 'alpha_L_inv': -0.011250175535678864, 'beta_var': 1.8966495990753174, 'delta_cov': 7.963462849147618e-06, 'L_norm_anchor': 10.488972663879395, 'norm_raw_frob_mean': 2445.255859375, 'norm_star_frob_mean': 95.91248321533203, 'norm_raw_frob_p95': 2478.306396484375, 'norm_star_frob_p95': 98.6993179321289, 'norm_log_frob_absdiff_mean': 3.238553524017334, 'L_spike': 0.0, 'spike_spike_ratio_mean': 0.6318647861480713, 'spike_spike_ratio_p95': 1.2040544748306274, 'spike_lam_max_abs_mean': 60.70952606201172, 'L_hist_anchor': 4.464038372039795, 'hist_diag_mu_t': -1.8201289176940918, 'hist_diag_sd_t': 0.03855546936392784, 'hist_diag_mu_s': 0.1883225291967392, 'hist_diag_sd_s': 0.2052484154701233, 'hist_L_diag_mu': 4.033877372741699, 'hist_L_diag_sd': 0.027786536142230034, 'hist_off_mu_t': -0.21451589465141296, 'hist_off_sd_t': 0.021946851164102554, 'hist_off_mu_s': -0.033599816262722015, 'hist_off_sd_s': 0.01626492291688919, 'hist_L_off_mu': 0.032730624079704285, 'hist_L_off_sd': 3.2284307962981984e-05, 'hist_trace_mu_t': -0.3868502378463745, 'hist_trace_sd_t': 0.008302594535052776, 'hist_trace_mu_s': 0.1883225291967392, 'hist_trace_sd_s': 0.2052484154701233, 'hist_L_trace_mu': 0.33082371950149536, 'hist_L_trace_sd': 0.03878765553236008, 'L_total': 17.199901580810547, 'L_eigfloor': 0.36148300766944885, 'lam_min_mean': -180.00497436523438, 'lam_min_min': -185.85073852539062, 'Znorm_max': 1587.119140625, 'Zstar_abs_mean': 4.4927167892456055, 'Zstar_max_abs': 38.490142822265625}\n",
      "{'L_total_base': 1.8732819557189941, 'L_inv': -0.011505072005093098, 'L_var_T': 0.9336065053939819, 'L_var_PH': 0.9511688947677612, 'L_cov_T': 5.863480964762857e-06, 'L_cov_PH': 5.814954420202412e-06, 'quad_mean': 70.64680480957031, 'quad_min': 47.94007110595703, 'quad_max': 87.66923522949219, 'alpha_L_inv': -0.011505072005093098, 'beta_var': 1.8847754001617432, 'delta_cov': 1.167843583971262e-05, 'L_norm_anchor': 10.455766677856445, 'norm_raw_frob_mean': 2439.4453125, 'norm_star_frob_mean': 96.16796112060547, 'norm_raw_frob_p95': 2496.311279296875, 'norm_star_frob_p95': 98.13578033447266, 'norm_log_frob_absdiff_mean': 3.233482837677002, 'L_spike': 0.0, 'spike_spike_ratio_mean': 0.6902894973754883, 'spike_spike_ratio_p95': 1.10012686252594, 'spike_lam_max_abs_mean': 66.24430847167969, 'L_hist_anchor': 3.239164352416992, 'hist_diag_mu_t': -1.832303762435913, 'hist_diag_sd_t': 0.028472302481532097, 'hist_diag_mu_s': -0.08240613341331482, 'hist_diag_sd_s': 0.18659646809101105, 'hist_L_diag_mu': 3.0621416568756104, 'hist_L_diag_sd': 0.025003250688314438, 'hist_off_mu_t': -0.21953454613685608, 'hist_off_sd_t': 0.029766691848635674, 'hist_off_mu_s': -0.06208009272813797, 'hist_off_sd_s': 0.01616295985877514, 'hist_L_off_mu': 0.024791907519102097, 'hist_L_off_sd': 0.00018506152264308184, 'hist_trace_mu_t': -0.39001014828681946, 'hist_trace_sd_t': 0.006534675136208534, 'hist_trace_mu_s': -0.08240613341331482, 'hist_trace_sd_s': 0.18659646809101105, 'hist_L_trace_mu': 0.0946202278137207, 'hist_L_trace_sd': 0.032422248274087906, 'L_total': 15.894159317016602, 'L_eigfloor': 0.3259466290473938, 'lam_min_mean': -176.93759155273438, 'lam_min_min': -182.4840545654297, 'Znorm_max': 1572.985595703125, 'Zstar_abs_mean': 4.505736827850342, 'Zstar_max_abs': 38.833229064941406}\n",
      "{'L_total_base': 1.863919973373413, 'L_inv': -0.010413727723062038, 'L_var_T': 0.9265944957733154, 'L_var_PH': 0.9477173686027527, 'L_cov_T': 8.600819455750752e-06, 'L_cov_PH': 1.3161241440684535e-05, 'quad_mean': 64.66065979003906, 'quad_min': 30.091127395629883, 'quad_max': 88.49343872070312, 'alpha_L_inv': -0.010413727723062038, 'beta_var': 1.874311923980713, 'delta_cov': 2.176206180592999e-05, 'L_norm_anchor': 10.460861206054688, 'norm_raw_frob_mean': 2456.597900390625, 'norm_star_frob_mean': 96.77560424804688, 'norm_raw_frob_p95': 2486.0380859375, 'norm_star_frob_p95': 99.69843292236328, 'norm_log_frob_absdiff_mean': 3.234257936477661, 'L_spike': 0.0, 'spike_spike_ratio_mean': 0.5806670188903809, 'spike_spike_ratio_p95': 0.8799278140068054, 'spike_lam_max_abs_mean': 56.15590286254883, 'L_hist_anchor': 2.357177495956421, 'hist_diag_mu_t': -1.8772616386413574, 'hist_diag_sd_t': 0.027440600097179413, 'hist_diag_mu_s': -0.36909615993499756, 'hist_diag_sd_s': 0.19869455695152283, 'hist_L_diag_mu': 2.2745630741119385, 'hist_L_diag_sd': 0.029327915981411934, 'hist_off_mu_t': -0.21708673238754272, 'hist_off_sd_t': 0.0165060106664896, 'hist_off_mu_s': -0.09346036612987518, 'hist_off_sd_s': 0.015405888669192791, 'hist_L_off_mu': 0.015283478423953056, 'hist_L_off_sd': 1.2102684650017181e-06, 'hist_trace_mu_t': -0.3997402787208557, 'hist_trace_sd_t': 0.006177670322358608, 'hist_trace_mu_s': -0.36909615993499756, 'hist_trace_sd_s': 0.19869455695152283, 'hist_L_trace_mu': 0.0009390620398335159, 'hist_L_trace_sd': 0.03706275299191475, 'L_total': 14.995183944702148, 'L_eigfloor': 0.31322595477104187, 'lam_min_mean': -175.84552001953125, 'lam_min_min': -180.6468963623047, 'Znorm_max': 1599.8045654296875, 'Zstar_abs_mean': 4.534817695617676, 'Zstar_max_abs': 38.01423645019531}\n",
      "{'L_total_base': 1.8608769178390503, 'L_inv': -0.010987827554345131, 'L_var_T': 0.9253120422363281, 'L_var_PH': 0.946530282497406, 'L_cov_T': 1.1867917237395886e-05, 'L_cov_PH': 1.048404101311462e-05, 'quad_mean': 68.72329711914062, 'quad_min': 54.14535140991211, 'quad_max': 104.77388000488281, 'alpha_L_inv': -0.010987827554345131, 'beta_var': 1.871842384338379, 'delta_cov': 2.2351958250510506e-05, 'L_norm_anchor': 10.370315551757812, 'norm_raw_frob_mean': 2449.71630859375, 'norm_star_frob_mean': 97.88139343261719, 'norm_raw_frob_p95': 2492.058349609375, 'norm_star_frob_p95': 102.1429214477539, 'norm_log_frob_absdiff_mean': 3.2202253341674805, 'L_spike': 0.0, 'spike_spike_ratio_mean': 0.6340694427490234, 'spike_spike_ratio_p95': 1.0482852458953857, 'spike_lam_max_abs_mean': 62.40463638305664, 'L_hist_anchor': 1.8750473260879517, 'hist_diag_mu_t': -1.824653148651123, 'hist_diag_sd_t': 0.05142099782824516, 'hist_diag_mu_s': -0.47366759181022644, 'hist_diag_sd_s': 0.15289106965065002, 'hist_L_diag_mu': 1.8251619338989258, 'hist_L_diag_sd': 0.010296174325048923, 'hist_off_mu_t': -0.21390381455421448, 'hist_off_sd_t': 0.030115019530057907, 'hist_off_mu_s': -0.1031327098608017, 'hist_off_sd_s': 0.019134121015667915, 'hist_L_off_mu': 0.012270237319171429, 'hist_L_off_sd': 0.0001205801308969967, 'hist_trace_mu_t': -0.38823777437210083, 'hist_trace_sd_t': 0.011823599226772785, 'hist_trace_mu_s': -0.47366759181022644, 'hist_trace_sd_s': 0.15289106965065002, 'hist_L_trace_mu': 0.0072982534766197205, 'hist_L_trace_sd': 0.019900033250451088, 'L_total': 14.40260124206543, 'L_eigfloor': 0.296362042427063, 'lam_min_mean': -174.33840942382812, 'lam_min_min': -180.23095703125, 'Znorm_max': 1639.4150390625, 'Zstar_abs_mean': 4.578676223754883, 'Zstar_max_abs': 39.17433166503906}\n",
      "{'L_total_base': 1.8557411432266235, 'L_inv': -0.00830233097076416, 'L_var_T': 0.9207893013954163, 'L_var_PH': 0.9432268142700195, 'L_cov_T': 1.2412882824719418e-05, 'L_cov_PH': 1.5008969967311714e-05, 'quad_mean': 52.868228912353516, 'quad_min': 31.211458206176758, 'quad_max': 82.25475311279297, 'alpha_L_inv': -0.00830233097076416, 'beta_var': 1.864016056060791, 'delta_cov': 2.742185279203113e-05, 'L_norm_anchor': 10.285079002380371, 'norm_raw_frob_mean': 2461.23828125, 'norm_star_frob_mean': 99.64488220214844, 'norm_raw_frob_p95': 2501.977294921875, 'norm_star_frob_p95': 102.462890625, 'norm_log_frob_absdiff_mean': 3.2069807052612305, 'L_spike': 0.0, 'spike_spike_ratio_mean': 0.9424440860748291, 'spike_spike_ratio_p95': 1.406269907951355, 'spike_lam_max_abs_mean': 93.42837524414062, 'L_hist_anchor': 1.4821126461029053, 'hist_diag_mu_t': -1.8532309532165527, 'hist_diag_sd_t': 0.023241067305207253, 'hist_diag_mu_s': -0.7107573747634888, 'hist_diag_sd_s': 0.202335387468338, 'hist_L_diag_mu': 1.3052458763122559, 'hist_L_diag_sd': 0.03207477182149887, 'hist_off_mu_t': -0.21465927362442017, 'hist_off_sd_t': 0.014926185831427574, 'hist_off_mu_s': -0.13722839951515198, 'hist_off_sd_s': 0.015688830986618996, 'hist_L_off_mu': 0.005995540414005518, 'hist_L_off_sd': 5.816276598125114e-07, 'hist_trace_mu_t': -0.39454013109207153, 'hist_trace_sd_t': 0.005352004896849394, 'hist_trace_mu_s': -0.7107573747634888, 'hist_trace_sd_s': 0.202335387468338, 'hist_L_trace_mu': 0.09999334812164307, 'hist_L_trace_sd': 0.03880245238542557, 'L_total': 13.92564868927002, 'L_eigfloor': 0.30271589756011963, 'lam_min_mean': -174.8961639404297, 'lam_min_min': -179.11846923828125, 'Znorm_max': 1650.2354736328125, 'Zstar_abs_mean': 4.6866230964660645, 'Zstar_max_abs': 38.49822235107422}\n",
      "{'L_total_base': 1.8584258556365967, 'L_inv': -0.010723140090703964, 'L_var_T': 0.9238865375518799, 'L_var_PH': 0.945238471031189, 'L_cov_T': 1.4798897609580308e-05, 'L_cov_PH': 9.149283869192004e-06, 'quad_mean': 67.86186218261719, 'quad_min': 28.444469451904297, 'quad_max': 83.62836456298828, 'alpha_L_inv': -0.010723140090703964, 'beta_var': 1.8691250085830688, 'delta_cov': 2.3948181478772312e-05, 'L_norm_anchor': 10.262727737426758, 'norm_raw_frob_mean': 2430.250732421875, 'norm_star_frob_mean': 98.73746490478516, 'norm_raw_frob_p95': 2480.6005859375, 'norm_star_frob_p95': 102.26126098632812, 'norm_log_frob_absdiff_mean': 3.203437328338623, 'L_spike': 0.0, 'spike_spike_ratio_mean': 0.8242849707603455, 'spike_spike_ratio_p95': 1.1608240604400635, 'spike_lam_max_abs_mean': 81.6810073852539, 'L_hist_anchor': 1.3500168323516846, 'hist_diag_mu_t': -1.8667120933532715, 'hist_diag_sd_t': 0.03218143433332443, 'hist_diag_mu_s': -0.7930341958999634, 'hist_diag_sd_s': 0.14805744588375092, 'hist_L_diag_mu': 1.1527842283248901, 'hist_L_diag_sd': 0.0134272500872612, 'hist_off_mu_t': -0.22781990468502045, 'hist_off_sd_t': 0.0184315238147974, 'hist_off_mu_s': -0.14397990703582764, 'hist_off_sd_s': 0.015099196694791317, 'hist_L_off_mu': 0.007029145024716854, 'hist_L_off_sd': 1.1104403711215127e-05, 'hist_trace_mu_t': -0.39668363332748413, 'hist_trace_sd_t': 0.007802730426192284, 'hist_trace_mu_s': -0.7930341958999634, 'hist_trace_sd_s': 0.14805744588375092, 'hist_L_trace_mu': 0.15709376335144043, 'hist_L_trace_sd': 0.01967138610780239, 'L_total': 13.738685607910156, 'L_eigfloor': 0.26751548051834106, 'lam_min_mean': -171.59291076660156, 'lam_min_min': -178.03915405273438, 'Znorm_max': 1650.658447265625, 'Zstar_abs_mean': 4.603570938110352, 'Zstar_max_abs': 37.65966033935547}\n",
      "{'L_total_base': 1.8600748777389526, 'L_inv': -0.01016889326274395, 'L_var_T': 0.9215830564498901, 'L_var_PH': 0.9486392140388489, 'L_cov_T': 1.6654672435834073e-05, 'L_cov_PH': 4.792178060597507e-06, 'quad_mean': 64.994384765625, 'quad_min': 41.8875732421875, 'quad_max': 112.33775329589844, 'alpha_L_inv': -0.01016889326274395, 'beta_var': 1.8702223300933838, 'delta_cov': 2.144685095117893e-05, 'L_norm_anchor': 10.23086166381836, 'norm_raw_frob_mean': 2444.82763671875, 'norm_star_frob_mean': 99.82389831542969, 'norm_raw_frob_p95': 2495.919677734375, 'norm_star_frob_p95': 104.04595947265625, 'norm_log_frob_absdiff_mean': 3.198518753051758, 'L_spike': 0.0, 'spike_spike_ratio_mean': 1.0901799201965332, 'spike_spike_ratio_p95': 1.5016372203826904, 'spike_lam_max_abs_mean': 108.77742004394531, 'L_hist_anchor': 1.187092900276184, 'hist_diag_mu_t': -1.854173183441162, 'hist_diag_sd_t': 0.04401656985282898, 'hist_diag_mu_s': -0.9600398540496826, 'hist_diag_sd_s': 0.20754551887512207, 'hist_L_diag_mu': 0.7994744181632996, 'hist_L_diag_sd': 0.026741717010736465, 'hist_off_mu_t': -0.2126108556985855, 'hist_off_sd_t': 0.0205761156976223, 'hist_off_mu_s': -0.1638060361146927, 'hist_off_sd_s': 0.020838603377342224, 'hist_L_off_mu': 0.0023819103371351957, 'hist_L_off_sd': 6.889978010349296e-08, 'hist_trace_mu_t': -0.3949868977069855, 'hist_trace_sd_t': 0.009530631825327873, 'hist_trace_mu_s': -0.9600398540496826, 'hist_trace_sd_s': 0.20754551887512207, 'hist_L_trace_mu': 0.3192848861217499, 'hist_L_trace_sd': 0.039209894835948944, 'L_total': 13.563882827758789, 'L_eigfloor': 0.2858537435531616, 'lam_min_mean': -173.24395751953125, 'lam_min_min': -179.50155639648438, 'Znorm_max': 1686.83447265625, 'Zstar_abs_mean': 4.673368453979492, 'Zstar_max_abs': 39.867454528808594}\n",
      "{'L_total_base': 1.8620657920837402, 'L_inv': -0.008705131709575653, 'L_var_T': 0.934168815612793, 'L_var_PH': 0.9365819096565247, 'L_cov_T': 1.133495334215695e-05, 'L_cov_PH': 8.901221917767543e-06, 'quad_mean': 57.03424072265625, 'quad_min': 31.582923889160156, 'quad_max': 84.93364715576172, 'alpha_L_inv': -0.008705131709575653, 'beta_var': 1.8707506656646729, 'delta_cov': 2.0236175259924494e-05, 'L_norm_anchor': 10.096863746643066, 'norm_raw_frob_mean': 2449.071533203125, 'norm_star_frob_mean': 102.13839721679688, 'norm_raw_frob_p95': 2489.268310546875, 'norm_star_frob_p95': 106.31108856201172, 'norm_log_frob_absdiff_mean': 3.177386522293091, 'L_spike': 0.0, 'spike_spike_ratio_mean': 1.1151210069656372, 'spike_spike_ratio_p95': 1.4480839967727661, 'spike_lam_max_abs_mean': 113.62397003173828, 'L_hist_anchor': 1.1265004873275757, 'hist_diag_mu_t': -1.8517975807189941, 'hist_diag_sd_t': 0.04346040263772011, 'hist_diag_mu_s': -1.2024033069610596, 'hist_diag_sd_s': 0.17967848479747772, 'hist_L_diag_mu': 0.4217129349708557, 'hist_L_diag_sd': 0.01855536736547947, 'hist_off_mu_t': -0.2273753583431244, 'hist_off_sd_t': 0.028051380068063736, 'hist_off_mu_s': -0.17547297477722168, 'hist_off_sd_s': 0.02110517956316471, 'hist_L_off_mu': 0.0026938575319945812, 'hist_L_off_sd': 4.824970164918341e-05, 'hist_trace_mu_t': -0.3933435380458832, 'hist_trace_sd_t': 0.009641953743994236, 'hist_trace_mu_s': -1.2024033069610596, 'hist_trace_sd_s': 0.17967848479747772, 'hist_L_trace_mu': 0.6545776724815369, 'hist_L_trace_sd': 0.028912419453263283, 'L_total': 13.411410331726074, 'L_eigfloor': 0.32598012685775757, 'lam_min_mean': -176.93728637695312, 'lam_min_min': -183.37075805664062, 'Znorm_max': 1706.1829833984375, 'Zstar_abs_mean': 4.792500019073486, 'Zstar_max_abs': 39.935462951660156}\n",
      "{'L_total_base': 1.853606939315796, 'L_inv': -0.0078037818893790245, 'L_var_T': 0.922493577003479, 'L_var_PH': 0.9388946294784546, 'L_cov_T': 1.152850927610416e-05, 'L_cov_PH': 1.10325199784711e-05, 'quad_mean': 50.92404556274414, 'quad_min': 19.125999450683594, 'quad_max': 76.57864379882812, 'alpha_L_inv': -0.0078037818893790245, 'beta_var': 1.8613882064819336, 'delta_cov': 2.256102925457526e-05, 'L_norm_anchor': 10.053470611572266, 'norm_raw_frob_mean': 2426.15576171875, 'norm_star_frob_mean': 101.86029815673828, 'norm_raw_frob_p95': 2488.071533203125, 'norm_star_frob_p95': 106.33317565917969, 'norm_log_frob_absdiff_mean': 3.1705822944641113, 'L_spike': 0.0, 'spike_spike_ratio_mean': 0.6372343897819519, 'spike_spike_ratio_p95': 1.1327141523361206, 'spike_lam_max_abs_mean': 65.35842895507812, 'L_hist_anchor': 1.2647277116775513, 'hist_diag_mu_t': -1.8794121742248535, 'hist_diag_sd_t': 0.03626808151602745, 'hist_diag_mu_s': -1.3864076137542725, 'hist_diag_sd_s': 0.1768142282962799, 'hist_L_diag_mu': 0.24305349588394165, 'hist_L_diag_sd': 0.019753217697143555, 'hist_off_mu_t': -0.22019177675247192, 'hist_off_sd_t': 0.022985342890024185, 'hist_off_mu_s': -0.1862090528011322, 'hist_off_sd_s': 0.022744758054614067, 'hist_L_off_mu': 0.0011548255570232868, 'hist_L_off_sd': 5.7881063497688956e-08, 'hist_trace_mu_t': -0.4004431962966919, 'hist_trace_sd_t': 0.007579635828733444, 'hist_trace_mu_s': -1.3864076137542725, 'hist_trace_sd_s': 0.1768142282962799, 'hist_L_trace_mu': 0.9721258282661438, 'hist_L_trace_sd': 0.028640346601605415, 'L_total': 13.452558517456055, 'L_eigfloor': 0.28075289726257324, 'lam_min_mean': -172.85543823242188, 'lam_min_min': -178.67901611328125, 'Znorm_max': 1720.54931640625, 'Zstar_abs_mean': 4.756525993347168, 'Zstar_max_abs': 40.15480422973633}\n",
      "{'L_total_base': 1.8610732555389404, 'L_inv': -0.009236030280590057, 'L_var_T': 0.9286890029907227, 'L_var_PH': 0.9415945410728455, 'L_cov_T': 1.5599754988215864e-05, 'L_cov_PH': 9.99916301225312e-06, 'quad_mean': 60.5174446105957, 'quad_min': 41.12542724609375, 'quad_max': 83.28515625, 'alpha_L_inv': -0.009236030280590057, 'beta_var': 1.870283603668213, 'delta_cov': 2.5598918000468984e-05, 'L_norm_anchor': 10.044089317321777, 'norm_raw_frob_mean': 2445.0771484375, 'norm_star_frob_mean': 102.79081726074219, 'norm_raw_frob_p95': 2512.449951171875, 'norm_star_frob_p95': 106.30467224121094, 'norm_log_frob_absdiff_mean': 3.1691675186157227, 'L_spike': 0.0, 'spike_spike_ratio_mean': 1.093421459197998, 'spike_spike_ratio_p95': 1.4128737449645996, 'spike_lam_max_abs_mean': 112.77645874023438, 'L_hist_anchor': 1.5440869331359863, 'hist_diag_mu_t': -1.8463577032089233, 'hist_diag_sd_t': 0.04238780215382576, 'hist_diag_mu_s': -1.564967393875122, 'hist_diag_sd_s': 0.2400112897157669, 'hist_L_diag_mu': 0.079180508852005, 'hist_L_diag_sd': 0.03905504569411278, 'hist_off_mu_t': -0.21959728002548218, 'hist_off_sd_t': 0.021196018904447556, 'hist_off_mu_s': -0.19448518753051758, 'hist_off_sd_s': 0.0167524553835392, 'hist_L_off_mu': 0.0006306171999312937, 'hist_L_off_sd': 1.974525730474852e-05, 'hist_trace_mu_t': -0.3933664858341217, 'hist_trace_sd_t': 0.010768278501927853, 'hist_trace_mu_s': -1.564967393875122, 'hist_trace_sd_s': 0.2400112897157669, 'hist_L_trace_mu': 1.3726487159729004, 'hist_L_trace_sd': 0.05255235731601715, 'L_total': 13.738225936889648, 'L_eigfloor': 0.2889760136604309, 'lam_min_mean': -173.67507934570312, 'lam_min_min': -177.78369140625, 'Znorm_max': 1715.3182373046875, 'Zstar_abs_mean': 4.810922622680664, 'Zstar_max_abs': 40.08356475830078}\n",
      "{'L_total_base': 1.8690658807754517, 'L_inv': -0.011702144518494606, 'L_var_T': 0.925549328327179, 'L_var_PH': 0.9551742076873779, 'L_cov_T': 3.7983660149620846e-05, 'L_cov_PH': 6.570336154254619e-06, 'quad_mean': 79.43973541259766, 'quad_min': 52.059600830078125, 'quad_max': 101.50747680664062, 'alpha_L_inv': -0.011702144518494606, 'beta_var': 1.880723476409912, 'delta_cov': 4.455399539438076e-05, 'L_norm_anchor': 9.891777038574219, 'norm_raw_frob_mean': 2454.7705078125, 'norm_star_frob_mean': 105.72508239746094, 'norm_raw_frob_p95': 2507.04638671875, 'norm_star_frob_p95': 107.87898254394531, 'norm_log_frob_absdiff_mean': 3.1450703144073486, 'L_spike': 0.0, 'spike_spike_ratio_mean': 1.085533857345581, 'spike_spike_ratio_p95': 1.1393444538116455, 'spike_lam_max_abs_mean': 114.92547607421875, 'L_hist_anchor': 1.4617353677749634, 'hist_diag_mu_t': -1.8773701190948486, 'hist_diag_sd_t': 0.03366156667470932, 'hist_diag_mu_s': -1.530949592590332, 'hist_diag_sd_s': 0.19293256103992462, 'hist_L_diag_mu': 0.12000717967748642, 'hist_L_diag_sd': 0.02536725252866745, 'hist_off_mu_t': -0.22075718641281128, 'hist_off_sd_t': 0.011960916221141815, 'hist_off_mu_s': -0.21581101417541504, 'hist_off_sd_s': 0.020136142149567604, 'hist_L_off_mu': 2.4464619855280034e-05, 'hist_L_off_sd': 6.683432002319023e-05, 'hist_trace_mu_t': -0.3988404870033264, 'hist_trace_sd_t': 0.00692576402798295, 'hist_trace_mu_s': -1.530949592590332, 'hist_trace_sd_s': 0.19293256103992462, 'hist_L_trace_mu': 1.281671166419983, 'hist_L_trace_sd': 0.03459852933883667, 'L_total': 13.522566795349121, 'L_eigfloor': 0.29998844861984253, 'lam_min_mean': -174.6561737060547, 'lam_min_min': -177.57907104492188, 'Znorm_max': 1728.695556640625, 'Zstar_abs_mean': 4.9711408615112305, 'Zstar_max_abs': 38.190704345703125}\n"
     ]
    }
   ],
   "source": [
    "##### ------------------ TRAINING WITH Z CONSTRAINTS AND PLOTS ------------------\n",
    "\n",
    "# ---- Hyperparams ----\n",
    "B, L_T_pad, D = emb_T.shape\n",
    "rL = 8\n",
    "rD = 16\n",
    "d  = 128\n",
    "\n",
    "eps = 1e-8  # you use eps in normalisation; define it explicitly\n",
    "\n",
    "# # EMA + scale loss\n",
    "# logZf_ema = None\n",
    "# ema = 0.99\n",
    "# lambda_scale = 0.01   # start tiny, tune up slowly\n",
    "\n",
    "lambda_norm_anchor = 1.0      # start 0.1–1.0\n",
    "lambda_spike = 1e-4           # start tiny; increase if needed\n",
    "target_ratio = 2.0            # 1.5–3.0 typical\n",
    "\n",
    "\n",
    "R_PH = 0.7\n",
    "gP = torch.tensor(R_PH ** 0.5, device=device)        # scalar gate\n",
    "gH = torch.tensor((1.0 - R_PH) ** 0.5, device=device)\n",
    "\n",
    "alpha = 1.0 # The variance was dominating the loss, so this needs to scale appropriately\n",
    "beta  = 1.0\n",
    "delta = 1.0\n",
    "gamma_var = 1.0\n",
    "\n",
    "\n",
    "# ---- Z eigenvalue floor ----\n",
    "lambda_eigfloor = 1e-4    # start small; you may need 1e-1 if L_inv still runs away\n",
    "tau_eigfloor    = -120.0    # floor on smallest eigenvalue (tune this)\n",
    "# setting to very small to fix training instability\n",
    "\n",
    "# ---- Global max lengths ----\n",
    "L_T_max = train_dataset.emb_T.shape[1]\n",
    "L_P_max = train_dataset.emb_P.shape[1]\n",
    "L_H_max = train_dataset.emb_H.shape[1]\n",
    "\n",
    "# ---- Models ----\n",
    "tcr_factorised  = ESMFactorisedEncoder(D, rL, rD, d, L_max=L_T_max).to(device)\n",
    "pmhc_factorised = PMHCFactorisedEncoder(\n",
    "    D, rL, rD, d,\n",
    "    L_P_max=L_P_max,\n",
    "    L_H_max=L_H_max,\n",
    "    R_PH=R_PH\n",
    ").to(device)\n",
    "\n",
    "boltz_factoriser = BoltzFactorised(\n",
    "    dB=128,\n",
    "    rB=16,\n",
    "    rT=8,\n",
    "    rPH=8,\n",
    "    d=d,\n",
    "    L_max=L_T_max_boltz,\n",
    "    L_PH_max=L_PH_max_boltz,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "print(\"=== Boltz factoriser capacity check ===\")\n",
    "print(\"A_T rows (model TCR capacity):\", boltz_factoriser.A_T.shape[0])\n",
    "print(\"A_PH rows (model pMHC capacity):\", boltz_factoriser.A_PH.shape[0])\n",
    "\n",
    "print(\"Expected max L_T from manifest:\", L_T_max_boltz)\n",
    "print(\"Expected max L_PH from manifest:\", L_PH_max_boltz)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": tcr_factorised.parameters(),   \"lr\": 1e-3},\n",
    "    {\"params\": pmhc_factorised.parameters(),  \"lr\": 1e-3},\n",
    "    {\"params\": boltz_factoriser.parameters(), \"lr\": 1e-5},\n",
    "])\n",
    "\n",
    "\n",
    "for step, (batch, boltz_batch) in enumerate(zip(train_loader, boltz_loader)):\n",
    "\n",
    "    # -----------------------\n",
    "    # 1) SEQUENCE SIDE\n",
    "    # -----------------------\n",
    "    emb_T  = batch[\"emb_T\"].to(device)\n",
    "    mask_T = batch[\"mask_T\"].to(device)\n",
    "    emb_P  = batch[\"emb_P\"].to(device)\n",
    "    mask_P = batch[\"mask_P\"].to(device)\n",
    "    emb_H  = batch[\"emb_H\"].to(device)\n",
    "    mask_H = batch[\"mask_H\"].to(device)\n",
    "\n",
    "    zT  = tcr_factorised(emb_T, mask_T)\n",
    "    zT  = zT / (zT.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "    zPH = pmhc_factorised(emb_P, mask_P, emb_H, mask_H)\n",
    "    zPH = zPH / (zPH.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "    e_hat = torch.cat([zT, zPH], dim=-1)\n",
    "\n",
    "    # -----------------------\n",
    "    # 2) BOLTZ SIDE (MOVE ONCE)\n",
    "    # -----------------------\n",
    "    z_boltz = boltz_batch[\"z\"].to(device)\n",
    "    L_p     = boltz_batch[\"pep_len\"].to(device)\n",
    "    L_alpha = boltz_batch[\"tcra_len\"].to(device)\n",
    "    L_beta  = boltz_batch[\"tcrb_len\"].to(device)\n",
    "    L_h     = boltz_batch[\"hla_len\"].to(device)\n",
    "\n",
    "    # -----------------------\n",
    "    # 3) Base loss forward + Zstar (student)\n",
    "    # -----------------------\n",
    "    loss_base, components, Zstar = non_contrastive_hamiltonian_loss(\n",
    "        zT=zT, zPH=zPH, e_hat=e_hat,\n",
    "        z_boltz_batch=z_boltz,\n",
    "        L_alpha=L_alpha, L_beta=L_beta, L_p=L_p, L_h=L_h,\n",
    "        gP=gP, gH=gH,\n",
    "        boltz_factoriser=boltz_factoriser,\n",
    "        alpha=alpha, beta=beta, delta=delta, gamma_var=gamma_var,\n",
    "        eps=1e-4,\n",
    "        return_Zstar=True,\n",
    "        use_limit_Zstar=False,\n",
    "    )\n",
    "\n",
    "    # --- True norm anchoring to raw Boltz (per sample) ---\n",
    "    L_norm_anchor, norm_diag = boltz_norm_anchor_loss(\n",
    "        z_boltz=z_boltz,\n",
    "        Zstar=Zstar,\n",
    "        L_alpha=L_alpha, L_beta=L_beta, L_p=L_p, L_h=L_h,\n",
    "        gP=gP, gH=gH,\n",
    "        d=d,\n",
    "        kappa_scaffold=KAPPA_SCAFFOLD,\n",
    "        length_norm_raw=\"sqrt\",\n",
    "        length_norm_star=\"sqrt\",\n",
    "    )\n",
    "\n",
    "    components.update({\n",
    "        \"L_norm_anchor\": float(L_norm_anchor.item()),\n",
    "        **{f\"norm_{k}\": v for k, v in norm_diag.items()}\n",
    "    })\n",
    "\n",
    "    # compute spikiness \n",
    "    L_spike, spike_diag = Z_spikiness_loss(Zstar, target_ratio=target_ratio)\n",
    "\n",
    "    components.update({\n",
    "        \"L_spike\": float(L_spike.item()),\n",
    "        **{f\"spike_{k}\": v for k, v in spike_diag.items()}\n",
    "    })\n",
    "\n",
    "    \n",
    "    # -----------------------\n",
    "    # (NEW) Boltz histogram anchoring on residual ΔZ*\n",
    "    # -----------------------\n",
    "    KAPPA_SCAFFOLD = 1.0  # because add I_d in cross blocks\n",
    "\n",
    "    raw_m = raw_boltz_sym_hist_metrics(\n",
    "        z_boltz=z_boltz,\n",
    "        L_alpha=L_alpha, L_beta=L_beta, L_p=L_p, L_h=L_h,\n",
    "        gP=gP, gH=gH\n",
    "    )\n",
    "\n",
    "    res_m = residual_zstar_hist_metrics(\n",
    "        Zstar=Zstar,\n",
    "        d=d,\n",
    "        kappa_scaffold=KAPPA_SCAFFOLD\n",
    "    )\n",
    "\n",
    "    L_hist_anchor, hist_diag = hist_anchor_loss_batch_moments(raw_m, res_m)\n",
    "\n",
    "    lambda_hist_anchor = 1.0  # start 0.1 -> 1.0 -> 10.0 if needed\n",
    "\n",
    "    # Logging\n",
    "    components.update({\n",
    "        \"L_hist_anchor\": float(L_hist_anchor.item()),\n",
    "        **{f\"hist_{k}\": v for k, v in hist_diag.items()}\n",
    "    })\n",
    "\n",
    "   \n",
    "    L_eigfloor_raw, lam_min_mean, lam_min_min = Z_eig_floor_loss(Zstar, tau=tau_eigfloor)\n",
    "    L_eigfloor = lambda_eigfloor * L_eigfloor_raw\n",
    "\n",
    "    \n",
    "    #loss = loss_base + lambda_hist_anchor * L_hist_anchor + L_eigfloor\n",
    "    loss = (\n",
    "    loss_base\n",
    "    + lambda_hist_anchor * L_hist_anchor\n",
    "    + L_eigfloor\n",
    "    + lambda_norm_anchor * L_norm_anchor\n",
    "    + lambda_spike * L_spike\n",
    ")\n",
    "\n",
    "\n",
    "    # Logging\n",
    "    Znorm_per_sample = Zstar.flatten(1).norm(dim=1)\n",
    "    components.update({\n",
    "        \"L_total\": loss.item(),        \n",
    "        \"L_eigfloor\": L_eigfloor.item(),\n",
    "        \"lam_min_mean\": lam_min_mean,\n",
    "        \"lam_min_min\": lam_min_min,\n",
    "        \"Znorm_max\": Znorm_per_sample.max().item(),\n",
    "        \"Zstar_abs_mean\": Zstar.abs().mean().item(),\n",
    "        \"Zstar_max_abs\": Zstar.abs().max().item(),\n",
    "    })\n",
    "\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(boltz_factoriser.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    print(components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bc9bb4",
   "metadata": {},
   "source": [
    "Evaluation on Positives vs Negatives After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "937500a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a CPU copy of the boltz factoriser for evaluation\n",
    "boltz_factoriser_cpu = copy.deepcopy(boltz_factoriser).to(\"cpu\")\n",
    "boltz_factoriser_cpu.eval()\n",
    "\n",
    "\n",
    "gP_cpu = gP.detach().to(\"cpu\")\n",
    "gH_cpu = gH.detach().to(\"cpu\")\n",
    "\n",
    "# optional: free as much GPU as possible now\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a468f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "boltz_loader_eval = DataLoader(\n",
    "    dataset_original, batch_size=1, shuffle=False, collate_fn=boltz_collate_fn, num_workers=0\n",
    ")\n",
    "\n",
    "negatives_loader_eval = DataLoader(\n",
    "    dataset_negatives, batch_size=1, shuffle=False, collate_fn=boltz_collate_fn, num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5c602e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1200 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------- Evaluation on Positives/Negatives ---------------\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "\n",
    "\n",
    "\n",
    "def compute_zstar_metrics_selected_cpu(Zstar_cpu, d, kappa=0.0, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Zstar_cpu: (B, 2d, 2d) on CPU\n",
    "    d: latent dim (so matrix is 2d x 2d)\n",
    "    kappa: identity weight used in cross-blocks (your identity_K)\n",
    "    \"\"\"\n",
    "    Z = 0.5 * (Zstar_cpu + Zstar_cpu.transpose(-1, -2))\n",
    "    B, n, _ = Z.shape\n",
    "    assert n == 2*d, f\"Expected n=2d, got n={n}, d={d}\"\n",
    "\n",
    "    # ---------- Original metrics ----------\n",
    "    diag = torch.diagonal(Z, dim1=-2, dim2=-1)     # (B, n)\n",
    "    diag_mean_mean = diag.mean(dim=-1)             # (B,)\n",
    "    z_trace = diag.sum(dim=-1)                     # (B,)\n",
    "\n",
    "    off = Z.clone()\n",
    "    off.diagonal(dim1=-2, dim2=-1).zero_()\n",
    "    offdiag_mean_mean = off.sum(dim=(-1, -2)) / (n*(n-1) + eps)\n",
    "\n",
    "    evals = torch.linalg.eigvalsh(Z)               # (B, n)\n",
    "    abs_evals = evals.abs()\n",
    "    denom = abs_evals.sum(dim=-1) + eps\n",
    "    top1 = abs_evals[:, -1] / denom\n",
    "    top2 = abs_evals[:, -2] / denom\n",
    "\n",
    "    # ---------- New metric 1: crossblock_offdiag_mean ----------\n",
    "    Z_tph = Z[:, :d, d:]                           # (B, d, d)\n",
    "    Z_tph_off = Z_tph.clone()\n",
    "    Z_tph_off.diagonal(dim1=-2, dim2=-1).zero_()   # remove kappa*I contribution\n",
    "    crossblock_offdiag_mean = Z_tph_off.sum(dim=(-1, -2)) / (d*(d-1) + eps)  # (B,)\n",
    "\n",
    "    # ---------- New metric 2: residual_offdiag_mean_mean ----------\n",
    "    # Build scaffold: [[I, kappa I], [kappa I, I]]\n",
    "    I = torch.eye(d, device=Z.device).unsqueeze(0).expand(B, d, d)\n",
    "    Z_scaffold = torch.zeros(B, 2*d, 2*d, device=Z.device)\n",
    "    Z_scaffold[:, :d, :d] = I\n",
    "    Z_scaffold[:, d:, d:] = I\n",
    "    Z_scaffold[:, :d, d:] = kappa * I\n",
    "    Z_scaffold[:, d:, :d] = kappa * I\n",
    "\n",
    "    Z_res = Z - Z_scaffold\n",
    "    Z_res_off = Z_res.clone()\n",
    "    Z_res_off.diagonal(dim1=-2, dim2=-1).zero_()\n",
    "    residual_offdiag_mean_mean = Z_res_off.sum(dim=(-1, -2)) / (n*(n-1) + eps)\n",
    "\n",
    "    return {\n",
    "        \"diag_mean_mean\": diag_mean_mean,\n",
    "        \"offdiag_mean_mean\": offdiag_mean_mean,\n",
    "        \"z_eig_top1_share\": top1,\n",
    "        \"z_eig_top2_share\": top2,\n",
    "        \"z_trace\": z_trace,\n",
    "        \"crossblock_offdiag_mean\": crossblock_offdiag_mean,\n",
    "        \"residual_offdiag_mean_mean\": residual_offdiag_mean_mean,\n",
    "    }\n",
    "\n",
    "\n",
    "def collect_metrics_cpu(boltz_loader, boltz_factoriser_cpu, gP_cpu, gH_cpu, d, kappa=0.0, max_batches=None):\n",
    "    keys = [\n",
    "        \"diag_mean_mean\",\n",
    "        \"offdiag_mean_mean\",\n",
    "        \"z_eig_top1_share\",\n",
    "        \"z_eig_top2_share\",\n",
    "        \"z_trace\",\n",
    "        \"crossblock_offdiag_mean\",\n",
    "        \"residual_offdiag_mean_mean\",\n",
    "    ]\n",
    "    out = {k: [] for k in keys}\n",
    "\n",
    "    boltz_factoriser_cpu.eval()\n",
    "    with torch.no_grad():\n",
    "        for bi, boltz_batch in enumerate(boltz_loader):\n",
    "            if max_batches is not None and bi >= max_batches:\n",
    "                break\n",
    "\n",
    "            z_boltz = boltz_batch[\"z\"].float()\n",
    "            L_p     = boltz_batch[\"pep_len\"].long()\n",
    "            L_alpha = boltz_batch[\"tcra_len\"].long()\n",
    "            L_beta  = boltz_batch[\"tcrb_len\"].long()\n",
    "            L_h     = boltz_batch[\"hla_len\"].long()\n",
    "\n",
    "            Zstar = boltz_factoriser_cpu(z_boltz, L_alpha, L_beta, L_p, L_h, gP_cpu, gH_cpu)\n",
    "\n",
    "            metrics = compute_zstar_metrics_selected_cpu(Zstar, d=d, kappa=kappa)\n",
    "\n",
    "            for k, v in metrics.items():\n",
    "                out[k].append(v.numpy())\n",
    "\n",
    "            del z_boltz, L_p, L_alpha, L_beta, L_h, Zstar, metrics\n",
    "\n",
    "    for k in out:\n",
    "        out[k] = np.concatenate(out[k], axis=0) if len(out[k]) else np.array([])\n",
    "    return out\n",
    "\n",
    "\n",
    "def plot_histograms_full(metrics_pos, metrics_neg, bins=60, suptitle=\"Z★ features — full dataset (CPU)\"):\n",
    "    keys = [\n",
    "        \"diag_mean_mean\",\n",
    "        \"offdiag_mean_mean\",\n",
    "        \"crossblock_offdiag_mean\",\n",
    "        \"residual_offdiag_mean_mean\",\n",
    "        \"z_eig_top1_share\",\n",
    "        \"z_eig_top2_share\",\n",
    "        \"z_trace\",\n",
    "    ]\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    rows = 3\n",
    "    cols = 3\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, k in enumerate(keys):\n",
    "        ax = axes[i]\n",
    "        ax.hist(metrics_pos[k], bins=bins, alpha=0.6, label=\"Positives\")\n",
    "        ax.hist(metrics_neg[k], bins=bins, alpha=0.6, label=\"Negatives\")\n",
    "        ax.set_title(k)\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.legend()\n",
    "\n",
    "    for j in range(len(keys), rows*cols):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    fig.suptitle(suptitle, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "d = 128\n",
    "kappa = 0.2  # identity_K you used\n",
    "\n",
    "metrics_pos_full = collect_metrics_cpu(boltz_loader_eval, boltz_factoriser_cpu, gP_cpu, gH_cpu, d=d, kappa=kappa)\n",
    "metrics_neg_full = collect_metrics_cpu(negatives_loader_eval, boltz_factoriser_cpu, gP_cpu, gH_cpu, d=d, kappa=kappa)\n",
    "\n",
    "\n",
    "plot_histograms_full(metrics_pos_full, metrics_neg_full, bins=60,\n",
    "                     suptitle=f\"Z★ features — Positives vs Negatives (kappa={kappa}, CPU)\")\n",
    "\n",
    "\n",
    "\n",
    "# took 2 mins to plot 130 data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabaa254",
   "metadata": {},
   "source": [
    "Training Loop without Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8770f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ---- old functions to try and stabilise/regularise Zstar ----\n",
    "\n",
    "# # Z spectral variance loss - similar to VICreg variance but for Zstar\n",
    "# def Z_spectral_variance_loss(Z, target_ratio=1.5, iters=5, eps=1e-8):\n",
    "#     \"\"\"\n",
    "#     Encourages the top eigenvalue not to dominate the spectrum.\n",
    "#     target_ratio ≈ lambda_max / mean_lambda\n",
    "#     \"\"\"\n",
    "#     B, n, _ = Z.shape\n",
    "\n",
    "#     # Frobenius norm gives sum of squared eigenvalues\n",
    "#     frob = Z.flatten(1).norm(dim=1)  # (B,)\n",
    "#     mean_lambda = frob / (n ** 0.5 + eps)\n",
    "\n",
    "#     # Power iteration for largest eigenvalue\n",
    "#     v = torch.randn(B, n, 1, device=Z.device)\n",
    "#     v = v / (v.norm(dim=1, keepdim=True) + eps)\n",
    "\n",
    "#     for _ in range(iters):\n",
    "#         v = torch.bmm(Z, v)\n",
    "#         v = v / (v.norm(dim=1, keepdim=True) + eps)\n",
    "\n",
    "#     Zv = torch.bmm(Z, v)\n",
    "#     lambda_max = torch.bmm(v.transpose(1,2), Zv).squeeze(-1).squeeze(-1)\n",
    "#     lambda_max = lambda_max.abs()\n",
    "\n",
    "#     ratio = lambda_max / (mean_lambda + eps)\n",
    "\n",
    "#     # Penalise if ratio too large\n",
    "#     return torch.relu(ratio - target_ratio).pow(2).mean()\n",
    "\n",
    "\n",
    "# # Z isotropy loss - similar to VICreg covariance but for Zstar\n",
    "# def Z_isotropy_loss(Z, num_vec=4, eps=1e-8):\n",
    "#     \"\"\"\n",
    "#     Penalises directional anisotropy of Z.\n",
    "#     \"\"\"\n",
    "#     B, n, _ = Z.shape\n",
    "#     norms = []\n",
    "\n",
    "#     for _ in range(num_vec):\n",
    "#         v = torch.randn(B, n, device=Z.device)\n",
    "#         v = v / (v.norm(dim=1, keepdim=True) + eps)\n",
    "\n",
    "#         Zv = torch.bmm(Z, v.unsqueeze(-1)).squeeze(-1)\n",
    "#         norms.append(Zv.norm(dim=1))\n",
    "\n",
    "#     norms = torch.stack(norms, dim=1)\n",
    "#     return norms.var(dim=1).mean()\n",
    "\n",
    "\n",
    "# # third regulariser - anchor Zstar norms to initial Zstar norms\n",
    "# def Z_anchor_loss(Z, Z0_norm):\n",
    "#     Znorm = Z.flatten(1).norm(dim=1).mean()\n",
    "#     return (Znorm - Z0_norm).pow(2), Znorm\n",
    "# # removing this as this anchors to the batch, so it can still run away and also only cares about magnitude \n",
    "\n",
    "\n",
    "# def operator_distill_loss(Z_student, Z_teacher):\n",
    "#     \"\"\"\n",
    "#     Z_student, Z_teacher: (B, 2d, 2d)\n",
    "#     Returns scalar: mean squared Frobenius distance per sample.\n",
    "#     \"\"\"\n",
    "#     diff = Z_student - Z_teacher                      # (B, 2d, 2d)\n",
    "#     per_sample = (diff * diff).flatten(1).mean(dim=1) # (B,) mean over entries\n",
    "#     return per_sample.mean()                          # scalar\n",
    "\n",
    "\n",
    "# def operator_distill_loss_rel(Z_student, Z_teacher, eps=1e-8):\n",
    "#     # symmetrise both to compare in same space\n",
    "#     Zs = 0.5 * (Z_student + Z_student.transpose(-1, -2))\n",
    "#     Zt = 0.5 * (Z_teacher + Z_teacher.transpose(-1, -2))\n",
    "\n",
    "#     diff2 = (Zs - Zt).pow(2).flatten(1).mean(dim=1)   # (B,)\n",
    "#     base2 = (Zt).pow(2).flatten(1).mean(dim=1) + eps  # (B,)\n",
    "#     return (diff2 / base2).mean()\n",
    "\n",
    "\n",
    "# # introducce seoarate shape only operator\n",
    "# def operator_shape_distill(Zs, Zt, eps=1e-8):\n",
    "#     Zs = 0.5 * (Zs + Zs.transpose(-1, -2))\n",
    "#     Zt = 0.5 * (Zt + Zt.transpose(-1, -2))\n",
    "\n",
    "#     s = Zs.flatten(1)\n",
    "#     t = Zt.flatten(1)\n",
    "\n",
    "#     # cosine distance in operator space\n",
    "#     s = s / (s.norm(dim=1, keepdim=True) + eps)\n",
    "#     t = t / (t.norm(dim=1, keepdim=True) + eps)\n",
    "#     return (1.0 - (s * t).sum(dim=1)).mean()\n",
    "\n",
    "\n",
    "# # maybe won't include - to preserve histogram relationships\n",
    "# def operator_stats_distill(Zs, Zt, eps=1e-8):\n",
    "#     Zs = 0.5 * (Zs + Zs.transpose(-1, -2))\n",
    "#     Zt = 0.5 * (Zt + Zt.transpose(-1, -2))\n",
    "\n",
    "#     B, n, _ = Zs.shape\n",
    "#     diag_mask = torch.eye(n, device=Zs.device).bool()\n",
    "#     off_mask  = ~diag_mask\n",
    "\n",
    "#     ds = Zs[:, diag_mask].mean(dim=1)\n",
    "#     dt = Zt[:, diag_mask].mean(dim=1)\n",
    "#     os = Zs[:, off_mask].mean(dim=1)\n",
    "#     ot = Zt[:, off_mask].mean(dim=1)\n",
    "\n",
    "#     # normalise by teacher scale so “mean” errors are comparable across runs\n",
    "#     nt = Zt.flatten(1).norm(dim=1) + eps\n",
    "#     return (((ds - dt)/nt)**2 + ((os - ot)/nt)**2).mean()\n",
    "\n",
    "\n",
    "\n",
    "# # introduce separate magnitude only operator\n",
    "# def operator_scale_distill(Zs, Zt, eps=1e-8):\n",
    "#     Zs = 0.5 * (Zs + Zs.transpose(-1, -2))\n",
    "#     Zt = 0.5 * (Zt + Zt.transpose(-1, -2))\n",
    "\n",
    "#     ns = Zs.flatten(1).norm(dim=1) + eps\n",
    "#     nt = Zt.flatten(1).norm(dim=1) + eps\n",
    "#     # log ratio is symmetric + stable\n",
    "#     return ((ns.log() - nt.log())**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0a7ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "=== FIRST BATCH (pre-step) ===\n",
      "{'L_total_base': 0.8405921459197998, 'L_inv': -1.0792791843414307, 'L_var_T': 0.9572292566299438, 'L_var_PH': 0.962637722492218, 'L_cov_T': 9.38673224482045e-07, 'L_cov_PH': 3.3443009215261554e-06, 'L_total': 0.8416640758514404, 'L_anchor': 0.0010719199199229479, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 32.74018859863281, 'Z0_mean': 32.74018859863281, 'Znorm_max': 33.624874114990234, 'Zstar_abs_mean': 0.07694073766469955, 'Zstar_max_abs': 1.5871294736862183}\n",
      "zT norm mean: 1.0\n",
      "zPH norm mean: 1.0\n",
      "grad ||tcr||: 3.052505971247148\n",
      "grad ||pmhc||: 2.700382748012117\n",
      "grad ||boltz||: 9.091772057260457\n",
      "term_A mean/max: 0.97406405210495 1.1129586696624756\n",
      "term_B mean/max: 1.0660457611083984 1.3093974590301514\n",
      "term_C mean/max: 0.11844874918460846 0.19927363097667694\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 32.74018859863281 33.624874114990234\n",
      "Z0 mean: 32.74018859863281\n",
      "{'L_total_base': 0.8405921459197998, 'L_inv': -1.0792791843414307, 'L_var_T': 0.9572292566299438, 'L_var_PH': 0.962637722492218, 'L_cov_T': 9.38673224482045e-07, 'L_cov_PH': 3.3443009215261554e-06, 'L_total': 0.8416640758514404, 'L_anchor': 0.0010719199199229479, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 32.74018859863281, 'Z0_mean': 32.74018859863281, 'Znorm_max': 33.624874114990234, 'Zstar_abs_mean': 0.07694073766469955, 'Zstar_max_abs': 1.5871294736862183, 'L_eigfloor': 0.0, 'lam_min_mean': -2.6681227684020996, 'lam_min_min': -2.9029853343963623}\n",
      "1\n",
      "term_A mean/max: 1.6516116857528687 1.7059895992279053\n",
      "term_B mean/max: 1.327394723892212 1.4982490539550781\n",
      "term_C mean/max: 0.8329610824584961 1.1441190242767334\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 33.163631439208984 34.013065338134766\n",
      "Z0 mean: 32.744422912597656\n",
      "{'L_total_base': 0.02022930420935154, 'L_inv': -1.9059836864471436, 'L_var_T': 0.9612693190574646, 'L_var_PH': 0.9649421572685242, 'L_cov_T': 7.106071961970883e-07, 'L_cov_PH': 8.032955065573333e-07, 'L_total': 0.020229483023285866, 'L_anchor': 1.7930383933162375e-07, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 33.163631439208984, 'Z0_mean': 32.744422912597656, 'Znorm_max': 34.013065338134766, 'Zstar_abs_mean': 0.07850518077611923, 'Zstar_max_abs': 1.549753189086914, 'L_eigfloor': 0.0, 'lam_min_mean': -2.7415013313293457, 'lam_min_min': -2.936993360519409}\n",
      "2\n",
      "term_A mean/max: 2.1334218978881836 2.423257350921631\n",
      "term_B mean/max: 1.4162551164627075 1.6262035369873047\n",
      "term_C mean/max: 1.3757026195526123 1.5683155059814453\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 33.100006103515625 33.81802749633789\n",
      "Z0 mean: 32.747982025146484\n",
      "{'L_total_base': -0.53678959608078, 'L_inv': -2.4626898765563965, 'L_var_T': 0.9616833329200745, 'L_var_PH': 0.9642139673233032, 'L_cov_T': 9.954044344340218e-07, 'L_cov_PH': 1.9191277260688366e-06, 'L_total': -0.5367894768714905, 'L_anchor': 1.2643940294765343e-07, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 33.100006103515625, 'Z0_mean': 32.747982025146484, 'Znorm_max': 33.81802749633789, 'Zstar_abs_mean': 0.07816262543201447, 'Zstar_max_abs': 1.5425664186477661, 'L_eigfloor': 0.0, 'lam_min_mean': -2.7164196968078613, 'lam_min_min': -2.8454580307006836}\n",
      "3\n",
      "term_A mean/max: 2.465498685836792 2.822991371154785\n",
      "term_B mean/max: 1.5811463594436646 1.7697714567184448\n",
      "term_C mean/max: 1.9658377170562744 2.109330654144287\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 33.28099060058594 34.160675048828125\n",
      "Z0 mean: 32.75331115722656\n",
      "{'L_total_base': -1.07386314868927, 'L_inv': -3.0062413215637207, 'L_var_T': 0.9629569053649902, 'L_var_PH': 0.9694193601608276, 'L_cov_T': 4.6903022621336277e-07, 'L_cov_PH': 1.4197587461239891e-06, 'L_total': -1.073862910270691, 'L_anchor': 2.8409814945007383e-07, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 33.28099060058594, 'Z0_mean': 32.75331115722656, 'Znorm_max': 34.160675048828125, 'Zstar_abs_mean': 0.07878193259239197, 'Zstar_max_abs': 1.559786319732666, 'L_eigfloor': 0.0, 'lam_min_mean': -2.738642692565918, 'lam_min_min': -2.9694342613220215}\n",
      "4\n",
      "term_A mean/max: 2.778252601623535 2.959306478500366\n",
      "term_B mean/max: 1.7103443145751953 1.8809230327606201\n",
      "term_C mean/max: 2.3296191692352295 2.404386520385742\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 32.940673828125 33.71785354614258\n",
      "Z0 mean: 32.755184173583984\n",
      "{'L_total_base': -1.4694640636444092, 'L_inv': -3.4091081619262695, 'L_var_T': 0.9688433408737183, 'L_var_PH': 0.9707998037338257, 'L_cov_T': 2.8672195639956044e-07, 'L_cov_PH': 6.592965746676782e-07, 'L_total': -1.4694640636444092, 'L_anchor': 3.510476886958713e-08, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 32.940673828125, 'Z0_mean': 32.755184173583984, 'Znorm_max': 33.71785354614258, 'Zstar_abs_mean': 0.07754464447498322, 'Zstar_max_abs': 1.6029866933822632, 'L_eigfloor': 0.0, 'lam_min_mean': -2.662823438644409, 'lam_min_min': -2.7856431007385254}\n",
      "5\n",
      "term_A mean/max: 3.235260486602783 3.522480010986328\n",
      "term_B mean/max: 1.8438780307769775 1.9056038856506348\n",
      "term_C mean/max: 2.8282032012939453 2.952573299407959\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 33.32421875 34.362361907958984\n",
      "Z0 mean: 32.76087188720703\n",
      "{'L_total_base': -2.011643886566162, 'L_inv': -3.9536709785461426, 'L_var_T': 0.9710299968719482, 'L_var_PH': 0.9709963798522949, 'L_cov_T': 1.8348998764849966e-07, 'L_cov_PH': 6.130096608103486e-07, 'L_total': -2.011643648147583, 'L_anchor': 3.238003500882769e-07, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 33.32421875, 'Z0_mean': 32.76087188720703, 'Znorm_max': 34.362361907958984, 'Zstar_abs_mean': 0.0790587067604065, 'Zstar_max_abs': 1.6105067729949951, 'L_eigfloor': 0.0, 'lam_min_mean': -2.721193790435791, 'lam_min_min': -2.9732136726379395}\n",
      "6\n",
      "term_A mean/max: 3.8071131706237793 4.097410202026367\n",
      "term_B mean/max: 2.0490493774414062 2.168205738067627\n",
      "term_C mean/max: 3.2917985916137695 3.413231372833252\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 33.862632751464844 34.54997253417969\n",
      "Z0 mean: 32.771888732910156\n",
      "{'L_total_base': -2.624014139175415, 'L_inv': -4.573980808258057, 'L_var_T': 0.9757389426231384, 'L_var_PH': 0.9742271900177002, 'L_cov_T': 8.584758859342401e-08, 'L_cov_PH': 3.532970822561765e-07, 'L_total': -2.6240129470825195, 'L_anchor': 1.21387699891784e-06, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 33.862632751464844, 'Z0_mean': 32.771888732910156, 'Znorm_max': 34.54997253417969, 'Zstar_abs_mean': 0.08070990443229675, 'Zstar_max_abs': 1.5341893434524536, 'L_eigfloor': 0.0, 'lam_min_mean': -2.8423969745635986, 'lam_min_min': -2.9876179695129395}\n",
      "7\n",
      "term_A mean/max: 3.9456307888031006 4.206984519958496\n",
      "term_B mean/max: 2.246619462966919 2.3639957904815674\n",
      "term_C mean/max: 3.6615543365478516 3.8270020484924316\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 33.76707458496094 34.643123626708984\n",
      "Z0 mean: 32.78184127807617\n",
      "{'L_total_base': -2.975107192993164, 'L_inv': -4.926902770996094, 'L_var_T': 0.97409588098526, 'L_var_PH': 0.9776995182037354, 'L_cov_T': 8.397223894007766e-08, 'L_cov_PH': 7.606799101722572e-08, 'L_total': -2.9751062393188477, 'L_anchor': 9.903949376166565e-07, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 33.76707458496094, 'Z0_mean': 32.78184127807617, 'Znorm_max': 34.643123626708984, 'Zstar_abs_mean': 0.08078661561012268, 'Zstar_max_abs': 1.6183931827545166, 'L_eigfloor': 0.0, 'lam_min_mean': -2.7795963287353516, 'lam_min_min': -2.9762284755706787}\n",
      "8\n",
      "term_A mean/max: 4.366969585418701 4.709043025970459\n",
      "term_B mean/max: 2.376633405685425 2.438462018966675\n",
      "term_C mean/max: 4.055475234985352 4.263920307159424\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 33.55878448486328 34.7598762512207\n",
      "Z0 mean: 32.789608001708984\n",
      "{'L_total_base': -3.4437170028686523, 'L_inv': -5.399538993835449, 'L_var_T': 0.9763456583023071, 'L_var_PH': 0.9794760346412659, 'L_cov_T': 5.6785665947245434e-08, 'L_cov_PH': 1.1809270006324368e-07, 'L_total': -3.443716287612915, 'L_anchor': 6.036407285137102e-07, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 33.55878448486328, 'Z0_mean': 32.789608001708984, 'Znorm_max': 34.7598762512207, 'Zstar_abs_mean': 0.0794868916273117, 'Zstar_max_abs': 1.5665712356567383, 'L_eigfloor': 0.0, 'lam_min_mean': -2.7294187545776367, 'lam_min_min': -2.976754665374756}\n",
      "9\n",
      "term_A mean/max: 5.026423454284668 5.307099342346191\n",
      "term_B mean/max: 2.6322741508483887 2.7918553352355957\n",
      "term_C mean/max: 4.511255264282227 4.659682750701904\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 34.43989562988281 35.221343994140625\n",
      "Z0 mean: 32.80611038208008\n",
      "{'L_total_base': -4.127717018127441, 'L_inv': -6.0849761962890625, 'L_var_T': 0.9780799150466919, 'L_var_PH': 0.9791795015335083, 'L_cov_T': 4.3486096501510474e-08, 'L_cov_PH': 4.194480140995438e-08, 'L_total': -4.127714157104492, 'L_anchor': 2.7234491426497698e-06, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 34.43989562988281, 'Z0_mean': 32.80611038208008, 'Znorm_max': 35.221343994140625, 'Zstar_abs_mean': 0.0825597494840622, 'Zstar_max_abs': 1.5596174001693726, 'L_eigfloor': 0.0, 'lam_min_mean': -2.873938798904419, 'lam_min_min': -3.0008862018585205}\n",
      "10\n",
      "term_A mean/max: 5.5020928382873535 5.813536167144775\n",
      "term_B mean/max: 2.753726005554199 2.859466075897217\n",
      "term_C mean/max: 5.038987636566162 5.321328163146973\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 34.36842346191406 35.0925407409668\n",
      "Z0 mean: 32.82173538208008\n",
      "{'L_total_base': -4.6829094886779785, 'L_inv': -6.647403240203857, 'L_var_T': 0.9811240434646606, 'L_var_PH': 0.9833698272705078, 'L_cov_T': 1.5075135806341677e-08, 'L_cov_PH': 3.62793599606448e-08, 'L_total': -4.6829071044921875, 'L_anchor': 2.4408220724581042e-06, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 34.36842346191406, 'Z0_mean': 32.82173538208008, 'Znorm_max': 35.0925407409668, 'Zstar_abs_mean': 0.08214471489191055, 'Zstar_max_abs': 1.6070640087127686, 'L_eigfloor': 0.0, 'lam_min_mean': -2.854515790939331, 'lam_min_min': -3.006798505783081}\n",
      "11\n",
      "term_A mean/max: 5.701440811157227 6.21292781829834\n",
      "term_B mean/max: 2.866988182067871 3.073014259338379\n",
      "term_C mean/max: 5.397226810455322 5.655472755432129\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 33.8980712890625 34.7422981262207\n",
      "Z0 mean: 32.83250045776367\n",
      "{'L_total_base': -5.016797065734863, 'L_inv': -6.982828140258789, 'L_var_T': 0.9838137626647949, 'L_var_PH': 0.9822170734405518, 'L_cov_T': 1.3047478653049893e-08, 'L_cov_PH': 3.248511859510472e-08, 'L_total': -5.016796112060547, 'L_anchor': 1.1584990033952636e-06, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 33.8980712890625, 'Z0_mean': 32.83250045776367, 'Znorm_max': 34.7422981262207, 'Zstar_abs_mean': 0.08077168464660645, 'Zstar_max_abs': 1.616367220878601, 'L_eigfloor': 0.0, 'lam_min_mean': -2.735461711883545, 'lam_min_min': -2.912433385848999}\n",
      "12\n",
      "term_A mean/max: 5.871006488800049 6.206693649291992\n",
      "term_B mean/max: 3.168187141418457 3.308495044708252\n",
      "term_C mean/max: 5.733527183532715 5.897137641906738\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Znorm mean/max: 33.589317321777344 34.13768768310547\n",
      "Z0 mean: 32.84006881713867\n",
      "{'L_total_base': -5.417607307434082, 'L_inv': -7.386360168457031, 'L_var_T': 0.9847467541694641, 'L_var_PH': 0.9840062856674194, 'L_cov_T': 1.3695691691850698e-08, 'L_cov_PH': 2.1891271018148473e-08, 'L_total': -5.417606830596924, 'L_anchor': 5.727717962145107e-07, 'L_spec': 0.0, 'L_iso': 0.0, 'Znorm_mean': 33.589317321777344, 'Z0_mean': 32.84006881713867, 'Znorm_max': 34.13768768310547, 'Zstar_abs_mean': 0.07966683804988861, 'Zstar_max_abs': 1.5408499240875244, 'L_eigfloor': 0.0, 'lam_min_mean': -2.627443313598633, 'lam_min_min': -2.703632354736328}\n"
     ]
    }
   ],
   "source": [
    "# ##### ------------------ TRAINING WITH Z CONSTRAINTS ------------------\n",
    "\n",
    "# # ---- Hyperparams ----\n",
    "# B, L_T_pad, D = emb_T.shape\n",
    "# rL = 8\n",
    "# rD = 16\n",
    "# d  = 128\n",
    "\n",
    "# eps = 1e-8  # you use eps in normalisation; define it explicitly\n",
    "\n",
    "# # rL = 8\n",
    "# # rD = 16\n",
    "# # d  = 128\n",
    "\n",
    "# R_PH = 0.7\n",
    "# gP = torch.tensor(R_PH ** 0.5, device=device)        # scalar gate\n",
    "# gH = torch.tensor((1.0 - R_PH) ** 0.5, device=device)\n",
    "\n",
    "# alpha = 1.0\n",
    "# beta  = 1.0\n",
    "# delta = 1.0\n",
    "# gamma_var = 1.0\n",
    "\n",
    "# # ---- Z regulariser weights ----\n",
    "# lambda_anchor = 1e-6\n",
    "# lambda_spec   = 0 #remove for  now\n",
    "# lambda_iso    = 0 #remove for  now\n",
    "\n",
    "# target_ratio = 1.5\n",
    "# spec_iters = 5\n",
    "# iso_num_vec = 4\n",
    "\n",
    "# # ---- Z eigenvalue floor ----\n",
    "# lambda_eigfloor = 1e-1    # start small; you may need 1e-1 if L_inv still runs away\n",
    "# tau_eigfloor    = -5.0    # floor on smallest eigenvalue (tune this)\n",
    "\n",
    "# # ---- Global max lengths ----\n",
    "# L_T_max = train_dataset.emb_T.shape[1]\n",
    "# L_P_max = train_dataset.emb_P.shape[1]\n",
    "# L_H_max = train_dataset.emb_H.shape[1]\n",
    "\n",
    "# # ---- Models ----\n",
    "# tcr_factorised  = ESMFactorisedEncoder(D, rL, rD, d, L_max=L_T_max).to(device)\n",
    "# pmhc_factorised = PMHCFactorisedEncoder(\n",
    "#     D, rL, rD, d,\n",
    "#     L_P_max=L_P_max,\n",
    "#     L_H_max=L_H_max,\n",
    "#     R_PH=R_PH\n",
    "# ).to(device)\n",
    "\n",
    "# boltz_factoriser = BoltzFactorised(\n",
    "#     dB=128,\n",
    "#     rB=16,\n",
    "#     rT=8,\n",
    "#     rPH=8,\n",
    "#     d=d,\n",
    "#     L_max=L_T_max_boltz,\n",
    "#     L_PH_max=L_PH_max_boltz,\n",
    "# ).to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam([\n",
    "#     {\"params\": tcr_factorised.parameters(),   \"lr\": 1e-3},\n",
    "#     {\"params\": pmhc_factorised.parameters(),  \"lr\": 1e-3},\n",
    "#     {\"params\": boltz_factoriser.parameters(), \"lr\": 1e-5},\n",
    "# ])\n",
    "\n",
    "# # ---- Anchored-norm state ----\n",
    "# Z0_norm = None\n",
    "# anchor_ema = 0.99  # your 0.99/0.01 EMA\n",
    "\n",
    "# for step, (batch, boltz_batch) in enumerate(zip(train_loader, boltz_loader)):\n",
    "\n",
    "#     print(step)\n",
    "\n",
    "#     # -----------------------\n",
    "#     # 1) SEQUENCE SIDE\n",
    "#     # -----------------------\n",
    "#     emb_T  = batch[\"emb_T\"].to(device)\n",
    "#     mask_T = batch[\"mask_T\"].to(device)\n",
    "#     emb_P  = batch[\"emb_P\"].to(device)\n",
    "#     mask_P = batch[\"mask_P\"].to(device)\n",
    "#     emb_H  = batch[\"emb_H\"].to(device)\n",
    "#     mask_H = batch[\"mask_H\"].to(device)\n",
    "\n",
    "#     zT  = tcr_factorised(emb_T, mask_T)                 # (B, d)\n",
    "#     zT  = zT / (zT.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "#     zPH = pmhc_factorised(emb_P, mask_P, emb_H, mask_H) # (B, d)\n",
    "#     zPH = zPH / (zPH.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "#     e_hat = torch.cat([zT, zPH], dim=-1)                # (B, 2d)\n",
    "\n",
    "#     # -----------------------\n",
    "#     # 2) BOLTZ SIDE\n",
    "#     # -----------------------\n",
    "#     z_boltz = boltz_batch[\"z\"]\n",
    "#     L_p     = boltz_batch[\"pep_len\"]\n",
    "#     L_alpha = boltz_batch[\"tcra_len\"]\n",
    "#     L_beta  = boltz_batch[\"tcrb_len\"]\n",
    "#     L_h     = boltz_batch[\"hla_len\"]\n",
    "\n",
    "#     # -----------------------\n",
    "#     # 3) Base loss forward + Zstar\n",
    "#     # -----------------------\n",
    "#     loss_base, components, Zstar = non_contrastive_hamiltonian_loss(\n",
    "#         zT=zT,\n",
    "#         zPH=zPH,\n",
    "#         e_hat=e_hat,\n",
    "#         z_boltz_batch=z_boltz,\n",
    "#         L_alpha=L_alpha,\n",
    "#         L_beta=L_beta,\n",
    "#         L_p=L_p,\n",
    "#         L_h=L_h,\n",
    "#         gP=gP,\n",
    "#         gH=gH,\n",
    "#         boltz_factoriser=boltz_factoriser,\n",
    "#         alpha=alpha,\n",
    "#         beta=beta,\n",
    "#         delta=delta,\n",
    "#         gamma_var=gamma_var,\n",
    "#         eps=1e-4,\n",
    "#         return_Zstar=True,\n",
    "#         use_limit_Zstar=False,\n",
    "#     )\n",
    "\n",
    "#     # -----------------------\n",
    "#     # 4) Z REGULARISERS (computed on SAME Zstar)\n",
    "#     # -----------------------\n",
    "#     # -----------------------\n",
    "#     # 2b) Z-regularisers (ALL computed on the SAME Zstar)\n",
    "#     # -----------------------\n",
    "\n",
    "#     # (i) anchor loss (your existing)\n",
    "#     L_anchor_raw, Znorm_mean = Z_anchor_loss(Zstar, Z0_norm if Z0_norm is not None else 0.0)\n",
    "#     # initialise/update anchor target\n",
    "#     if Z0_norm is None:\n",
    "#         Z0_norm = Znorm_mean.detach()\n",
    "#     else:\n",
    "#         Z0_norm = 0.99 * Z0_norm + 0.01 * Znorm_mean.detach()\n",
    "\n",
    "#     L_anchor = lambda_anchor * L_anchor_raw\n",
    "\n",
    "#     # (ii) spectral ratio penalty (your existing)\n",
    "#     L_spec = lambda_spec * Z_spectral_variance_loss(Zstar, target_ratio=target_ratio, iters=spec_iters)\n",
    "\n",
    "#     # (iii) isotropy penalty (your existing)\n",
    "#     L_iso = lambda_iso * Z_isotropy_loss(Zstar, num_vec=iso_num_vec)\n",
    "\n",
    "#     # (iv) NEW: eigenvalue floor penalty (prevents runaway negative curvature)\n",
    "#     L_eigfloor_raw, lam_min_mean, lam_min_min = Z_eig_floor_loss(Zstar, tau=tau_eigfloor)\n",
    "#     L_eigfloor = lambda_eigfloor * L_eigfloor_raw\n",
    "\n",
    "#     # Final loss\n",
    "#     loss = loss_base + L_anchor + L_spec + L_iso + L_eigfloor\n",
    "\n",
    "\n",
    "#     # log\n",
    "#     Znorm_per_sample = Zstar.flatten(1).norm(dim=1)  # (B,)\n",
    "#     components.update({\n",
    "#         \"L_total\":       loss.item(),\n",
    "#         \"L_anchor\":      L_anchor.item(),\n",
    "#         \"L_spec\":        L_spec.item(),\n",
    "#         \"L_iso\":         L_iso.item(),\n",
    "#         \"Znorm_mean\":    Znorm_mean.item(),\n",
    "#         \"Z0_mean\":       Z0_norm.item(),\n",
    "#         \"Znorm_max\":     Znorm_per_sample.max().item(),\n",
    "#         \"Zstar_abs_mean\": Zstar.abs().mean().item(),\n",
    "#         \"Zstar_max_abs\":  Zstar.abs().max().item(),\n",
    "#     })\n",
    "\n",
    "#     # -----------------------\n",
    "#     # 5) Pre-step prints\n",
    "#     # -----------------------\n",
    "#     if step == 0:\n",
    "#         print(\"=== FIRST BATCH (pre-step) ===\")\n",
    "#         print(components)\n",
    "#         print(\"zT norm mean:\",  zT.norm(dim=-1).mean().item())\n",
    "#         print(\"zPH norm mean:\", zPH.norm(dim=-1).mean().item())\n",
    "\n",
    "#     # -----------------------\n",
    "#     # 6) Backward\n",
    "#     # -----------------------\n",
    "#     optimizer.zero_grad(set_to_none=True)\n",
    "#     loss.backward()\n",
    "\n",
    "#     if step == 0:\n",
    "#         def grad_norm(m):\n",
    "#             tot = 0.0\n",
    "#             for p in m.parameters():\n",
    "#                 if p.grad is not None:\n",
    "#                     tot += p.grad.detach().float().norm().item()**2\n",
    "#             return tot**0.5\n",
    "\n",
    "#         print(\"grad ||tcr||:\",   grad_norm(tcr_factorised))\n",
    "#         print(\"grad ||pmhc||:\",  grad_norm(pmhc_factorised))\n",
    "#         print(\"grad ||boltz||:\", grad_norm(boltz_factoriser))\n",
    "\n",
    "#     # -----------------------\n",
    "#     # 7) Debug: decompose quadratic into blocks (same Zstar)\n",
    "#     # -----------------------\n",
    "#     d_local = zT.shape[-1]\n",
    "#     A_block = Zstar[:, :d_local, :d_local]\n",
    "#     C_block = Zstar[:, :d_local, d_local:]\n",
    "#     B_block = Zstar[:, d_local:, d_local:]\n",
    "\n",
    "#     term_A = torch.einsum(\"bi,bij,bj->b\", zT,  A_block, zT)\n",
    "#     term_B = torch.einsum(\"bi,bij,bj->b\", zPH, B_block, zPH)\n",
    "#     term_C = 2.0 * torch.einsum(\"bi,bij,bj->b\", zT, C_block, zPH)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         print(\"term_A mean/max:\", term_A.mean().item(), term_A.abs().max().item())\n",
    "#         print(\"term_B mean/max:\", term_B.mean().item(), term_B.abs().max().item())\n",
    "#         print(\"term_C mean/max:\", term_C.mean().item(), term_C.abs().max().item())\n",
    "\n",
    "#         sym_err = (Zstar - Zstar.transpose(-1, -2)).abs().max().item()\n",
    "#         print(\"Zstar symmetry max|Z-ZT|:\", sym_err)\n",
    "#         print(\"Znorm mean/max:\", Znorm_mean.item(), Znorm_per_sample.max().item())\n",
    "#         print(\"Z0 mean:\", Z0_norm.item())\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         components[\"L_total_base\"] = float(loss_base.item())\n",
    "#         components[\"L_total\"]      = float(loss.item())\n",
    "\n",
    "#         components[\"L_anchor\"]     = float(L_anchor.item())\n",
    "#         components[\"L_spec\"]       = float(L_spec.item())\n",
    "#         components[\"L_iso\"]        = float(L_iso.item())\n",
    "#         components[\"L_eigfloor\"]   = float(L_eigfloor.item())\n",
    "\n",
    "#         components[\"lam_min_mean\"] = lam_min_mean\n",
    "#         components[\"lam_min_min\"]  = lam_min_min\n",
    "\n",
    "#         # Useful Z stats\n",
    "#         Zflat = Zstar.flatten(1)\n",
    "#         components[\"Zstar_abs_mean\"] = float(Zstar.abs().mean().item())\n",
    "#         components[\"Zstar_max_abs\"]  = float(Zstar.abs().max().item())\n",
    "\n",
    "\n",
    "#     print(components)\n",
    "\n",
    "#     # -----------------------\n",
    "#     # 8) Gradient clipping (after backward, before step)\n",
    "#     # -----------------------\n",
    "#     torch.nn.utils.clip_grad_norm_(boltz_factoriser.parameters(), max_norm=1.0) #was 10, but getting huge gradients\n",
    "\n",
    "#     # -----------------------\n",
    "#     # 9) Optimiser step\n",
    "#     # -----------------------\n",
    "#     optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862961c1",
   "metadata": {},
   "source": [
    "##### Previous Z constraints and Limit Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e88fbe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_contrastive_hamiltonian_loss(\n",
    "    zT,\n",
    "    zPH,\n",
    "    e_hat,\n",
    "    z_boltz_batch,\n",
    "    L_alpha,\n",
    "    L_beta,\n",
    "    L_p,\n",
    "    L_h,\n",
    "    gP,\n",
    "    gH,\n",
    "    boltz_factoriser,\n",
    "    alpha=1.0,\n",
    "    beta=1.0,\n",
    "    delta=1.0,\n",
    "    gamma_var=1.0,\n",
    "    eps=1e-4,\n",
    "    return_Zstar=False,\n",
    "    use_limit_Zstar=False,          # limit case\n",
    "    lambda_Z=0.0,                   # regulariser weight\n",
    "):\n",
    "    \"\"\"\n",
    "    zT, zPH    : (B, d) TCR and pMHC embeddings (the two halves of e_hat)\n",
    "    e_hat      : (B, 2d) concatenation [zT || zPH]\n",
    "    z_boltz_*  : Boltz inputs, as in your BoltzFactorised.forward\n",
    "    boltz_factoriser : instance of BoltzFactorised\n",
    "\n",
    "    alpha : weight on invariance (Hamiltonian) term\n",
    "    beta  : weight on variance terms\n",
    "    delta : weight on covariance terms\n",
    "    gamma_var : VICReg variance threshold (usually 1.0)\n",
    "    \"\"\"\n",
    "\n",
    "    device = e_hat.device\n",
    "    B, two_d = e_hat.shape\n",
    "    d = two_d // 2\n",
    "\n",
    "\n",
    "    # ---- 1) Boltz → Z★(n) for each sample ---- (B, 2d, 2d)\n",
    "    \n",
    "    # Try to simulate limit case - all identity matrixes\n",
    "    if use_limit_Zstar:\n",
    "        # paper limit: Z* = [[I, I],[I, I]]\n",
    "        I = torch.eye(d, device=device).unsqueeze(0).expand(B, d, d)\n",
    "        Zstar = torch.zeros(B, 2*d, 2*d, device=device)\n",
    "        Zstar[:, :d, :d] = I\n",
    "        Zstar[:, :d, d:] = I\n",
    "        Zstar[:, d:, :d] = I\n",
    "        Zstar[:, d:, d:] = I\n",
    "    else:\n",
    "        Zstar = boltz_factoriser(\n",
    "        z_boltz_batch.to(device),\n",
    "        L_alpha.to(device),\n",
    "        L_beta.to(device),\n",
    "        L_p.to(device),\n",
    "        L_h.to(device),\n",
    "        gP.to(device),\n",
    "        gH.to(device),\n",
    "    )  # (B, 2d, 2d)\n",
    "        # symmetrise output every forward pass\n",
    "        # even if this is done in the factoriser, doing it here is cheap and guarantees it\n",
    "        Zstar = 0.5 * (Zstar + Zstar.transpose(-1, -2))\n",
    "        \n",
    "\n",
    "    # ---- 2) Hamiltonian proxy per sample: H^(n) = -1/2 e^T Z* e ----\n",
    "    # einsum: 'bi,bij,bj->b' gives e^T Z* e for each n\n",
    "    quad = torch.einsum(\"bi,bij,bj->b\", e_hat, Zstar, e_hat)  # (B,)\n",
    "    H = -0.5 * quad                                           # (B,)\n",
    "\n",
    "    if use_limit_Zstar:\n",
    "        cos = (zT * zPH).sum(dim=-1)          # (B,) since both unit norm\n",
    "        H_expected = -1.0 - cos\n",
    "        max_diff = (H - H_expected).abs().max().item()\n",
    "\n",
    "\n",
    "    # Invariance term: average Hamiltonian over batch\n",
    "    L_inv = H.mean()\n",
    "\n",
    "    # ---- 3) Variance & covariance terms per block (TCR and pMHC) ----\n",
    "    # zT and zPH are each (B, d)\n",
    "    L_var_T  = vicreg_variance(zT,  gamma=gamma_var, eps=eps)\n",
    "    L_var_PH = vicreg_variance(zPH, gamma=gamma_var, eps=eps)\n",
    "\n",
    "    L_cov_T  = vicreg_covariance(zT,  eps=eps)\n",
    "    L_cov_PH = vicreg_covariance(zPH, eps=eps)\n",
    "\n",
    "    # ---- 4) Combine as in your equation (4) ----\n",
    "    L_var_total = L_var_T + L_var_PH\n",
    "    L_cov_total = L_cov_T + L_cov_PH\n",
    "\n",
    "    L_total = (\n",
    "        alpha * L_inv\n",
    "        + beta * L_var_total\n",
    "        + delta * L_cov_total\n",
    "    )\n",
    "\n",
    "    components = {\n",
    "        \"L_total\":    L_total.item(),\n",
    "        \"L_inv\":      L_inv.item(),\n",
    "        \"L_var_T\":    L_var_T.item(),\n",
    "        \"L_var_PH\":   L_var_PH.item(),\n",
    "        \"L_cov_T\":    L_cov_T.item(),\n",
    "        \"L_cov_PH\":   L_cov_PH.item(),\n",
    "    }\n",
    "\n",
    "    if use_limit_Zstar:\n",
    "        components[\"H_expected_mean\"] = H_expected.mean().item()\n",
    "        components[\"H_actual_mean\"]   = H.mean().item()\n",
    "        components[\"H_max_abs_diff\"]  = max_diff\n",
    "        components[\"cos_mean\"]        = cos.mean().item()\n",
    "        components[\"H_min\"]           = H.min().item()\n",
    "        components[\"H_max\"]           = H.max().item()\n",
    "\n",
    "\n",
    "    if return_Zstar:\n",
    "        return L_total, components, Zstar\n",
    "    else:\n",
    "        return L_total, components\n",
    "\n",
    "    #return L_total, components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c58fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "=== FIRST BATCH (pre-step) ===\n",
      "{'L_total': 5.370140075683594, 'L_inv': 3.4459939002990723, 'L_var_T': 0.9593005180358887, 'L_var_PH': 0.9648425579071045, 'L_cov_T': 7.847547749406658e-07, 'L_cov_PH': 2.0807594864891144e-06, 'L_anchor': 0.0, 'Znorm_mean': 1849.241455078125, 'Z0_mean': 1849.241455078125, 'Znorm_max': 1927.605712890625}\n",
      "zT norm mean: 1.0\n",
      "zPH norm mean: 1.0\n",
      "grad ||tcr||: 216.6576049986074\n",
      "grad ||pmhc||: 137.79171387229712\n",
      "grad ||boltz||: 737.664848521035\n",
      "term_A mean/max: -9.978446960449219 30.804811477661133\n",
      "term_B mean/max: 0.6554511785507202 8.332881927490234\n",
      "term_C mean/max: 2.431009531021118 9.845224380493164\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 5.154047012329102\n",
      "Zstar max abs: 58.64965057373047\n",
      "Znorm mean/max: 1849.241455078125 1927.605712890625\n",
      "Z0 mean: 1849.241455078125\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'approx_spectral_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 204\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZnorm mean/max:\u001b[39m\u001b[38;5;124m\"\u001b[39m, Znorm\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem(), Znorm\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZ0 mean:\u001b[39m\u001b[38;5;124m\"\u001b[39m, Z0_norm\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m--> 204\u001b[0m     lam \u001b[38;5;241m=\u001b[39m \u001b[43mapprox_spectral_norm\u001b[49m(Zstar, iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda_max approx mean/max:\u001b[39m\u001b[38;5;124m\"\u001b[39m, lam\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem(), lam\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mprint\u001b[39m(components)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'approx_spectral_norm' is not defined"
     ]
    }
   ],
   "source": [
    "##### ------------------ ADDING Z CONSTRAINTS ------------------\n",
    "\n",
    "# ---- Hyperparams ----\n",
    "B, L_T_pad, D = emb_T.shape\n",
    "rL = 8\n",
    "rD = 16\n",
    "d  = 128\n",
    "\n",
    "R_PH = 0.7\n",
    "gP_scalar = R_PH ** 0.5\n",
    "gH_scalar = (1.0 - R_PH) ** 0.5\n",
    "gP = torch.tensor(gP_scalar, device=device)   # scalar gates\n",
    "gH = torch.tensor(gH_scalar, device=device)\n",
    "\n",
    "alpha = 1.0   # weight on invariance term\n",
    "beta  = 1.0   # weight on var terms\n",
    "delta = 1.0   # weight on cov terms\n",
    "\n",
    "# ---- Get global max lengths for ESM factorised encoders ----\n",
    "L_T_max = train_dataset.emb_T.shape[1]\n",
    "L_P_max = train_dataset.emb_P.shape[1]\n",
    "L_H_max = train_dataset.emb_H.shape[1]\n",
    "\n",
    "# do this outside of the batch training loop so the weights are not reinitialised for each batch\n",
    "tcr_factorised  = ESMFactorisedEncoder(D, rL, rD, d, L_max=L_T_max).to(device)\n",
    "pmhc_factorised = PMHCFactorisedEncoder(\n",
    "    D, rL, rD, d,\n",
    "    L_P_max=L_P_max,\n",
    "    L_H_max=L_H_max,\n",
    "    R_PH=R_PH\n",
    ").to(device)\n",
    "\n",
    "# ---- Boltz factoriser (choose sensible hyperparams) ----\n",
    "# dB = channel dimension of Boltz embeddings, e.g. boltz_batch[\"z\"].shape[-1]\n",
    "#dB   = boltz_loader[0][0].shape[-1]   # hard code, this line gives an erro\n",
    "dB   = 128\n",
    "rB   = 16     \n",
    "rT   = 8       \n",
    "rPH  = 8       \n",
    "# Use values computed from manifest (defined in cell above)\n",
    "L_T_max_boltz  = L_T_max_boltz  # max TCR length (alpha+beta) across manifest\n",
    "L_PH_max_boltz = L_PH_max_boltz  # max pMHC length (pep+hla) across manifest\n",
    "\n",
    "\n",
    "boltz_factoriser = BoltzFactorised(\n",
    "    dB=dB,\n",
    "    rB=rB,\n",
    "    rT=rT,\n",
    "    rPH=rPH,\n",
    "    d=d,\n",
    "    L_max=L_T_max_boltz,\n",
    "    L_PH_max=L_PH_max_boltz,\n",
    ").to(device)\n",
    "\n",
    "# ---- Collect parameters & define optimiser ----\n",
    "params = (\n",
    "    list(tcr_factorised.parameters()) +\n",
    "    list(pmhc_factorised.parameters()) +\n",
    "    list(boltz_factoriser.parameters())\n",
    ")\n",
    "\n",
    "#optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": tcr_factorised.parameters(),  \"lr\": 1e-3},\n",
    "    {\"params\": pmhc_factorised.parameters(), \"lr\": 1e-3},\n",
    "    {\"params\": boltz_factoriser.parameters(), \"lr\": 1e-4},  # or 3e-4\n",
    "])\n",
    "\n",
    "# -----------------------\n",
    "# Anchored-norm state - bound Zstar norms to initial Zstar norms\n",
    "# -----------------------\n",
    "lambda_anchor = 1e-2   # start here; if Z still ramps up try 1e-1, if it kills learning try 1e-3\n",
    "Z0_norm = None         # will be set on first iteration (detached)\n",
    "\n",
    "for step, (batch, boltz_batch) in enumerate(zip(train_loader, boltz_loader)):\n",
    "\n",
    "    print(step)\n",
    "\n",
    "    # ---- SEQUENCE SIDE ----\n",
    "    emb_T  = batch[\"emb_T\"].to(device)\n",
    "    mask_T = batch[\"mask_T\"].to(device)\n",
    "    emb_P  = batch[\"emb_P\"].to(device)\n",
    "    mask_P = batch[\"mask_P\"].to(device)\n",
    "    emb_H  = batch[\"emb_H\"].to(device)\n",
    "    mask_H = batch[\"mask_H\"].to(device)\n",
    "\n",
    "    zT  = tcr_factorised(emb_T, mask_T)\n",
    "    zT  = zT / (zT.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "    zPH = pmhc_factorised(emb_P, mask_P, emb_H, mask_H)\n",
    "    zPH = zPH / (zPH.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "\n",
    "    e_hat = torch.cat([zT, zPH], dim=-1)  # (B, 2d)\n",
    "\n",
    "    # ---- BOLTZ SIDE ----\n",
    "    z_boltz = boltz_batch[\"z\"]\n",
    "    L_p     = boltz_batch[\"pep_len\"]\n",
    "    L_alpha = boltz_batch[\"tcra_len\"]\n",
    "    L_beta  = boltz_batch[\"tcrb_len\"]\n",
    "    L_h     = boltz_batch[\"hla_len\"]\n",
    "\n",
    "    # -----------------------\n",
    "    # 2) Loss forward (returns Zstar)\n",
    "    # -----------------------\n",
    "    loss, components, Zstar = non_contrastive_hamiltonian_loss(\n",
    "        zT=zT,\n",
    "        zPH=zPH,\n",
    "        e_hat=e_hat,\n",
    "        z_boltz_batch=z_boltz,\n",
    "        L_alpha=L_alpha,\n",
    "        L_beta=L_beta,\n",
    "        L_p=L_p,\n",
    "        L_h=L_h,\n",
    "        gP=gP,\n",
    "        gH=gH,\n",
    "        boltz_factoriser=boltz_factoriser,\n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        delta=delta,\n",
    "        gamma_var=1.0,\n",
    "        return_Zstar=True,\n",
    "        use_limit_Zstar=False,   # IMPORTANT: anchored norm is for the REAL case\n",
    "        lambda_Z=0.0,\n",
    "    )\n",
    "\n",
    "    # -----------------------\n",
    "    # 2b) Anchored-norm penalty on Zstar\n",
    "    #     L_anchor = lambda * (||Z|| - ||Z0||)^2\n",
    "    # -----------------------\n",
    "    # Per-sample Frobenius norm (B,)\n",
    "    Znorm = Zstar.flatten(1).norm(dim=1)\n",
    "\n",
    "    # Initialise anchor on first iteration\n",
    "    if Z0_norm is None:\n",
    "        Z0_norm = Znorm.mean().detach()\n",
    "    else:\n",
    "        Z0_norm = 0.99 * Z0_norm + 0.01 * Znorm.mean().detach()\n",
    "\n",
    "    L_anchor = lambda_anchor * (Znorm.mean() - Z0_norm).pow(2)\n",
    "\n",
    "    #L_anchor = lambda_anchor * (Znorm - Z0_norm).pow(2).mean()\n",
    "\n",
    "    # Add to total loss\n",
    "    loss = loss + L_anchor\n",
    "\n",
    "    # Track in components\n",
    "    components[\"L_anchor\"]   = L_anchor.item()\n",
    "    components[\"Znorm_mean\"] = Znorm.mean().item()\n",
    "    components[\"Z0_mean\"]    = Z0_norm.mean().item()\n",
    "    components[\"Znorm_max\"]  = Znorm.max().item()\n",
    "\n",
    "    # -----------------------\n",
    "    # 3) Pre-step prints\n",
    "    # -----------------------\n",
    "    if step == 0:\n",
    "        print(\"=== FIRST BATCH (pre-step) ===\")\n",
    "        print(components)\n",
    "        print(\"zT norm mean:\",  zT.norm(dim=-1).mean().item())\n",
    "        print(\"zPH norm mean:\", zPH.norm(dim=-1).mean().item())\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # 4) Backward pass\n",
    "    # -----------------------------------------\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "\n",
    "    if step == 0:\n",
    "        def grad_norm(m):\n",
    "            tot = 0.0\n",
    "            for p in m.parameters():\n",
    "                if p.grad is not None:\n",
    "                    tot += p.grad.detach().float().norm().item()**2\n",
    "            return tot**0.5\n",
    "\n",
    "        print(\"grad ||tcr||:\",   grad_norm(tcr_factorised))\n",
    "        print(\"grad ||pmhc||:\",  grad_norm(pmhc_factorised))\n",
    "        print(\"grad ||boltz||:\", grad_norm(boltz_factoriser))\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # 5) Debug: decompose quadratic into blocks\n",
    "    # -----------------------------------------\n",
    "    d = zT.shape[-1]\n",
    "    A_block = Zstar[:, :d, :d]\n",
    "    C_block = Zstar[:, :d, d:]\n",
    "    B_block = Zstar[:, d:, d:]\n",
    "\n",
    "    term_A = torch.einsum(\"bi,bij,bj->b\", zT,  A_block, zT)\n",
    "    term_B = torch.einsum(\"bi,bij,bj->b\", zPH, B_block, zPH)\n",
    "    term_C = 2.0 * torch.einsum(\"bi,bij,bj->b\", zT, C_block, zPH)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"term_A mean/max:\", term_A.mean().item(), term_A.abs().max().item())\n",
    "        print(\"term_B mean/max:\", term_B.mean().item(), term_B.abs().max().item())\n",
    "        print(\"term_C mean/max:\", term_C.mean().item(), term_C.abs().max().item())\n",
    "\n",
    "        sym_err = (Zstar - Zstar.transpose(-1, -2)).abs().max().item()\n",
    "        print(\"Zstar symmetry max|Z-ZT|:\", sym_err)\n",
    "\n",
    "        print(\"Zstar abs mean:\", Zstar.abs().mean().item())\n",
    "        print(\"Zstar max abs:\",  Zstar.abs().max().item())\n",
    "        print(\"Znorm mean/max:\", Znorm.mean().item(), Znorm.max().item())\n",
    "        print(\"Z0 mean:\", Z0_norm.mean().item())\n",
    "\n",
    "        # lam = approx_spectral_norm(Zstar, iters=3)\n",
    "        # print(\"lambda_max approx mean/max:\", lam.mean().item(), lam.max().item())\n",
    "\n",
    "    print(components)\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # 6) Gradient clipping\n",
    "    # -----------------------------------------\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(\n",
    "        boltz_factoriser.parameters(),\n",
    "        max_norm=10.0\n",
    "    )\n",
    "\n",
    "    # 7) Optimiser step\n",
    "    # -----------------------------------------\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e662b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "=== FIRST BATCH (pre-step) ===\n",
      "{'L_total': 1.0994528532028198, 'L_inv': -0.8148517608642578, 'L_var_T': 0.9488778114318848, 'L_var_PH': 0.9654240608215332, 'L_cov_T': 1.3895734127800097e-06, 'L_cov_PH': 1.3311723705555778e-06, 'H_expected_mean': -0.814851701259613, 'H_actual_mean': -0.8148517608642578, 'H_max_abs_diff': 1.1920928955078125e-07, 'cos_mean': -0.18514826893806458, 'H_min': -0.8685272932052612, 'H_max': -0.7375632524490356}\n",
      "zT norm mean: 1.0\n",
      "zPH norm mean: 1.0\n",
      "grad ||tcr||: 2.022240165751352\n",
      "grad ||pmhc||: 1.7334930506630921\n",
      "grad ||boltz||: 0.0\n",
      "term_A mean/max: 1.0 1.000000238418579\n",
      "term_B mean/max: 1.0 1.0000001192092896\n",
      "term_C mean/max: -0.37029653787612915 0.5248736143112183\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 1.0994528532028198, 'L_inv': -0.8148517608642578, 'L_var_T': 0.9488778114318848, 'L_var_PH': 0.9654240608215332, 'L_cov_T': 1.3895734127800097e-06, 'L_cov_PH': 1.3311723705555778e-06, 'H_expected_mean': -0.814851701259613, 'H_actual_mean': -0.8148517608642578, 'H_max_abs_diff': 1.1920928955078125e-07, 'cos_mean': -0.18514826893806458, 'H_min': -0.8685272932052612, 'H_max': -0.7375632524490356}\n",
      "1\n",
      "term_A mean/max: 1.0 1.0000001192092896\n",
      "term_B mean/max: 1.0 1.0000001192092896\n",
      "term_C mean/max: 0.44099241495132446 0.6235586404800415\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 0.695022463798523, 'L_inv': -1.2204961776733398, 'L_var_T': 0.949549674987793, 'L_var_PH': 0.965965986251831, 'L_cov_T': 1.985519702429883e-06, 'L_cov_PH': 9.935907883118489e-07, 'H_expected_mean': -1.2204961776733398, 'H_actual_mean': -1.2204961776733398, 'H_max_abs_diff': 1.1920928955078125e-07, 'cos_mean': 0.22049620747566223, 'H_min': -1.311779260635376, 'H_max': -1.1276122331619263}\n",
      "2\n",
      "term_A mean/max: 1.0 1.0000001192092896\n",
      "term_B mean/max: 1.0 1.0000001192092896\n",
      "term_C mean/max: 1.009225606918335 1.2536461353302002\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 0.413161963224411, 'L_inv': -1.5046128034591675, 'L_var_T': 0.9518704414367676, 'L_var_PH': 0.9659006595611572, 'L_cov_T': 1.85925841833523e-06, 'L_cov_PH': 1.8095537370754755e-06, 'H_expected_mean': -1.5046128034591675, 'H_actual_mean': -1.5046128034591675, 'H_max_abs_diff': 1.1920928955078125e-07, 'cos_mean': 0.5046128034591675, 'H_min': -1.6268231868743896, 'H_max': -1.3047724962234497}\n",
      "3\n",
      "term_A mean/max: 1.0 1.0000001192092896\n",
      "term_B mean/max: 1.0 1.0\n",
      "term_C mean/max: 1.2926286458969116 1.4493439197540283\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 0.27945810556411743, 'L_inv': -1.646314263343811, 'L_var_T': 0.9597615599632263, 'L_var_PH': 0.9660083055496216, 'L_cov_T': 7.449630174960475e-07, 'L_cov_PH': 1.8190477248936077e-06, 'H_expected_mean': -1.6463143825531006, 'H_actual_mean': -1.646314263343811, 'H_max_abs_diff': 1.1920928955078125e-07, 'cos_mean': 0.6463143229484558, 'H_min': -1.7246718406677246, 'H_max': -1.557450294494629}\n",
      "4\n",
      "term_A mean/max: 1.0 1.0000001192092896\n",
      "term_B mean/max: 1.0 1.0000001192092896\n",
      "term_C mean/max: 1.3754228353500366 1.6229746341705322\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 0.23606190085411072, 'L_inv': -1.6877113580703735, 'L_var_T': 0.958151638507843, 'L_var_PH': 0.9656203985214233, 'L_cov_T': 4.781861662195297e-07, 'L_cov_PH': 6.898515039210906e-07, 'H_expected_mean': -1.6877113580703735, 'H_actual_mean': -1.6877113580703735, 'H_max_abs_diff': 1.1920928955078125e-07, 'cos_mean': 0.6877114176750183, 'H_min': -1.8114873170852661, 'H_max': -1.5912147760391235}\n",
      "5\n",
      "term_A mean/max: 1.0 1.0000001192092896\n",
      "term_B mean/max: 1.0 1.000000238418579\n",
      "term_C mean/max: 1.5180070400238037 1.716447353363037\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 0.17450019717216492, 'L_inv': -1.7590035200119019, 'L_var_T': 0.9636013507843018, 'L_var_PH': 0.9699010848999023, 'L_cov_T': 4.685532530857017e-07, 'L_cov_PH': 8.194713814191346e-07, 'H_expected_mean': -1.7590035200119019, 'H_actual_mean': -1.7590035200119019, 'H_max_abs_diff': 2.384185791015625e-07, 'cos_mean': 0.7590035200119019, 'H_min': -1.8582236766815186, 'H_max': -1.6360281705856323}\n",
      "6\n",
      "term_A mean/max: 1.0 1.0000001192092896\n",
      "term_B mean/max: 1.0 1.0000001192092896\n",
      "term_C mean/max: 1.5879346132278442 1.6841408014297485\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 0.14879612624645233, 'L_inv': -1.7939672470092773, 'L_var_T': 0.967350423336029, 'L_var_PH': 0.9754124283790588, 'L_cov_T': 2.3451619313163974e-07, 'L_cov_PH': 2.9126061917850166e-07, 'H_expected_mean': -1.7939672470092773, 'H_actual_mean': -1.7939672470092773, 'H_max_abs_diff': 1.1920928955078125e-07, 'cos_mean': 0.7939673662185669, 'H_min': -1.8420703411102295, 'H_max': -1.6630403995513916}\n",
      "7\n",
      "term_A mean/max: 0.9999998807907104 1.0000001192092896\n",
      "term_B mean/max: 1.0 1.0000001192092896\n",
      "term_C mean/max: 1.6500672101974487 1.744586706161499\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 0.11821837723255157, 'L_inv': -1.82503342628479, 'L_var_T': 0.9703555107116699, 'L_var_PH': 0.9728957414627075, 'L_cov_T': 1.3216450156505744e-07, 'L_cov_PH': 4.2095197727576306e-07, 'H_expected_mean': -1.8250336647033691, 'H_actual_mean': -1.82503342628479, 'H_max_abs_diff': 1.1920928955078125e-07, 'cos_mean': 0.8250336050987244, 'H_min': -1.8722933530807495, 'H_max': -1.802437663078308}\n",
      "8\n",
      "term_A mean/max: 1.0 1.0000001192092896\n",
      "term_B mean/max: 1.0 1.000000238418579\n",
      "term_C mean/max: 1.7340093851089478 1.7741613388061523\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 0.08041555434465408, 'L_inv': -1.867004632949829, 'L_var_T': 0.9740890264511108, 'L_var_PH': 0.9733308553695679, 'L_cov_T': 7.456435469066491e-08, 'L_cov_PH': 2.3057424414218985e-07, 'H_expected_mean': -1.867004632949829, 'H_actual_mean': -1.867004632949829, 'H_max_abs_diff': 1.1920928955078125e-07, 'cos_mean': 0.8670046329498291, 'H_min': -1.8870806694030762, 'H_max': -1.8383238315582275}\n",
      "9\n",
      "term_A mean/max: 1.0 1.0000001192092896\n",
      "term_B mean/max: 1.0 1.000000238418579\n",
      "term_C mean/max: 1.7896214723587036 1.823689579963684\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 0.058375462889671326, 'L_inv': -1.894810676574707, 'L_var_T': 0.9758937954902649, 'L_var_PH': 0.9772922992706299, 'L_cov_T': 4.573912093519539e-08, 'L_cov_PH': 5.919613244032007e-08, 'H_expected_mean': -1.894810676574707, 'H_actual_mean': -1.894810676574707, 'H_max_abs_diff': 1.1920928955078125e-07, 'cos_mean': 0.8948107361793518, 'H_min': -1.9118448495864868, 'H_max': -1.8709397315979004}\n",
      "10\n",
      "term_A mean/max: 1.0 1.0000001192092896\n",
      "term_B mean/max: 1.0 1.000000238418579\n",
      "term_C mean/max: 1.8215527534484863 1.8722779750823975\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 0.04458432272076607, 'L_inv': -1.9107763767242432, 'L_var_T': 0.97609943151474, 'L_var_PH': 0.9792611002922058, 'L_cov_T': 5.148707060698143e-08, 'L_cov_PH': 1.1528659626947046e-07, 'H_expected_mean': -1.9107763767242432, 'H_actual_mean': -1.9107763767242432, 'H_max_abs_diff': 2.384185791015625e-07, 'cos_mean': 0.9107763767242432, 'H_min': -1.9361391067504883, 'H_max': -1.872605562210083}\n",
      "11\n",
      "term_A mean/max: 1.0 1.0000001192092896\n",
      "term_B mean/max: 1.0000001192092896 1.000000238418579\n",
      "term_C mean/max: 1.8290305137634277 1.867927074432373\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 0.0413324199616909, 'L_inv': -1.9145152568817139, 'L_var_T': 0.9787105321884155, 'L_var_PH': 0.97713702917099, 'L_cov_T': 3.031027517863549e-08, 'L_cov_PH': 1.439305350459108e-07, 'H_expected_mean': -1.9145152568817139, 'H_actual_mean': -1.9145152568817139, 'H_max_abs_diff': 1.1920928955078125e-07, 'cos_mean': 0.9145152568817139, 'H_min': -1.9339635372161865, 'H_max': -1.8904445171356201}\n",
      "12\n",
      "term_A mean/max: 0.9999999403953552 1.0\n",
      "term_B mean/max: 1.0 1.0000001192092896\n",
      "term_C mean/max: 1.873944640159607 1.9084373712539673\n",
      "Zstar symmetry max|Z-ZT|: 0.0\n",
      "Zstar abs mean: 0.0078125\n",
      "Zstar max abs: 1.0\n",
      "{'L_total': 0.02440294250845909, 'L_inv': -1.9369723796844482, 'L_var_T': 0.979761004447937, 'L_var_PH': 0.9816142320632935, 'L_cov_T': 5.7534712993856374e-08, 'L_cov_PH': 2.887480476942983e-08, 'H_expected_mean': -1.9369722604751587, 'H_actual_mean': -1.9369723796844482, 'H_max_abs_diff': 2.384185791015625e-07, 'cos_mean': 0.9369723200798035, 'H_min': -1.9542187452316284, 'H_max': -1.8996388912200928}\n"
     ]
    }
   ],
   "source": [
    "##### ------------------ LIMIT CASE ------------------\n",
    "\n",
    "# ---- Hyperparams ----\n",
    "B, L_T_pad, D = emb_T.shape\n",
    "rL = 8\n",
    "rD = 16\n",
    "d  = 128\n",
    "\n",
    "R_PH = 0.7\n",
    "gP_scalar = R_PH ** 0.5\n",
    "gH_scalar = (1.0 - R_PH) ** 0.5\n",
    "gP = torch.tensor(gP_scalar, device=device)   # scalar gates\n",
    "gH = torch.tensor(gH_scalar, device=device)\n",
    "\n",
    "alpha = 1.0   # weight on invariance term\n",
    "beta  = 1.0   # weight on var terms\n",
    "delta = 1.0   # weight on cov terms\n",
    "\n",
    "# ---- Get global max lengths for ESM factorised encoders ----\n",
    "L_T_max = train_dataset.emb_T.shape[1]\n",
    "L_P_max = train_dataset.emb_P.shape[1]\n",
    "L_H_max = train_dataset.emb_H.shape[1]\n",
    "\n",
    "# do this outside of the batch training loop so the weights are not reinitialised for each batch\n",
    "tcr_factorised  = ESMFactorisedEncoder(D, rL, rD, d, L_max=L_T_max).to(device)\n",
    "pmhc_factorised = PMHCFactorisedEncoder(\n",
    "    D, rL, rD, d,\n",
    "    L_P_max=L_P_max,\n",
    "    L_H_max=L_H_max,\n",
    "    R_PH=R_PH\n",
    ").to(device)\n",
    "\n",
    "# ---- Boltz factoriser (choose sensible hyperparams) ----\n",
    "# dB = channel dimension of Boltz embeddings, e.g. boltz_batch[\"z\"].shape[-1]\n",
    "#dB   = boltz_loader[0][0].shape[-1]   # hard code, this line gives an erro\n",
    "dB   = 128\n",
    "rB   = 16     \n",
    "rT   = 8       \n",
    "rPH  = 8       \n",
    "# Use values computed from manifest (defined in cell above)\n",
    "L_T_max_boltz  = L_T_max_boltz  # max TCR length (alpha+beta) across manifest\n",
    "L_PH_max_boltz = L_PH_max_boltz  # max pMHC length (pep+hla) across manifest\n",
    "\n",
    "\n",
    "boltz_factoriser = BoltzFactorised(\n",
    "    dB=dB,\n",
    "    rB=rB,\n",
    "    rT=rT,\n",
    "    rPH=rPH,\n",
    "    d=d,\n",
    "    L_max=L_T_max_boltz,\n",
    "    L_PH_max=L_PH_max_boltz,\n",
    ").to(device)\n",
    "\n",
    "# ---- Collect parameters & define optimiser ----\n",
    "params = (\n",
    "    list(tcr_factorised.parameters()) +\n",
    "    list(pmhc_factorised.parameters()) +\n",
    "    list(boltz_factoriser.parameters())\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "\n",
    "#for batch, boltz_batch in zip(train_loader, boltz_loader):\n",
    "for step, (batch, boltz_batch) in enumerate(zip(train_loader, boltz_loader)):\n",
    "\n",
    "    print(step)\n",
    "\n",
    "    # ---- SEQUENCE SIDE ----\n",
    "    emb_T  = batch[\"emb_T\"].to(device)      # (B, L_T_pad, D)\n",
    "    mask_T = batch[\"mask_T\"].to(device)\n",
    "    emb_P  = batch[\"emb_P\"].to(device)\n",
    "    mask_P = batch[\"mask_P\"].to(device)\n",
    "    emb_H  = batch[\"emb_H\"].to(device)\n",
    "    mask_H = batch[\"mask_H\"].to(device)\n",
    "\n",
    "    # Factorised encoders\n",
    "    zT  = tcr_factorised(emb_T, mask_T)                 # (B, d)\n",
    "    zT  = zT / (zT.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "    zPH = pmhc_factorised(emb_P, mask_P, emb_H, mask_H) # (B, d)\n",
    "    zPH = zPH / (zPH.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "\n",
    "    e_hat = torch.cat([zT, zPH], dim=-1)                # (B, 2d)\n",
    "\n",
    "    # ---- BOLTZ SIDE ----\n",
    "    z_boltz = boltz_batch[\"z\"]       # (B, L_pad, L_pad, dB)  <-- correct key\n",
    "    L_p     = boltz_batch[\"pep_len\"]\n",
    "    L_alpha = boltz_batch[\"tcra_len\"]\n",
    "    L_beta  = boltz_batch[\"tcrb_len\"]\n",
    "    L_h     = boltz_batch[\"hla_len\"]\n",
    "\n",
    "    # -----------------------\n",
    "    # 2) Loss forward (also returns Zstar for debugging)\n",
    "    # -----------------------\n",
    "\n",
    "\n",
    "    # add 'Zstar' as an argument if you want to return it\n",
    "    loss, components, Zstar = non_contrastive_hamiltonian_loss(\n",
    "    zT=zT,\n",
    "    zPH=zPH,\n",
    "    e_hat=e_hat,\n",
    "    z_boltz_batch=z_boltz,\n",
    "    L_alpha=L_alpha,\n",
    "    L_beta=L_beta,\n",
    "    L_p=L_p,\n",
    "    L_h=L_h,\n",
    "    gP=gP,\n",
    "    gH=gH,\n",
    "    boltz_factoriser=boltz_factoriser,\n",
    "    alpha=alpha,\n",
    "    beta=beta,\n",
    "    delta=delta,\n",
    "    gamma_var=1.0,\n",
    "    return_Zstar=True,\n",
    "    use_limit_Zstar=True,\n",
    "    lambda_Z=0.0,\n",
    "    )\n",
    "    \n",
    "    # -----------------------\n",
    "    # 3) Pre-step prints (same forward pass)\n",
    "    # -----------------------\n",
    "    if step == 0:\n",
    "        print(\"=== FIRST BATCH (pre-step) ===\")\n",
    "        print(components)\n",
    "        print(\"zT norm mean:\",  zT.norm(dim=-1).mean().item())\n",
    "        print(\"zPH norm mean:\", zPH.norm(dim=-1).mean().item())\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # 4) Backward pass (compute gradients)\n",
    "    # -----------------------------------------\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "\n",
    "    # Print gradient norms (must be after backward)\n",
    "    if step == 0:\n",
    "        def grad_norm(m):\n",
    "            tot = 0.0\n",
    "            for p in m.parameters():\n",
    "                if p.grad is not None:\n",
    "                    tot += p.grad.detach().float().norm().item()**2\n",
    "            return tot**0.5\n",
    "\n",
    "        print(\"grad ||tcr||:\",   grad_norm(tcr_factorised))\n",
    "        print(\"grad ||pmhc||:\",  grad_norm(pmhc_factorised))\n",
    "        print(\"grad ||boltz||:\", grad_norm(boltz_factoriser))\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # 5) Debug: decompose quadratic into blocks\n",
    "    #    (this uses the SAME Zstar as forward)\n",
    "    # -----------------------------------------\n",
    "    d = zT.shape[-1]\n",
    "    A_block = Zstar[:, :d, :d]\n",
    "    C_block = Zstar[:, :d, d:]\n",
    "    B_block = Zstar[:, d:, d:]\n",
    "\n",
    "    term_A = torch.einsum(\"bi,bij,bj->b\", zT,  A_block, zT)\n",
    "    term_B = torch.einsum(\"bi,bij,bj->b\", zPH, B_block, zPH)\n",
    "    term_C = 2.0 * torch.einsum(\"bi,bij,bj->b\", zT, C_block, zPH)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"term_A mean/max:\", term_A.mean().item(), term_A.abs().max().item())\n",
    "        print(\"term_B mean/max:\", term_B.mean().item(), term_B.abs().max().item())\n",
    "        print(\"term_C mean/max:\", term_C.mean().item(), term_C.abs().max().item())\n",
    "\n",
    "        sym_err = (Zstar - Zstar.transpose(-1, -2)).abs().max().item()\n",
    "        print(\"Zstar symmetry max|Z-ZT|:\", sym_err)\n",
    "\n",
    "        print(\"Zstar abs mean:\", Zstar.abs().mean().item())\n",
    "        print(\"Zstar max abs:\",  Zstar.abs().max().item())\n",
    "\n",
    "    print(components)\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # 6) Optimiser step (update parameters)\n",
    "    # -----------------------------------------\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06272c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #dB   = boltz_loader[0][0].shape[-1]   # or hard-code if you know it\n",
    "\n",
    "# #boltz_loader['z'][0][0]\n",
    "\n",
    "# B, L_T_pad, D = emb_T.shape\n",
    "\n",
    "# print(B, L_T_pad, D)\n",
    "\n",
    "# # Get first batch from DataLoader to inspect shape\n",
    "# # DataLoaders are iterables, not subscriptable - use next(iter(...)) to get a batch\n",
    "# first_batch = next(iter(boltz_loader))\n",
    "# dB = first_batch['z'].shape[-1]   # Get channel dimension from the 'z' tensor\n",
    "# print(f\"dB (Boltz channel dimension): {dB}\")\n",
    "# print(f\"First batch 'z' shape: {first_batch['z'].shape}\")\n",
    "# print(f\"First element of first batch: {first_batch['z'][0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a7011",
   "metadata": {},
   "source": [
    "##### Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232dc16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'masked_input_ids': tensor([[ 0, 32, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        ...,\n",
      "        [ 0,  6, 32,  ...,  1,  1,  1],\n",
      "        [ 0, 32, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 32,  ...,  2,  1,  1]]), 'labels': tensor([[-100,    6, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [-100, -100,   17,  ..., -100, -100, -100],\n",
      "        [-100,    6, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100,   17,  ..., -100, -100, -100]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 0, 0]]), 'clean_input_ids': tensor([[ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        ...,\n",
      "        [ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 17,  ...,  2,  1,  1]]), 'clean_sequences': ['GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALHDYKLSFGAGTTVTVRANEAQVTQNPRYLITVTGKKLTVTCSQNMNHEYMSWYRQDPGLGLRQIYYSMNVEVTDKGDVPEGYKVSRKEKRNFPLILESPSPNQTSLYFCASSLSAGLDEQFFGPGTRLTVLE', 'GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALRPAFGGGTRVLVKPNETGVTQTPRHLVMGMTNKKSLKCEQHLGHNAMYWYKQSAKKPLELMFVYSLEERVENNSVPSRFSPECPNSSHLFLHLHTLQPEDSALYLCASSERNHLEAFFGQGTRLTVVE', 'GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALGSGNQFYFGTGTSLTVIPNEAQVTQNPRYLITVTGKKLTVTCSQNMNHEYMSWYRQDPGLGLRQIYYSMNVEVTDKGDVPEGYKVSRKEKRNFPLILESPSPNQTSLYFCASGQRDYNEQFFGPGTRLTVLE', 'GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALRYGKLTFGQGTILTVHPNEAQVTQNPRYLITVTGKKLTVTCSQNMNHEYMSWYRQDPGLGLRQIYYSMNVEVTDKGDVPEGYKVSRKEKRNFPLILESPSPNQTSLYFCASSLSGSPLHFGNGTRLTVTE', 'GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALDSGNTGKLIFGQGTTLQVKPDEAGVAQSPRYKIIEKRQSVAFWCNPISGHATLYWYQQILGQGPKLLIQFQNNGVVDDSQLPKDRFSAERLKGVDSTLKIQPAKLEDSAVYLCASTTGGGGYEQYFGPGTRLTVTE', 'GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALIYNTDKLIFGTGTRLQVFPNDSGVTQTPKHLITATGQRVTLRCSPRSGDLSVYWYQQSLDQGLQFLIQYYNGEERAKGNILERFSAQQFPDLHSELNLSSLELGDSALYFCASSVNPAQGSGANVLTFGAGSRLTVLE', 'GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALGDDKIIFGKGTRLHILPNGAGVSQSPSNKVTEKGKDVELRCDPISGHTALYWYRQSLGQGLEFLIYFQGNSAPDKSGLPSDRFSAERTGGSVSTLTIQRTQQEDSAVYLCASSLASEGFTEAFFGQGTRLTVVE', 'GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALSDPRGGSEKLVFGKGTKLTVNPYDTEVTQTPKHLVMGMTNKKSLKCEQHMGHRAMYWYKQKAKKPPELMFVYSYEKLSINESVPSRFSPECPNSSLLNLHLHALQPEDSALYLCASSQDVGQGVLYGYTFGSGTRLTVVE'], 'masked_sequences': ['<mask>NSVTQMEGPVTLSEEAFLT<mask>NCT<mask>TATGYP<mask>LF<mask>YVQDPGEG<mask>QLLLKA<mask>KADDKGSNKGFLATYRKETTSFHLEKGSV<mask>TSDS<mask>V<mask>FCALHDYK<mask>SFGAGTTV<mask>VRA<mask>EAQVTQNPRYLITVTGKKLTV<mask>CSQNMNHEYMSWY<mask>QDPGAGLRQ<mask>YY<mask>MN<mask>EV<mask>RKH<mask>VPEGYK<mask>SRK<mask>KRNFPLI<mask><mask>SPSP<mask>QTSLY<mask>CASSLSAGLDEQFFGP<mask>TRLTVLE', 'GNSV<mask>QMEGPVTLS<mask>EAFLTINCTYTATGYPNLFWYVQYPGEG<mask>QLLLKATKADDK<mask>SNKG<mask>EATYRKETTSFHLEKGS<mask>Q<mask><mask>DSAVYFCA<mask>RPAFGGGTRVLVKP<mask>ETAVTQTPRHLVP<mask>ETNKK<mask>LKCE<mask>H<mask>GHNAMYWDKQ<mask><mask>KKPLELM<mask>VAS<mask>E<mask>RV<mask>NNSVPSR<mask>SPECPNSS<mask>LF<mask>HLHTLQPEDSALYLC<mask>SSERNHLEAF<mask>GQGTRL<mask>V<mask>E', 'GNSVTQME<mask><mask>VTLD<mask>EAFLTINCTYTATGYPSLFWY<mask>QYPGEGLQLLLKATKADDKGSNKGFEA<mask>YRKETTSFHL<mask>KGSVQVSDSAV<mask>F<mask>ALGSGN<mask>FYSGTGTSLTVIP<mask>EAQVTQ<mask>PRY<mask>IT<mask>TE<mask><mask>LTVTCSQNMNHEYMSWYRQDPG<mask>GLP<mask>IYYSMNVEH<mask>DKGDVPEGYKV<mask>RKE<mask>RNF<mask>LILP<mask>PSPNQT<mask>LYFCASGQRD<mask>NEQFF<mask>PGTRLT<mask>L<mask>', 'GNSVT<mask>MEGPVTLSEEAFLTINCTYTATG<mask>PSL<mask>WYVQYPGEGLQLLLKA<mask><mask>LDDKGSNKGF<mask>ATYKKET<mask>SFHLEKGS<mask>QVSDSAV<mask>FFALRYGT<mask>TFGQGTILTVHPNEAQVTQ<mask><mask>RYNIT<mask>TGKKLT<mask>TC<mask><mask>NM<mask>HEYMS<mask>YRQ<mask>PG<mask>GLRQIYYSDNVEVTDKGDVPEGYK<mask>SRKEKRNFPLILESPSP<mask>QT<mask>LYFCAS<mask><mask>SGSPLHFGNGT<mask>L<mask>VTE', 'GN<mask>VTQMEGPVTL<mask><mask>E<mask>FLTINCTYTATGYPSLFWYVQY<mask>G<mask><mask><mask>QLLLKATKWDDTGSNK<mask>F<mask>A<mask>YRKETTS<mask><mask>LEKGSV<mask>SSDSAVYFCALD<mask>GNTGK<mask>IFGQGTSLQVK<mask>D<mask><mask>GVAQ<mask>PP<mask>K<mask>IEK<mask>QSVAFWCNPISGHATLYWYQQILGQ<mask>PKLLIQ<mask>QNNGVVD<mask>SQLPKDRFSAERLKGVDST<mask>KIQP<mask>KLEDSAVYLCASTTGGGGYEQYFGPGTRLTFTE', 'G<mask>SVTQMEGPVTLSEEAFLEI<mask>CTYTATGY<mask>ELFWYV<mask><mask>PGEG<mask>QLLLKATKADDKGSNK<mask>FEAQYRKETTS<mask>HLEKGSVQVS<mask>SAVYFCALIY<mask>TDKLP<mask>GT<mask><mask>RLQRFP<mask>DSGVTQTPKHLITAT<mask>Q<mask>VTLRCSPRSGDLSVYWYQQSLDQGLQFLI<mask><mask><mask>NGEER<mask>K<mask>NILE<mask>FSAQQFP<mask>LHSELNLSSLEL<mask>DS<mask><mask>VF<mask>ASSVNPAQGSGANVLTFGAG<mask>RLTVLE', '<mask>NSV<mask>QMEGPVTLSELAFLTINVTYTATGYP<mask>LFWYVQ<mask><mask>GEGL<mask><mask>LLKATKADDKGSN<mask>G<mask>EATYRK<mask>T<mask>SFH<mask>EKGY<mask><mask>V<mask>DSAVYFC<mask>LGDDKIIFGKGTR<mask>HILPNGAGVSQSPSN<mask>VTEKGKDV<mask>LRCDPISGHTALYW<mask>RQSLQQ<mask>LEFLIYFQGNSAPDK<mask>GLPSDH<mask>SAERTGGSVSTLT<mask>QRT<mask>QEDSAVYV<mask>ASSLASEGFTEAFFGQGT<mask>LTVVE', 'G<mask>SVTQMEGPVTLSEEA<mask>LT<mask>N<mask>TYTA<mask>EYPSLFWIVQYPGE<mask>LQLLLKATKADDKGSNK<mask>F<mask>A<mask>Y<mask><mask>ETTSFHLEKGKVQVS<mask>SAVYF<mask>ALSDPRGGSEKLV<mask>GKGTKLTV<mask>PYDTEVTQTPKHLVMG<mask>TNKKSLKK<mask>QHMG<mask>RAM<mask>WT<mask>QKAKKPP<mask>LMFVYSYCKLSINESVPS<mask>FSPE<mask>PNSSLLNLH<mask>HALQPED<mask>ALYLCASSQ<mask>VGQGVLYGY<mask>FG<mask>GTRLTVVE'], 'clean_sequences_ESMprotein': [ESMProtein(sequence='GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALHDYKLSFGAGTTVTVRANEAQVTQNPRYLITVTGKKLTVTCSQNMNHEYMSWYRQDPGLGLRQIYYSMNVEVTDKGDVPEGYKVSRKEKRNFPLILESPSPNQTSLYFCASSLSAGLDEQFFGPGTRLTVLE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALRPAFGGGTRVLVKPNETGVTQTPRHLVMGMTNKKSLKCEQHLGHNAMYWYKQSAKKPLELMFVYSLEERVENNSVPSRFSPECPNSSHLFLHLHTLQPEDSALYLCASSERNHLEAFFGQGTRLTVVE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALGSGNQFYFGTGTSLTVIPNEAQVTQNPRYLITVTGKKLTVTCSQNMNHEYMSWYRQDPGLGLRQIYYSMNVEVTDKGDVPEGYKVSRKEKRNFPLILESPSPNQTSLYFCASGQRDYNEQFFGPGTRLTVLE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALRYGKLTFGQGTILTVHPNEAQVTQNPRYLITVTGKKLTVTCSQNMNHEYMSWYRQDPGLGLRQIYYSMNVEVTDKGDVPEGYKVSRKEKRNFPLILESPSPNQTSLYFCASSLSGSPLHFGNGTRLTVTE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALDSGNTGKLIFGQGTTLQVKPDEAGVAQSPRYKIIEKRQSVAFWCNPISGHATLYWYQQILGQGPKLLIQFQNNGVVDDSQLPKDRFSAERLKGVDSTLKIQPAKLEDSAVYLCASTTGGGGYEQYFGPGTRLTVTE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALIYNTDKLIFGTGTRLQVFPNDSGVTQTPKHLITATGQRVTLRCSPRSGDLSVYWYQQSLDQGLQFLIQYYNGEERAKGNILERFSAQQFPDLHSELNLSSLELGDSALYFCASSVNPAQGSGANVLTFGAGSRLTVLE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALGDDKIIFGKGTRLHILPNGAGVSQSPSNKVTEKGKDVELRCDPISGHTALYWYRQSLGQGLEFLIYFQGNSAPDKSGLPSDRFSAERTGGSVSTLTIQRTQQEDSAVYLCASSLASEGFTEAFFGQGTRLTVVE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GNSVTQMEGPVTLSEEAFLTINCTYTATGYPSLFWYVQYPGEGLQLLLKATKADDKGSNKGFEATYRKETTSFHLEKGSVQVSDSAVYFCALSDPRGGSEKLVFGKGTKLTVNPYDTEVTQTPKHLVMGMTNKKSLKCEQHMGHRAMYWYKQKAKKPPELMFVYSYEKLSINESVPSRFSPECPNSSLLNLHLHALQPEDSALYLCASSQDVGQGVLYGYTFGSGTRLTVVE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False)], 'masked_sequences_ESMprotein': [ESMProtein(sequence='<mask>NSVTQMEGPVTLSEEAFLT<mask>NCT<mask>TATGYP<mask>LF<mask>YVQDPGEG<mask>QLLLKA<mask>KADDKGSNKGFLATYRKETTSFHLEKGSV<mask>TSDS<mask>V<mask>FCALHDYK<mask>SFGAGTTV<mask>VRA<mask>EAQVTQNPRYLITVTGKKLTV<mask>CSQNMNHEYMSWY<mask>QDPGAGLRQ<mask>YY<mask>MN<mask>EV<mask>RKH<mask>VPEGYK<mask>SRK<mask>KRNFPLI<mask><mask>SPSP<mask>QTSLY<mask>CASSLSAGLDEQFFGP<mask>TRLTVLE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GNSV<mask>QMEGPVTLS<mask>EAFLTINCTYTATGYPNLFWYVQYPGEG<mask>QLLLKATKADDK<mask>SNKG<mask>EATYRKETTSFHLEKGS<mask>Q<mask><mask>DSAVYFCA<mask>RPAFGGGTRVLVKP<mask>ETAVTQTPRHLVP<mask>ETNKK<mask>LKCE<mask>H<mask>GHNAMYWDKQ<mask><mask>KKPLELM<mask>VAS<mask>E<mask>RV<mask>NNSVPSR<mask>SPECPNSS<mask>LF<mask>HLHTLQPEDSALYLC<mask>SSERNHLEAF<mask>GQGTRL<mask>V<mask>E', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GNSVTQME<mask><mask>VTLD<mask>EAFLTINCTYTATGYPSLFWY<mask>QYPGEGLQLLLKATKADDKGSNKGFEA<mask>YRKETTSFHL<mask>KGSVQVSDSAV<mask>F<mask>ALGSGN<mask>FYSGTGTSLTVIP<mask>EAQVTQ<mask>PRY<mask>IT<mask>TE<mask><mask>LTVTCSQNMNHEYMSWYRQDPG<mask>GLP<mask>IYYSMNVEH<mask>DKGDVPEGYKV<mask>RKE<mask>RNF<mask>LILP<mask>PSPNQT<mask>LYFCASGQRD<mask>NEQFF<mask>PGTRLT<mask>L<mask>', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GNSVT<mask>MEGPVTLSEEAFLTINCTYTATG<mask>PSL<mask>WYVQYPGEGLQLLLKA<mask><mask>LDDKGSNKGF<mask>ATYKKET<mask>SFHLEKGS<mask>QVSDSAV<mask>FFALRYGT<mask>TFGQGTILTVHPNEAQVTQ<mask><mask>RYNIT<mask>TGKKLT<mask>TC<mask><mask>NM<mask>HEYMS<mask>YRQ<mask>PG<mask>GLRQIYYSDNVEVTDKGDVPEGYK<mask>SRKEKRNFPLILESPSP<mask>QT<mask>LYFCAS<mask><mask>SGSPLHFGNGT<mask>L<mask>VTE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GN<mask>VTQMEGPVTL<mask><mask>E<mask>FLTINCTYTATGYPSLFWYVQY<mask>G<mask><mask><mask>QLLLKATKWDDTGSNK<mask>F<mask>A<mask>YRKETTS<mask><mask>LEKGSV<mask>SSDSAVYFCALD<mask>GNTGK<mask>IFGQGTSLQVK<mask>D<mask><mask>GVAQ<mask>PP<mask>K<mask>IEK<mask>QSVAFWCNPISGHATLYWYQQILGQ<mask>PKLLIQ<mask>QNNGVVD<mask>SQLPKDRFSAERLKGVDST<mask>KIQP<mask>KLEDSAVYLCASTTGGGGYEQYFGPGTRLTFTE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='G<mask>SVTQMEGPVTLSEEAFLEI<mask>CTYTATGY<mask>ELFWYV<mask><mask>PGEG<mask>QLLLKATKADDKGSNK<mask>FEAQYRKETTS<mask>HLEKGSVQVS<mask>SAVYFCALIY<mask>TDKLP<mask>GT<mask><mask>RLQRFP<mask>DSGVTQTPKHLITAT<mask>Q<mask>VTLRCSPRSGDLSVYWYQQSLDQGLQFLI<mask><mask><mask>NGEER<mask>K<mask>NILE<mask>FSAQQFP<mask>LHSELNLSSLEL<mask>DS<mask><mask>VF<mask>ASSVNPAQGSGANVLTFGAG<mask>RLTVLE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='<mask>NSV<mask>QMEGPVTLSELAFLTINVTYTATGYP<mask>LFWYVQ<mask><mask>GEGL<mask><mask>LLKATKADDKGSN<mask>G<mask>EATYRK<mask>T<mask>SFH<mask>EKGY<mask><mask>V<mask>DSAVYFC<mask>LGDDKIIFGKGTR<mask>HILPNGAGVSQSPSN<mask>VTEKGKDV<mask>LRCDPISGHTALYW<mask>RQSLQQ<mask>LEFLIYFQGNSAPDK<mask>GLPSDH<mask>SAERTGGSVSTLT<mask>QRT<mask>QEDSAVYV<mask>ASSLASEGFTEAFFGQGT<mask>LTVVE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='G<mask>SVTQMEGPVTLSEEA<mask>LT<mask>N<mask>TYTA<mask>EYPSLFWIVQYPGE<mask>LQLLLKATKADDKGSNK<mask>F<mask>A<mask>Y<mask><mask>ETTSFHLEKGKVQVS<mask>SAVYF<mask>ALSDPRGGSEKLV<mask>GKGTKLTV<mask>PYDTEVTQTPKHLVMG<mask>TNKKSLKK<mask>QHMG<mask>RAM<mask>WT<mask>QKAKKPP<mask>LMFVYSYCKLSINESVPS<mask>FSPE<mask>PNSSLLNLH<mask>HALQPED<mask>ALYLCASSQ<mask>VGQGVLYGY<mask>FG<mask>GTRLTVVE', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False)], 'masked_input_ids_ESMprotein_batched': _BatchedESMProteinTensor(sequence=tensor([[ 0, 32, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        ...,\n",
      "        [ 0,  6, 32,  ...,  1,  1,  1],\n",
      "        [ 0, 32, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 32,  ...,  2,  1,  1]]), structure=None, secondary_structure=None, sasa=None, function=None, residue_annotations=None, coordinates=None, potential_sequence_of_concern=False), 'clean_input_ids_ESMprotein_batched': _BatchedESMProteinTensor(sequence=tensor([[ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        ...,\n",
      "        [ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 17,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 17,  ...,  2,  1,  1]]), structure=None, secondary_structure=None, sasa=None, function=None, residue_annotations=None, coordinates=None, potential_sequence_of_concern=False)}\n",
      "{'masked_input_ids': tensor([[ 0, 11, 32, 13, 15,  8, 18,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 11, 32, 13, 14,  8, 23,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 10, 32, 10,  5, 19,  6, 19,  8, 15,  2,  1,  1,  1],\n",
      "        [ 0,  6, 11,  8,  6, 32, 14, 12,  7,  7, 10,  2,  1,  1],\n",
      "        [ 0,  8, 14, 10, 22, 19, 18, 32, 19,  8,  2,  1,  1,  1],\n",
      "        [ 0, 13, 11, 13, 14,  8, 18, 32,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 11, 11, 17, 14,  8, 32,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 11,  8, 11,  4, 16,  9, 14, 32,  6, 22,  2,  1,  1]]), 'labels': tensor([[-100, -100,   11, -100,   14, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100, -100,   11, -100, -100, -100,   18, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100, -100,    7, -100, -100, -100,   11, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100, -100, -100, -100, -100,    8, -100, -100, -100,   17, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100,   19, -100,    4, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100,   11, -100, -100, -100, -100, -100,    4, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100, -100, -100,   13, -100, -100,   18, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100,   16,   12, -100, -100, -100,\n",
      "         -100, -100]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]), 'clean_input_ids': tensor([[ 0, 11, 11, 13, 14,  8, 18,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 11, 11, 13, 14,  8, 18,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 10,  7, 10,  5, 19, 11, 19,  8, 15,  2,  1,  1,  1],\n",
      "        [ 0,  6, 11,  8,  6,  8, 14, 12,  7, 17, 10,  2,  1,  1],\n",
      "        [ 0,  8, 14, 10, 22, 19, 18, 19, 19,  4,  2,  1,  1,  1],\n",
      "        [ 0, 11, 11, 13, 14,  8, 18,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 11, 11, 13, 14,  8, 18,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 11,  8, 11,  4, 16,  9, 16, 12,  6, 22,  2,  1,  1]]), 'clean_sequences': ['TTDPSFLGRY', 'TTDPSFLGRY', 'RVRAYTYSK', 'GTSGSPIVNR', 'SPRWYFYYL', 'TTDPSFLGRY', 'TTDPSFLGRY', 'TSTLQEQIGW'], 'masked_sequences': ['T<mask>DKSFLGRY', 'T<mask>DPSCLGRY', 'R<mask>RAYGYSK', 'GTSG<mask>PIVVR', 'SPRWYF<mask>YS', 'DTDPSF<mask>GRY', 'TTNPS<mask>LGRY', 'TSTLQEP<mask>GW'], 'clean_sequences_ESMprotein': [ESMProtein(sequence='TTDPSFLGRY', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='TTDPSFLGRY', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='RVRAYTYSK', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GTSGSPIVNR', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='SPRWYFYYL', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='TTDPSFLGRY', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='TTDPSFLGRY', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='TSTLQEQIGW', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False)], 'masked_sequences_ESMprotein': [ESMProtein(sequence='T<mask>DKSFLGRY', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='T<mask>DPSCLGRY', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='R<mask>RAYGYSK', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='GTSG<mask>PIVVR', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='SPRWYF<mask>YS', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='DTDPSF<mask>GRY', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='TTNPS<mask>LGRY', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='TSTLQEP<mask>GW', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False)], 'masked_input_ids_ESMprotein_batched': _BatchedESMProteinTensor(sequence=tensor([[ 0, 11, 32, 13, 15,  8, 18,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 11, 32, 13, 14,  8, 23,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 10, 32, 10,  5, 19,  6, 19,  8, 15,  2,  1,  1,  1],\n",
      "        [ 0,  6, 11,  8,  6, 32, 14, 12,  7,  7, 10,  2,  1,  1],\n",
      "        [ 0,  8, 14, 10, 22, 19, 18, 32, 19,  8,  2,  1,  1,  1],\n",
      "        [ 0, 13, 11, 13, 14,  8, 18, 32,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 11, 11, 17, 14,  8, 32,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 11,  8, 11,  4, 16,  9, 14, 32,  6, 22,  2,  1,  1]]), structure=None, secondary_structure=None, sasa=None, function=None, residue_annotations=None, coordinates=None, potential_sequence_of_concern=False), 'clean_input_ids_ESMprotein_batched': _BatchedESMProteinTensor(sequence=tensor([[ 0, 11, 11, 13, 14,  8, 18,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 11, 11, 13, 14,  8, 18,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 10,  7, 10,  5, 19, 11, 19,  8, 15,  2,  1,  1,  1],\n",
      "        [ 0,  6, 11,  8,  6,  8, 14, 12,  7, 17, 10,  2,  1,  1],\n",
      "        [ 0,  8, 14, 10, 22, 19, 18, 19, 19,  4,  2,  1,  1,  1],\n",
      "        [ 0, 11, 11, 13, 14,  8, 18,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 11, 11, 13, 14,  8, 18,  4,  6, 10, 19,  2,  1,  1],\n",
      "        [ 0, 11,  8, 11,  4, 16,  9, 16, 12,  6, 22,  2,  1,  1]]), structure=None, secondary_structure=None, sasa=None, function=None, residue_annotations=None, coordinates=None, potential_sequence_of_concern=False)}\n",
      "{'masked_input_ids': tensor([[ 0, 32,  5,  ..., 15,  7,  2],\n",
      "        [ 0, 20,  5,  ..., 32,  7,  2],\n",
      "        [ 0, 20,  5,  ..., 15,  7,  2],\n",
      "        ...,\n",
      "        [ 0, 20, 11,  ...,  1,  1,  1],\n",
      "        [ 0, 20,  9,  ..., 32,  7,  2],\n",
      "        [ 0, 20,  5,  ..., 15, 32,  2]]), 'labels': tensor([[-100,   20, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ...,   15, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [-100, -100,    4,  ..., -100, -100, -100],\n",
      "        [-100, -100,    5,  ...,   15, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100,    7, -100]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'clean_input_ids': tensor([[ 0, 20,  5,  ..., 15,  7,  2],\n",
      "        [ 0, 20,  5,  ..., 15,  7,  2],\n",
      "        [ 0, 20,  5,  ..., 15,  7,  2],\n",
      "        ...,\n",
      "        [ 0, 20,  4,  ...,  1,  1,  1],\n",
      "        [ 0, 20,  5,  ..., 15,  7,  2],\n",
      "        [ 0, 20,  5,  ..., 15,  7,  2]]), 'clean_sequences': ['MAVMAPRTLLLLLSGALALTQTWAGSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQKMEPRAPWIEQEGPEYWDQETRNMKAHSQTDRANLGTLRGYYNQSEDGSHTIQIMYGCDVGPDGRFLRGYRQDAYDGKDYIALNEDLRSWTAADMAAQITKRKWEAVHAAEQRRVYLEGRCVDGLRRYLENGKETLQRTDPPKTHMTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWELSSQPTIPIVGIIAGLVLLGAVITGAVVAAVMWRRKSSDRKGGSYTQAASSDSAQGSDVSLTACKV', 'MAVMAPRTLVLLLSGALALTQTWAGSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETLQRTDAPKTHMTHHAVSDHEATLRCWALSFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGQEQRYTCHVQHEGLPKPLTLRWEPSSQPTIPIVGIIAGLVLFGAVITGAVVAAVMWRRKSSDRKGGSYSQAASSDSAQGSDVSLTACKV', 'MAVMAPRTLLLLLSGALALTQTWAGSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQKMEPRAPWIEQEGPEYWDQETRNMKAHSQTDRANLGTLRGYYNQSEDGSHTIQIMYGCDVGPDGRFLRGYRQDAYDGKDYIALNEDLRSWTAADMAAQITKRKWEAVHAAEQRRVYLEGRCVDGLRRYLENGKETLQRTDPPKTHMTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWELSSQPTIPIVGIIAGLVLLGAVITGAVVAAVMWRRKSSDRKGGSYTQAASSDSAQGSDVSLTACKV', 'MAVMAPRTLVLLLSGALALTQTWAGSHSMRYFSTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDEETGKVKAHSQTDRENLRIALRYYNQSEAGSHTLQMMFGCDVGSDGRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQITKRKWEAAHVAEQQRAYLEGTCVDGLRRYLENGKETLQRTDPPKTHMTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWEPSSQPTVPIVGIIAGLVLLGAVITGAVVAAVMWRRNSSDRKGGSYSQAASSDSAQGSDVSLTACKV', 'MAVMAPRTLVLLLSGALALTQTWAGSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETLQRTDAPKTHMTHHAVSDHEATLRCWALSFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGQEQRYTCHVQHEGLPKPLTLRWEPSSQPTIPIVGIIAGLVLFGAVITGAVVAAVMWRRKSSDRKGGSYSQAASSDSAQGSDVSLTACKV', 'MLVMAPRTVLLLLSAALALTETWAGSHSMRYFDTAMSRPGRGEPRFISVGYVDDTQFVRFDSDAASPREEPRAPWIEQEGPEYWDRNTQIFKTNTQTDRESLRNLRGYYNQSEAGSHTLQSMYGCDVGPDGRLLRGHNQYAYDGKDYIALNEDLRSWTAADTAAQITQRKWEAARVAEQDRAYLEGTCVEWLRRYLENGKDTLERADPPKTHVTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDRTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWEPSSQSTVPIVGIVAGLAVLAVVVIGAVVAAVMCRRKSSGGKGGSYSQAACSDSAQGSDVSLTA', 'MAVMAPRTLLLLLSGALALTQTWAGSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQKMEPRAPWIEQEGPEYWDQETRNMKAHSQTDRANLGTLRGYYNQSEDGSHTIQIMYGCDVGPDGRFLRGYRQDAYDGKDYIALNEDLRSWTAADMAAQITKRKWEAVHAAEQRRVYLEGRCVDGLRRYLENGKETLQRTDPPKTHMTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWELSSQPTIPIVGIIAGLVLLGAVITGAVVAAVMWRRKSSDRKGGSYTQAASSDSAQGSDVSLTACKV', 'MAVMAPRTLLLLLSGALALTQTWAGSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQKMEPRAPWIEQEGPEYWDQETRNMKAHSQTDRANLGTLRGYYNQSEDGSHTIQIMYGCDVGPDGRFLRGYRQDAYDGKDYIALNEDLRSWTAADMAAQITKRKWEAVHAAEQRRVYLEGRCVDGLRRYLENGKETLQRTDPPKTHMTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWELSSQPTIPIVGIIAGLVLLGAVITGAVVAAVMWRRKSSDRKGGSYTQAASSDSAQGSDVSLTACKV'], 'masked_sequences': ['<mask>AVMAPRTLLLLLS<mask>ALALTQTWAGSHSMRY<mask>FTSVSRPGRGERRFIVVGYVDD<mask>QFVMFDSDAASQKM<mask>PRAPW<mask>EQEG<mask>E<mask>WDQE<mask>RNMK<mask>HSPT<mask>R<mask>NLGTLRGYYNQSEDGSHTIQIMYGCDVGPDGRFLRTYRQ<mask>AYDGKDYIALNEDLRSW<mask>AADMAAQITK<mask>KWEAVHAHEQRRVYLEGR<mask>VDG<mask>RRYLENGKETL<mask>RTDPPKT<mask>MTH<mask>PISD<mask>EATLRCWALG<mask>YPAEITLTWQRDGED<mask>T<mask><mask>TEL<mask>ETRPAG<mask>GTFNKWKAVVVPSGEEQRYTCHVQHEGLPKPLTLRWELSSQPTIP<mask>VGIIAG<mask><mask>LLGAVIT<mask>AV<mask>AAVMWRIKSSDRKGG<mask><mask><mask>QA<mask>SSDSAQGSDVSLTACKV', 'MAVMAPRTLVLLLSGALALTQTWAGSH<mask>MRYFFTS<mask>SRP<mask>RGEPRFI<mask>VCYVDDTQFVRFDSDAA<mask>QRMEPR<mask>PWIE<mask>EGPEYWDGETRKVKAH<mask>Q<mask>HRVDLGTLRGYYNQS<mask><mask>GSHT<mask>QRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLFS<mask>TAACMA<mask>Q<mask><mask><mask>HKWEA<mask>HVA<mask>QLRAYLEG<mask>CVEWLRRYLENGKETLQR<mask>DAPP<mask>HMTSHAVSD<mask><mask>ATLRCWALSFYP<mask><mask>I<mask>LTWQRSGEDQTQDTELVETRPAGD<mask>TFQKWAAV<mask><mask>PSGKEQRYTCHVQH<mask>GL<mask>KPLTLRWEPSSQP<mask>IPISGIIAGLVLFGAVITG<mask>VVAAVMWRRKSSDCKG<mask>SYSQAASSDSAQGSDVSLTAC<mask>V', 'MAVMAPWTL<mask>LLL<mask>GALALT<mask>TWACS<mask>SMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRF<mask>SDAAS<mask>KMEARA<mask>WI<mask>QEGPE<mask>WDQETRNMKAHSQTDRAELGTLRGY<mask>NQSEDGSHT<mask>QIMYGCDVGPDGRFLR<mask>YRQDAYDGKDYIAL<mask>EDGRSWTRA<mask>MAAQITKRKW<mask><mask>VH<mask>AEQRRVYLE<mask>RCVDGLRRYLENGKETLQRTD<mask>PKTH<mask>T<mask>HPISDHEATLRCWALG<mask>YPAEITLTNQRDG<mask>DQTQDT<mask>LVETRPAFDGTFQKWAA<mask>VVPSGEEQRYTCHVQHEHLPKPLTLRWELSSQPTIPIVG<mask><mask>AGLVL<mask>GAV<mask>TGA<mask>VAAVMWRRKS<mask>DR<mask>GGSYT<mask>A<mask>SSDSAQG<mask><mask>VSLTACKV', 'M<mask>VMAPRTLVLLLSGACALTQTWAGS<mask>SMRYDSTSVS<mask>PGRGEP<mask><mask>IAVG<mask><mask>DDTQ<mask>VHFDSDAASQ<mask>MERRAPWI<mask>QEGPEYWDE<mask>TGKVKAH<mask><mask>TDRENLRIALRYYNQ<mask>EAGSHTLQMMFGCDVGSDG<mask><mask>LCGYHQYAYDGKD<mask>IALKEDLR<mask>WTAADMAAQI<mask>KRKWEAAHVAEQQRAYLEGTCYDNLRRYLENGKETLQRTDPPKTHMTHHPISDHEATLRCWALGFYPAEI<mask>LTW<mask>R<mask>GEDQTQ<mask>TELVETRPAGDGTFQKWAAVVVPSGEE<mask>MYTCHVQHEGLP<mask>PLTLRWEPSS<mask>P<mask><mask>PIVGI<mask>AGLVLLGA<mask>ITGAVVAAVMWRRNSSDR<mask>G<mask><mask>YSQ<mask>ASSDSA<mask>GSD<mask>SLTACKV', 'MAVMAPRTLVLLL<mask>GALALTQTWA<mask>SHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRF<mask>SDA<mask>SQRMEPRAP<mask>IEQE<mask>PEYWDGETRKVKAHSQTHR<mask>DLGTLRGYTNQSEA<mask>SH<mask>VQRM<mask>GCD<mask>GSDWRFLR<mask>YHQ<mask>AYD<mask>KDYIALK<mask>DLR<mask><mask><mask>AADM<mask>AQTTKHKWEAAHVAEQ<mask>RAYLEMFC<mask>EYLRRYL<mask>N<mask>KETLHRTDAPKTHMTHHAVSD<mask>EATL<mask>CWALSFYPAEITLTW<mask>RDGEDQTQDTELVE<mask>R<mask>AGDGTFQKWA<mask>VVVPSGQEQRYTCHVQHKGLPKPLTTRWEPSSQPTIPIVGIIAG<mask>VLFLA<mask>I<mask>GAVVEAVMWRRKSS<mask>RKGG<mask>YSQA<mask>SS<mask>SAQGSDVSLTACKV', 'MTVMAP<mask><mask>V<mask>LLLSAAL<mask>LTET<mask>AGSHSMRYFDT<mask>MSRP<mask>RG<mask>PRFISVGYVDYT<mask>FVRFD<mask><mask>AA<mask>PREEPRAPWIEQE<mask>PEYWDRNTQIFKTNTQTDR<mask>SLRDLRGYYNQSEAGSHTLQ<mask>MYGCDVGPDGRLL<mask>GHNQYAYDGKDYIALNEDLR<mask>WTAA<mask>TAAQITQRKWEAARVA<mask>QDRA<mask>LEGT<mask>VEWMRRYLENG<mask>DT<mask>ERSDPPKTHV<mask>HHP<mask>WDHEVTLRCWALGFYPAEIT<mask>TWQRDGE<mask>QTQDTELVETRPAGDRTFQKWAAVVV<mask><mask>GEEQRY<mask><mask>HVQHEGLPKPLTLRWEPSSQSTVPIVGIVACLAVLAVVVIGA<mask>VAYV<mask>CRR<mask>SSG<mask>KGGS<mask>SQAACSDSAQGSDVSLTA', 'MEV<mask>APRTLLLLLSGA<mask>A<mask>TQTWAGSHSMRYFFT<mask>MSRMG<mask>GEPRFI<mask>VGYVDDTQF<mask>RFDSDAASQKMEPRAP<mask>PEQE<mask>PEYWDQETR<mask>MKAHSQTDRANL<mask>TLRGYYNQSEDGSHIIQIMYGCDVHPDGRFLRGYRTDAYD<mask>KD<mask>IALNEDWRSWTAADMAAQITLRKWEAVHAAEQ<mask>RVYLEGR<mask>VD<mask>LR<mask>YL<mask>NGKE<mask>LQRTDP<mask>KTHMTHHPISD<mask>EATLRCWAL<mask>F<mask>P<mask>EITLTWQRDGEDQT<mask>DTELVETRPAGDGTFQKWAAVVVPSGEEQR<mask><mask>C<mask><mask>Q<mask>EGLPKPLTLRWELSSQPTIPIVG<mask>IAGLVL<mask>GAVITGAVVAAVM<mask>RRKSSDRKGGSYTQAAS<mask>DSAQGS<mask>VSLTAC<mask>V', 'MAVM<mask>PRTLLLLLSGALALTQTWAGSH<mask>MRY<mask>FTSVSRPGRGE<mask>RFIAVGY<mask>DDTQFVRFDSDAASQKMEPRAMWIEQEGP<mask><mask>WDQ<mask>TRNMKAHSQTD<mask>RNLGTLR<mask>YYNQSEDGSHTIQIMYGCDVGPDGRFLRGYRQDAYDGKDYIALSEDLRSWTAADMAAQITKRKWEADHAAEQRRV<mask>LEGRC<mask>DGLRRYLENG<mask><mask>TLQRTDP<mask>KT<mask>MT<mask><mask>PISDHEA<mask>LSC<mask>A<mask>GF<mask>PA<mask>I<mask>L<mask>WQRDGE<mask>QTQDTEL<mask>ETRPAGDGTF<mask>KN<mask>AVVVPSGEEQRYTCHVQHEGLPKPL<mask>LRWELSSGPTKPIVGIIAGLVLL<mask>AVITGAVVAAV<mask>WRRKSSDR<mask>GG<mask>YTQ<mask>ASSDSAQGSDVSLTACK<mask>'], 'clean_sequences_ESMprotein': [ESMProtein(sequence='MAVMAPRTLLLLLSGALALTQTWAGSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQKMEPRAPWIEQEGPEYWDQETRNMKAHSQTDRANLGTLRGYYNQSEDGSHTIQIMYGCDVGPDGRFLRGYRQDAYDGKDYIALNEDLRSWTAADMAAQITKRKWEAVHAAEQRRVYLEGRCVDGLRRYLENGKETLQRTDPPKTHMTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWELSSQPTIPIVGIIAGLVLLGAVITGAVVAAVMWRRKSSDRKGGSYTQAASSDSAQGSDVSLTACKV', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MAVMAPRTLVLLLSGALALTQTWAGSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETLQRTDAPKTHMTHHAVSDHEATLRCWALSFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGQEQRYTCHVQHEGLPKPLTLRWEPSSQPTIPIVGIIAGLVLFGAVITGAVVAAVMWRRKSSDRKGGSYSQAASSDSAQGSDVSLTACKV', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MAVMAPRTLLLLLSGALALTQTWAGSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQKMEPRAPWIEQEGPEYWDQETRNMKAHSQTDRANLGTLRGYYNQSEDGSHTIQIMYGCDVGPDGRFLRGYRQDAYDGKDYIALNEDLRSWTAADMAAQITKRKWEAVHAAEQRRVYLEGRCVDGLRRYLENGKETLQRTDPPKTHMTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWELSSQPTIPIVGIIAGLVLLGAVITGAVVAAVMWRRKSSDRKGGSYTQAASSDSAQGSDVSLTACKV', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MAVMAPRTLVLLLSGALALTQTWAGSHSMRYFSTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDEETGKVKAHSQTDRENLRIALRYYNQSEAGSHTLQMMFGCDVGSDGRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQITKRKWEAAHVAEQQRAYLEGTCVDGLRRYLENGKETLQRTDPPKTHMTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWEPSSQPTVPIVGIIAGLVLLGAVITGAVVAAVMWRRNSSDRKGGSYSQAASSDSAQGSDVSLTACKV', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MAVMAPRTLVLLLSGALALTQTWAGSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETLQRTDAPKTHMTHHAVSDHEATLRCWALSFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGQEQRYTCHVQHEGLPKPLTLRWEPSSQPTIPIVGIIAGLVLFGAVITGAVVAAVMWRRKSSDRKGGSYSQAASSDSAQGSDVSLTACKV', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MLVMAPRTVLLLLSAALALTETWAGSHSMRYFDTAMSRPGRGEPRFISVGYVDDTQFVRFDSDAASPREEPRAPWIEQEGPEYWDRNTQIFKTNTQTDRESLRNLRGYYNQSEAGSHTLQSMYGCDVGPDGRLLRGHNQYAYDGKDYIALNEDLRSWTAADTAAQITQRKWEAARVAEQDRAYLEGTCVEWLRRYLENGKDTLERADPPKTHVTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDRTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWEPSSQSTVPIVGIVAGLAVLAVVVIGAVVAAVMCRRKSSGGKGGSYSQAACSDSAQGSDVSLTA', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MAVMAPRTLLLLLSGALALTQTWAGSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQKMEPRAPWIEQEGPEYWDQETRNMKAHSQTDRANLGTLRGYYNQSEDGSHTIQIMYGCDVGPDGRFLRGYRQDAYDGKDYIALNEDLRSWTAADMAAQITKRKWEAVHAAEQRRVYLEGRCVDGLRRYLENGKETLQRTDPPKTHMTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWELSSQPTIPIVGIIAGLVLLGAVITGAVVAAVMWRRKSSDRKGGSYTQAASSDSAQGSDVSLTACKV', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MAVMAPRTLLLLLSGALALTQTWAGSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQKMEPRAPWIEQEGPEYWDQETRNMKAHSQTDRANLGTLRGYYNQSEDGSHTIQIMYGCDVGPDGRFLRGYRQDAYDGKDYIALNEDLRSWTAADMAAQITKRKWEAVHAAEQRRVYLEGRCVDGLRRYLENGKETLQRTDPPKTHMTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWELSSQPTIPIVGIIAGLVLLGAVITGAVVAAVMWRRKSSDRKGGSYTQAASSDSAQGSDVSLTACKV', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False)], 'masked_sequences_ESMprotein': [ESMProtein(sequence='<mask>AVMAPRTLLLLLS<mask>ALALTQTWAGSHSMRY<mask>FTSVSRPGRGERRFIVVGYVDD<mask>QFVMFDSDAASQKM<mask>PRAPW<mask>EQEG<mask>E<mask>WDQE<mask>RNMK<mask>HSPT<mask>R<mask>NLGTLRGYYNQSEDGSHTIQIMYGCDVGPDGRFLRTYRQ<mask>AYDGKDYIALNEDLRSW<mask>AADMAAQITK<mask>KWEAVHAHEQRRVYLEGR<mask>VDG<mask>RRYLENGKETL<mask>RTDPPKT<mask>MTH<mask>PISD<mask>EATLRCWALG<mask>YPAEITLTWQRDGED<mask>T<mask><mask>TEL<mask>ETRPAG<mask>GTFNKWKAVVVPSGEEQRYTCHVQHEGLPKPLTLRWELSSQPTIP<mask>VGIIAG<mask><mask>LLGAVIT<mask>AV<mask>AAVMWRIKSSDRKGG<mask><mask><mask>QA<mask>SSDSAQGSDVSLTACKV', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MAVMAPRTLVLLLSGALALTQTWAGSH<mask>MRYFFTS<mask>SRP<mask>RGEPRFI<mask>VCYVDDTQFVRFDSDAA<mask>QRMEPR<mask>PWIE<mask>EGPEYWDGETRKVKAH<mask>Q<mask>HRVDLGTLRGYYNQS<mask><mask>GSHT<mask>QRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLFS<mask>TAACMA<mask>Q<mask><mask><mask>HKWEA<mask>HVA<mask>QLRAYLEG<mask>CVEWLRRYLENGKETLQR<mask>DAPP<mask>HMTSHAVSD<mask><mask>ATLRCWALSFYP<mask><mask>I<mask>LTWQRSGEDQTQDTELVETRPAGD<mask>TFQKWAAV<mask><mask>PSGKEQRYTCHVQH<mask>GL<mask>KPLTLRWEPSSQP<mask>IPISGIIAGLVLFGAVITG<mask>VVAAVMWRRKSSDCKG<mask>SYSQAASSDSAQGSDVSLTAC<mask>V', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MAVMAPWTL<mask>LLL<mask>GALALT<mask>TWACS<mask>SMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRF<mask>SDAAS<mask>KMEARA<mask>WI<mask>QEGPE<mask>WDQETRNMKAHSQTDRAELGTLRGY<mask>NQSEDGSHT<mask>QIMYGCDVGPDGRFLR<mask>YRQDAYDGKDYIAL<mask>EDGRSWTRA<mask>MAAQITKRKW<mask><mask>VH<mask>AEQRRVYLE<mask>RCVDGLRRYLENGKETLQRTD<mask>PKTH<mask>T<mask>HPISDHEATLRCWALG<mask>YPAEITLTNQRDG<mask>DQTQDT<mask>LVETRPAFDGTFQKWAA<mask>VVPSGEEQRYTCHVQHEHLPKPLTLRWELSSQPTIPIVG<mask><mask>AGLVL<mask>GAV<mask>TGA<mask>VAAVMWRRKS<mask>DR<mask>GGSYT<mask>A<mask>SSDSAQG<mask><mask>VSLTACKV', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='M<mask>VMAPRTLVLLLSGACALTQTWAGS<mask>SMRYDSTSVS<mask>PGRGEP<mask><mask>IAVG<mask><mask>DDTQ<mask>VHFDSDAASQ<mask>MERRAPWI<mask>QEGPEYWDE<mask>TGKVKAH<mask><mask>TDRENLRIALRYYNQ<mask>EAGSHTLQMMFGCDVGSDG<mask><mask>LCGYHQYAYDGKD<mask>IALKEDLR<mask>WTAADMAAQI<mask>KRKWEAAHVAEQQRAYLEGTCYDNLRRYLENGKETLQRTDPPKTHMTHHPISDHEATLRCWALGFYPAEI<mask>LTW<mask>R<mask>GEDQTQ<mask>TELVETRPAGDGTFQKWAAVVVPSGEE<mask>MYTCHVQHEGLP<mask>PLTLRWEPSS<mask>P<mask><mask>PIVGI<mask>AGLVLLGA<mask>ITGAVVAAVMWRRNSSDR<mask>G<mask><mask>YSQ<mask>ASSDSA<mask>GSD<mask>SLTACKV', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MAVMAPRTLVLLL<mask>GALALTQTWA<mask>SHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRF<mask>SDA<mask>SQRMEPRAP<mask>IEQE<mask>PEYWDGETRKVKAHSQTHR<mask>DLGTLRGYTNQSEA<mask>SH<mask>VQRM<mask>GCD<mask>GSDWRFLR<mask>YHQ<mask>AYD<mask>KDYIALK<mask>DLR<mask><mask><mask>AADM<mask>AQTTKHKWEAAHVAEQ<mask>RAYLEMFC<mask>EYLRRYL<mask>N<mask>KETLHRTDAPKTHMTHHAVSD<mask>EATL<mask>CWALSFYPAEITLTW<mask>RDGEDQTQDTELVE<mask>R<mask>AGDGTFQKWA<mask>VVVPSGQEQRYTCHVQHKGLPKPLTTRWEPSSQPTIPIVGIIAG<mask>VLFLA<mask>I<mask>GAVVEAVMWRRKSS<mask>RKGG<mask>YSQA<mask>SS<mask>SAQGSDVSLTACKV', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MTVMAP<mask><mask>V<mask>LLLSAAL<mask>LTET<mask>AGSHSMRYFDT<mask>MSRP<mask>RG<mask>PRFISVGYVDYT<mask>FVRFD<mask><mask>AA<mask>PREEPRAPWIEQE<mask>PEYWDRNTQIFKTNTQTDR<mask>SLRDLRGYYNQSEAGSHTLQ<mask>MYGCDVGPDGRLL<mask>GHNQYAYDGKDYIALNEDLR<mask>WTAA<mask>TAAQITQRKWEAARVA<mask>QDRA<mask>LEGT<mask>VEWMRRYLENG<mask>DT<mask>ERSDPPKTHV<mask>HHP<mask>WDHEVTLRCWALGFYPAEIT<mask>TWQRDGE<mask>QTQDTELVETRPAGDRTFQKWAAVVV<mask><mask>GEEQRY<mask><mask>HVQHEGLPKPLTLRWEPSSQSTVPIVGIVACLAVLAVVVIGA<mask>VAYV<mask>CRR<mask>SSG<mask>KGGS<mask>SQAACSDSAQGSDVSLTA', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MEV<mask>APRTLLLLLSGA<mask>A<mask>TQTWAGSHSMRYFFT<mask>MSRMG<mask>GEPRFI<mask>VGYVDDTQF<mask>RFDSDAASQKMEPRAP<mask>PEQE<mask>PEYWDQETR<mask>MKAHSQTDRANL<mask>TLRGYYNQSEDGSHIIQIMYGCDVHPDGRFLRGYRTDAYD<mask>KD<mask>IALNEDWRSWTAADMAAQITLRKWEAVHAAEQ<mask>RVYLEGR<mask>VD<mask>LR<mask>YL<mask>NGKE<mask>LQRTDP<mask>KTHMTHHPISD<mask>EATLRCWAL<mask>F<mask>P<mask>EITLTWQRDGEDQT<mask>DTELVETRPAGDGTFQKWAAVVVPSGEEQR<mask><mask>C<mask><mask>Q<mask>EGLPKPLTLRWELSSQPTIPIVG<mask>IAGLVL<mask>GAVITGAVVAAVM<mask>RRKSSDRKGGSYTQAAS<mask>DSAQGS<mask>VSLTAC<mask>V', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False), ESMProtein(sequence='MAVM<mask>PRTLLLLLSGALALTQTWAGSH<mask>MRY<mask>FTSVSRPGRGE<mask>RFIAVGY<mask>DDTQFVRFDSDAASQKMEPRAMWIEQEGP<mask><mask>WDQ<mask>TRNMKAHSQTD<mask>RNLGTLR<mask>YYNQSEDGSHTIQIMYGCDVGPDGRFLRGYRQDAYDGKDYIALSEDLRSWTAADMAAQITKRKWEADHAAEQRRV<mask>LEGRC<mask>DGLRRYLENG<mask><mask>TLQRTDP<mask>KT<mask>MT<mask><mask>PISDHEA<mask>LSC<mask>A<mask>GF<mask>PA<mask>I<mask>L<mask>WQRDGE<mask>QTQDTEL<mask>ETRPAGDGTF<mask>KN<mask>AVVVPSGEEQRYTCHVQHEGLPKPL<mask>LRWELSSGPTKPIVGIIAGLVLL<mask>AVITGAVVAAV<mask>WRRKSSDR<mask>GG<mask>YTQ<mask>ASSDSAQGSDVSLTACK<mask>', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False)], 'masked_input_ids_ESMprotein_batched': _BatchedESMProteinTensor(sequence=tensor([[ 0, 32,  5,  ..., 15,  7,  2],\n",
      "        [ 0, 20,  5,  ..., 32,  7,  2],\n",
      "        [ 0, 20,  5,  ..., 15,  7,  2],\n",
      "        ...,\n",
      "        [ 0, 20, 11,  ...,  1,  1,  1],\n",
      "        [ 0, 20,  9,  ..., 32,  7,  2],\n",
      "        [ 0, 20,  5,  ..., 15, 32,  2]]), structure=None, secondary_structure=None, sasa=None, function=None, residue_annotations=None, coordinates=None, potential_sequence_of_concern=False), 'clean_input_ids_ESMprotein_batched': _BatchedESMProteinTensor(sequence=tensor([[ 0, 20,  5,  ..., 15,  7,  2],\n",
      "        [ 0, 20,  5,  ..., 15,  7,  2],\n",
      "        [ 0, 20,  5,  ..., 15,  7,  2],\n",
      "        ...,\n",
      "        [ 0, 20,  4,  ...,  1,  1,  1],\n",
      "        [ 0, 20,  5,  ..., 15,  7,  2],\n",
      "        [ 0, 20,  5,  ..., 15,  7,  2]]), structure=None, secondary_structure=None, sasa=None, function=None, residue_annotations=None, coordinates=None, potential_sequence_of_concern=False)}\n",
      "{'z': tensor([[[[-7.5787e+01, -5.1310e+01,  9.8781e+01,  ...,  2.0847e+02,\n",
      "            4.0586e+01, -1.0726e+02],\n",
      "          [-6.3666e+01, -5.9162e+01, -5.1830e+01,  ..., -3.2411e+01,\n",
      "            1.0520e+01, -1.9134e+01],\n",
      "          [-1.0415e+01, -2.2381e+01, -1.4173e+01,  ..., -5.0613e+01,\n",
      "            4.7942e+01, -5.3205e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-2.7920e+01,  2.8625e+00,  6.4688e+01,  ...,  4.4811e+01,\n",
      "            1.5642e+01,  3.4788e+01],\n",
      "          [-2.6895e+01, -2.3034e+01,  6.2510e+01,  ...,  8.8285e+01,\n",
      "           -1.9051e+00, -1.3137e+02],\n",
      "          [-1.4323e+01, -2.6959e+00,  2.3477e+01,  ...,  3.5719e+00,\n",
      "            3.3964e-02, -6.1911e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-9.7394e+00,  2.0081e+01,  3.4673e+01,  ..., -2.7890e-01,\n",
      "           -1.0967e+01,  3.4225e+01],\n",
      "          [ 9.4209e+00,  1.2335e+00,  9.0935e+01,  ...,  4.4095e+01,\n",
      "           -3.8400e+01,  7.3194e+01],\n",
      "          [ 1.7774e+01, -9.6788e+00, -1.2485e+01,  ...,  1.2586e+01,\n",
      "            4.0355e+01, -7.7395e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-9.0633e+01, -5.3944e+01,  9.9216e+01,  ...,  1.9452e+02,\n",
      "            2.9525e+01, -1.0949e+02],\n",
      "          [-6.1494e+01, -6.0051e+01, -5.9876e+01,  ..., -5.2594e+01,\n",
      "            4.1139e+00, -2.2814e+01],\n",
      "          [ 2.7097e-01, -2.5368e+01, -8.3957e+00,  ..., -4.8651e+01,\n",
      "            4.6714e+01, -5.9701e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-4.0599e+01,  1.4086e+01,  6.2028e+01,  ...,  4.9069e+01,\n",
      "            2.0663e+01,  3.6595e+01],\n",
      "          [-3.4037e+01, -6.5183e+00,  6.2519e+01,  ...,  8.9143e+01,\n",
      "           -9.1002e+00, -1.4977e+02],\n",
      "          [ 1.6794e+00, -8.7587e+00,  3.2249e+01,  ...,  1.3206e+01,\n",
      "            1.1681e+01, -6.6727e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-1.4467e+00,  1.5287e+01,  2.8315e+01,  ...,  3.0188e+00,\n",
      "           -9.7570e+00,  2.4901e+01],\n",
      "          [ 1.1826e+01, -3.1632e+00,  8.9367e+01,  ...,  4.4564e+01,\n",
      "           -3.8673e+01,  5.0349e+01],\n",
      "          [ 1.1039e+01, -1.1170e+01, -8.0234e+00,  ...,  7.8601e+00,\n",
      "            3.5962e+01, -8.0373e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-8.0584e+01, -4.7928e+01,  9.8558e+01,  ...,  2.0461e+02,\n",
      "            3.4531e+01, -1.0459e+02],\n",
      "          [-6.4622e+01, -5.1473e+01, -5.0964e+01,  ..., -4.0959e+01,\n",
      "            5.5560e+00, -1.5845e+01],\n",
      "          [-1.0542e+01, -2.5485e+01, -1.0878e+01,  ..., -5.0264e+01,\n",
      "            4.7069e+01, -5.0074e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-3.1890e+01,  6.5963e+00,  6.8057e+01,  ...,  3.9506e+01,\n",
      "            1.3026e+01,  3.8050e+01],\n",
      "          [-2.1317e+01, -7.6482e+00,  6.6364e+01,  ...,  9.9257e+01,\n",
      "            1.1724e+01, -1.4415e+02],\n",
      "          [-8.9612e-01, -1.0810e+01,  2.2784e+01,  ...,  1.2917e+00,\n",
      "            6.1515e+00, -6.3194e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-9.8199e+00,  1.7599e+01,  3.4084e+01,  ..., -1.8010e+00,\n",
      "           -1.0067e+01,  2.9578e+01],\n",
      "          [ 1.1699e+01,  4.0391e+00,  9.3571e+01,  ...,  3.9873e+01,\n",
      "           -3.6558e+01,  5.8486e+01],\n",
      "          [ 1.5462e+01, -7.5816e+00, -1.2345e+01,  ...,  8.0321e+00,\n",
      "            4.4006e+01, -7.7038e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-8.5675e+01, -5.6557e+01,  9.9903e+01,  ...,  1.9795e+02,\n",
      "            3.3628e+01, -1.0696e+02],\n",
      "          [-5.8795e+01, -6.0209e+01, -6.0639e+01,  ..., -4.3757e+01,\n",
      "            5.9186e+00, -2.0508e+01],\n",
      "          [-2.0193e+00, -2.8314e+01, -1.1540e+01,  ..., -5.0825e+01,\n",
      "            4.9439e+01, -5.7245e+01],\n",
      "          ...,\n",
      "          [-6.1280e+00,  1.0689e+01,  3.6608e+00,  ..., -3.1073e+00,\n",
      "            1.2708e+01, -4.7829e+00],\n",
      "          [-9.9795e+00,  3.1534e+00,  6.2991e+00,  ..., -2.0562e+00,\n",
      "            9.2929e+00,  2.7467e+00],\n",
      "          [-3.2073e+01,  2.9799e+01,  1.3938e+01,  ...,  2.2661e+01,\n",
      "            8.7887e+00,  3.1207e+00]],\n",
      "\n",
      "         [[-3.7015e+01,  1.0347e+01,  6.0538e+01,  ...,  4.7303e+01,\n",
      "            1.8857e+01,  4.0805e+01],\n",
      "          [-3.5532e+01, -1.1926e+01,  6.8037e+01,  ...,  8.6972e+01,\n",
      "           -4.3160e+00, -1.4841e+02],\n",
      "          [-3.9978e+00, -1.3038e+01,  3.0272e+01,  ...,  1.0824e+01,\n",
      "            1.4609e+01, -6.9711e+01],\n",
      "          ...,\n",
      "          [-6.2685e+00, -3.6390e+00, -1.1288e+01,  ...,  1.2923e+01,\n",
      "            4.2697e+00, -3.2927e+00],\n",
      "          [-1.3622e+01,  1.2895e-01, -1.2627e+01,  ...,  3.1730e+00,\n",
      "            9.8878e-01, -8.8053e+00],\n",
      "          [-1.4863e+01,  1.0134e+01, -8.7632e+00,  ..., -6.2454e+00,\n",
      "            1.1382e+01, -1.5462e+01]],\n",
      "\n",
      "         [[-3.0478e+00,  1.8920e+01,  2.8517e+01,  ...,  3.5334e+00,\n",
      "           -1.2132e+01,  2.8910e+01],\n",
      "          [ 1.1321e+01,  5.1602e-01,  8.7559e+01,  ...,  4.5745e+01,\n",
      "           -4.0812e+01,  5.9416e+01],\n",
      "          [ 1.5381e+01, -1.1881e+01, -1.0085e+01,  ...,  9.8215e+00,\n",
      "            4.1938e+01, -8.2825e+01],\n",
      "          ...,\n",
      "          [ 9.4441e-01, -7.1907e+00, -4.4117e-03,  ...,  1.6856e+01,\n",
      "            5.4601e+00, -5.5743e+00],\n",
      "          [-1.1009e+01,  2.2721e+00, -4.4464e+00,  ...,  7.7782e+00,\n",
      "            1.1162e+00, -8.7675e-01],\n",
      "          [-1.0276e+01,  7.9159e+00, -3.0885e+00,  ..., -3.2684e+00,\n",
      "            1.2607e+01, -7.3048e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1380e+01,  1.2231e+01, -7.9701e+00,  ..., -1.9197e+01,\n",
      "            9.8349e+00, -1.8719e+01],\n",
      "          [ 1.9333e+01, -2.7691e+00,  3.3259e+00,  ..., -9.1672e+00,\n",
      "            8.7885e+00, -7.6842e+00],\n",
      "          [ 1.1863e+01, -1.0818e+00,  2.3047e+00,  ..., -4.4312e+00,\n",
      "            2.4613e+00, -1.2729e+01],\n",
      "          ...,\n",
      "          [ 7.2059e+01,  9.8580e+00,  6.8542e+01,  ...,  1.9241e+01,\n",
      "           -2.8864e+01, -1.2759e+02],\n",
      "          [ 1.4003e+01, -1.3780e+01,  2.8244e+01,  ...,  1.6128e+01,\n",
      "           -3.6242e+01, -8.6507e+00],\n",
      "          [-1.1069e+01,  1.2709e+01,  1.1287e+01,  ...,  7.7137e+00,\n",
      "           -1.2927e+01, -1.0244e+01]],\n",
      "\n",
      "         [[-7.9335e+00,  5.6292e+00, -6.2357e+00,  ..., -2.5344e+01,\n",
      "            6.3168e+00, -2.6695e+01],\n",
      "          [ 1.8482e+01, -2.4724e+00,  1.2176e+00,  ..., -1.7628e+01,\n",
      "            2.0431e+00, -1.6093e+01],\n",
      "          [ 2.2905e+00, -1.3309e-01, -2.5560e+00,  ..., -1.2732e+01,\n",
      "            2.5616e-01, -9.5704e+00],\n",
      "          ...,\n",
      "          [ 1.1594e+01,  4.9203e+00,  5.3738e+01,  ...,  4.4892e+01,\n",
      "           -4.6698e+01,  2.8097e+01],\n",
      "          [ 3.2627e+01, -1.6924e+01, -2.7710e+01,  ...,  3.8002e+01,\n",
      "            8.4498e+00, -6.0959e+01],\n",
      "          [ 1.5938e+01,  5.5487e+00,  1.2822e+01,  ...,  4.1582e+00,\n",
      "           -4.0532e+01,  1.3745e+01]],\n",
      "\n",
      "         [[-1.4560e+01,  1.4676e+01,  4.2849e-01,  ...,  4.2711e+00,\n",
      "            1.1763e+01, -1.4445e+01],\n",
      "          [ 9.8684e+00,  6.2970e+00,  1.1367e+01,  ..., -2.2512e+01,\n",
      "            1.6250e+01, -9.4996e+00],\n",
      "          [ 3.8045e+00,  1.1691e+00,  5.9747e+00,  ..., -2.1614e+01,\n",
      "            4.2535e+00, -7.9023e+00],\n",
      "          ...,\n",
      "          [-9.2030e+00, -2.5783e+01,  3.0335e+01,  ...,  1.5717e+01,\n",
      "            8.2291e+00,  3.7036e+01],\n",
      "          [-2.8226e+01,  1.1563e+01,  4.6457e+01,  ...,  6.2266e+01,\n",
      "           -1.2297e+01,  4.5106e+01],\n",
      "          [-4.4053e+01, -6.8772e+01,  3.6863e+01,  ...,  1.2060e+02,\n",
      "           -6.0756e+01, -1.4963e+02]]],\n",
      "\n",
      "\n",
      "        [[[-8.6232e+01, -5.6776e+01,  9.8703e+01,  ...,  2.0233e+02,\n",
      "            3.5484e+01, -1.0922e+02],\n",
      "          [-5.9760e+01, -5.7313e+01, -5.6781e+01,  ..., -4.3028e+01,\n",
      "            5.1713e+00, -2.2068e+01],\n",
      "          [-1.6791e+00, -2.1682e+01, -1.2215e+01,  ..., -4.8484e+01,\n",
      "            4.4818e+01, -5.2936e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-3.3361e+01,  1.0509e+01,  6.3771e+01,  ...,  4.4376e+01,\n",
      "            1.7950e+01,  3.5115e+01],\n",
      "          [-2.7916e+01, -1.0028e+01,  6.4843e+01,  ...,  9.7112e+01,\n",
      "           -1.7003e+00, -1.5461e+02],\n",
      "          [-6.1033e+00, -6.4743e+00,  3.0683e+01,  ...,  6.1962e+00,\n",
      "            6.4106e+00, -6.6899e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-4.2062e+00,  1.6651e+01,  3.0384e+01,  ...,  2.7812e-01,\n",
      "           -1.1058e+01,  2.7071e+01],\n",
      "          [ 9.8915e+00, -5.1809e+00,  9.8344e+01,  ...,  4.8600e+01,\n",
      "           -4.2017e+01,  5.5031e+01],\n",
      "          [ 1.3458e+01, -1.1837e+01, -1.1488e+01,  ...,  2.6700e+00,\n",
      "            3.9189e+01, -7.9381e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-8.6405e+01, -5.5849e+01,  9.6350e+01,  ...,  1.9937e+02,\n",
      "            3.2435e+01, -1.1418e+02],\n",
      "          [-5.8095e+01, -6.2254e+01, -6.4914e+01,  ..., -4.6129e+01,\n",
      "            9.3389e+00, -2.1827e+01],\n",
      "          [ 1.3736e+00, -2.1307e+01, -1.1489e+01,  ..., -4.8070e+01,\n",
      "            4.7555e+01, -5.5138e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-3.5942e+01,  1.5130e+01,  5.5314e+01,  ...,  4.6797e+01,\n",
      "            2.0483e+01,  3.2103e+01],\n",
      "          [-2.4082e+01, -9.6169e+00,  5.7458e+01,  ...,  1.0017e+02,\n",
      "            1.3396e+01, -1.5304e+02],\n",
      "          [-1.5497e+00, -4.6722e+00,  2.6928e+01,  ...,  1.4799e+01,\n",
      "            8.3867e+00, -7.0396e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-1.3571e+00,  1.5344e+01,  2.7870e+01,  ...,  7.7299e-01,\n",
      "           -1.0111e+01,  2.4872e+01],\n",
      "          [ 8.1904e+00, -5.0364e+00,  9.4363e+01,  ...,  5.2095e+01,\n",
      "           -3.9099e+01,  4.8921e+01],\n",
      "          [ 1.2717e+01, -1.0721e+01, -1.2649e+01,  ...,  5.5267e+00,\n",
      "            4.1422e+01, -7.9576e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]]), 'pep_len': array([ 9,  9,  9, 10,  9,  9,  9, 10]), 'tcra_len': array([110, 112, 110, 115, 112, 117, 113, 113]), 'tcrb_len': array([114, 116, 115, 115, 115, 117, 115, 115]), 'hla_len': array([365, 362, 365, 362, 362, 365, 365, 365])}\n"
     ]
    }
   ],
   "source": [
    "# try for loop for one batch (break at the end)\n",
    "for tcr_batch, pep_batch, hla_batch, boltz_batch in zip(tcr_loader, pep_loader, hla_loader, boltz_loader):\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    break\n",
    "\n",
    "# to load one batch from each loader on CPU took 6.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08c16c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESM forwards (T+P+H) took: 17.520 s\n"
     ]
    }
   ],
   "source": [
    "# Testing on one batch so far\n",
    "# 1. Pick a batch and get token tensors and masks\n",
    "\n",
    "tcr_batch = next(iter(tcr_loader))\n",
    "pep_batch = next(iter(pep_loader))\n",
    "hla_batch = next(iter(hla_loader))\n",
    "\n",
    "tcr_ids = tcr_batch['clean_input_ids']\n",
    "tcr_mask = tcr_batch['attention_mask']\n",
    "\n",
    "pep_ids = pep_batch['clean_input_ids']\n",
    "pep_mask = pep_batch['attention_mask']\n",
    "\n",
    "hla_ids = hla_batch['clean_input_ids']\n",
    "hla_mask = hla_batch['attention_mask']\n",
    "\n",
    "# Unsqueze to make the right dims\n",
    "mT = tcr_mask.unsqueeze(-1).float()   # (B, L_T_pad, 1)\n",
    "mP = pep_mask.unsqueeze(-1).float()   # (B, L_P_pad, 1)\n",
    "mH = hla_mask.unsqueeze(-1).float()   # (B, L_H_pad, 1)\n",
    "\n",
    "\n",
    "#B = tcr_ids.size(0)\n",
    "\n",
    "# ---- TIME ESM FORWARDS ----\n",
    "t0 = time.perf_counter()\n",
    "# Call the BASE model inside the LoRA wrapper\n",
    "with torch.no_grad():\n",
    "    out_T = tcr_encoder.model.forward(sequence_tokens=tcr_ids)\n",
    "    out_P = peptide_encoder.model.forward(sequence_tokens=pep_ids)\n",
    "    out_H = hla_encoder.model.forward(sequence_tokens=hla_ids)\n",
    "t1 = time.perf_counter()\n",
    "print(f\"ESM forwards (T+P+H) took: {t1 - t0:.3f} s\")\n",
    "\n",
    "\n",
    "# Some ESMC builds expose .embeddings; otherwise take last hidden state\n",
    "emb_T = out_T.embeddings \n",
    "emb_P = out_P.embeddings \n",
    "emb_H = out_H.embeddings\n",
    "\n",
    "# it is true that the shape of the embeddings is (B, L_pad, D)\n",
    "# takes 18 seconds on CPU to do 24 embeddings\n",
    "\n",
    "# Shapes\n",
    "# is this B the same as the above B I defined?\n",
    "B, L_T_pad, D = emb_T.shape\n",
    "\n",
    "rL = 8\n",
    "rD = 16\n",
    "d  = 128      # shared d\n",
    "\n",
    "# true length estimates\n",
    "L_T_true = tcr_mask.sum(dim=1)\n",
    "L_T_max  = L_T_true.max()\n",
    "\n",
    "L_P_true = pep_mask.sum(dim=1)\n",
    "L_P_max  = L_P_true.max()\n",
    "\n",
    "L_H_true = hla_mask.sum(dim=1)\n",
    "L_H_max  = L_H_true.max()\n",
    "\n",
    "# Encoders\n",
    "tcr_encoder = ESMFactorisedEncoder(D, rL, rD, d, L_max=L_T_max).to(device)\n",
    "\n",
    "pmhc_encoder = PMHCFactorisedEncoder(\n",
    "    D, rL, rD, d,\n",
    "    L_P_max=L_P_max,\n",
    "    L_H_max=L_H_max,\n",
    "    R_PH=0.7\n",
    ").to(device)\n",
    "\n",
    "# Forward\n",
    "zT = tcr_encoder(emb_T, tcr_mask)                    # (B, d)\n",
    "zT = zT / (zT.norm(dim=-1, keepdim=True) + eps)          # normalise TCR\n",
    "\n",
    "zPH = pmhc_encoder(emb_P, pep_mask, emb_H, hla_mask) # (B, d)\n",
    "\n",
    "# Final fused representation\n",
    "e_hat = torch.cat([zT, zPH], dim=-1)                     # (B, 2d)\n",
    "\n",
    "\n",
    "# takes 18s for 1 batch :/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e3bfeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc8ab03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6818f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e789a3f",
   "metadata": {},
   "source": [
    "#### Code below is old code pre reorganisation and full loss function building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939b46f2",
   "metadata": {},
   "source": [
    "##### a) Factorisation/Projection for ESMC Encoders\n",
    "- N.B. From next cell onwards, need to reconfigure to do for loop for all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "561878ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pick a batch and get token tensors and masks\n",
    "\n",
    "tcr_batch = next(iter(tcr_loader))\n",
    "pep_batch = next(iter(pep_loader))\n",
    "hla_batch = next(iter(hla_loader))\n",
    "\n",
    "tcr_ids = tcr_batch['clean_input_ids']\n",
    "tcr_mask = tcr_batch['attention_mask']\n",
    "\n",
    "pep_ids = pep_batch['clean_input_ids']\n",
    "pep_mask = pep_batch['attention_mask']\n",
    "\n",
    "hla_ids = hla_batch['clean_input_ids']\n",
    "hla_mask = hla_batch['attention_mask']\n",
    "\n",
    "# Unsqueze to make the right dims\n",
    "mT = tcr_mask.unsqueeze(-1).float()   # (B, L_T_pad, 1)\n",
    "mP = pep_mask.unsqueeze(-1).float()   # (B, L_P_pad, 1)\n",
    "mH = hla_mask.unsqueeze(-1).float()   # (B, L_H_pad, 1)\n",
    "\n",
    "\n",
    "B = tcr_ids.size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "584ac9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the BASE model inside the LoRA wrapper\n",
    "with torch.no_grad():\n",
    "    out_T = tcr_encoder.model.forward(sequence_tokens=tcr_ids)\n",
    "    out_P = peptide_encoder.model.forward(sequence_tokens=pep_ids)\n",
    "    out_H = hla_encoder.model.forward(sequence_tokens=hla_ids)\n",
    "\n",
    "# Some ESMC builds expose .embeddings; otherwise take last hidden state\n",
    "emb_T = out_T.embeddings \n",
    "emb_P = out_P.embeddings \n",
    "emb_H = out_H.embeddings\n",
    "\n",
    "# it is true that the shape of the embeddings is (B, L_pad, D)\n",
    "# takes 18 seconds on CPU to do 24 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a921c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Think this cell is included in the below\n",
    "# # maximum true tcr length \n",
    "# L_T_true = tcr_mask.sum(dim=1)\n",
    "# L_T_max = L_T_true.max()\n",
    "\n",
    "# # maximum true peptide length \n",
    "# L_P_true = pep_mask.sum(dim=1)\n",
    "# L_P_max = L_P_true.max()\n",
    "\n",
    "# # maximum true HLA length \n",
    "# L_H_true = hla_mask.sum(dim=1)\n",
    "# L_H_max = L_H_true.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b2e111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorised Encoder to get z_T and Z_pMHC\n",
    "# z = vec(A^TXB)H\n",
    "# X - (B, L_pad, D)\n",
    "# B - (D, rD)\n",
    "# A - (L_pad, rL)\n",
    "# H - (rD * rL, d)\n",
    "\n",
    "\n",
    "eps = 1e-8\n",
    "\n",
    "class ESMFactorisedEncoder(nn.Module):\n",
    "    def __init__(self, D, rL, rD, d, L_max):\n",
    "        \"\"\"\n",
    "        D    : ESM embedding dim (e.g. 960)\n",
    "        rL   : positional rank\n",
    "        rD   : channel rank\n",
    "        d    : latent dim\n",
    "        L_max: max true length for this modality in the batch\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.D   = D\n",
    "        self.rL  = rL\n",
    "        self.rD  = rD\n",
    "        self.d   = d\n",
    "        self.L_max = L_max\n",
    "\n",
    "        # Channel mixing: D -> rD\n",
    "        self.B_c = nn.Parameter(torch.empty(D, rD))\n",
    "        nn.init.xavier_uniform_(self.B_c)\n",
    "\n",
    "        # Positional mixing: positions 0..L_max-1 -> rL\n",
    "        self.A_c = nn.Parameter(torch.empty(L_max, rL))\n",
    "        nn.init.xavier_uniform_(self.A_c)\n",
    "\n",
    "        # Final map: (rL * rD) -> d\n",
    "        self.H_c = nn.Parameter(torch.empty(rL * rD, d))\n",
    "        nn.init.xavier_uniform_(self.H_c)\n",
    "\n",
    "    def forward(self, emb, mask):\n",
    "        \"\"\"\n",
    "        emb  : (B, L_pad, D) token embeddings\n",
    "        mask : (B, L_pad)   1 = real token, 0 = pad\n",
    "        returns z : (B, d)\n",
    "        \"\"\"\n",
    "        device = emb.device\n",
    "        B, L_pad, D = emb.shape\n",
    "        assert D == self.D\n",
    "\n",
    "        # Compute true lengths\n",
    "        L_true = mask.sum(dim=1)            # (B,)\n",
    "        z_list = []\n",
    "\n",
    "        for b in range(B):\n",
    "            Lb = int(L_true[b].item())\n",
    "            if Lb == 0:\n",
    "                # Degenerate case: no tokens -> zero vector\n",
    "                z_b = torch.zeros(self.d, device=device)\n",
    "                z_list.append(z_b)\n",
    "                continue\n",
    "\n",
    "            Xb = emb[b, :Lb, :]                      # (Lb, D)\n",
    "            mb = mask[b, :Lb].unsqueeze(-1).float()  # (Lb, 1)\n",
    "            Xb = Xb * mb                             # (Lb, D)\n",
    "\n",
    "            # 1) Channel compression: D -> rD\n",
    "            Yb = Xb @ self.B_c                       # (Lb, rD)\n",
    "\n",
    "            # 2) Positional compression: Lb -> rL\n",
    "            A_pos = self.A_c[:Lb, :]                 # (Lb, rL)\n",
    "            Ub = A_pos.T @ Yb                        # (rL, rD)\n",
    "\n",
    "            # 3) Flatten and map to latent d\n",
    "            Ub_flat = Ub.reshape(-1)                 # (rL * rD,)\n",
    "            z_b = Ub_flat @ self.H_c                 # (d,)\n",
    "\n",
    "            # 4) Normalise (optional; you can drop this if you want magnitude to carry info)\n",
    "            #z_b = z_b / (z_b.norm() + eps)\n",
    "            #normalise after function because need to combine p and hla first\n",
    "\n",
    "            z_list.append(z_b)\n",
    "\n",
    "        z = torch.stack(z_list, dim=0)               # (B, d)\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bb7b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape\n",
    "B, L_T_pad, D = emb_T.shape\n",
    "\n",
    "# Latent ranks and final dimension (hyperparameters)\n",
    "rL = 8      # positional rank for TCR (tunable)\n",
    "rD = 16     # channel rank for TCR (tunable)\n",
    "d    = 128    # final latent dimension (same d as in Z*)\n",
    "\n",
    "# ratio of peptide to HLA (hyperparameter)\n",
    "#R = 0.7\n",
    "# Epsilon for numerical stability\n",
    "eps=1e-8\n",
    "\n",
    "# maximum true lenghts of the sequences\n",
    "# maximum true tcr length \n",
    "L_T_true = tcr_mask.sum(dim=1)\n",
    "L_T_max = L_T_true.max()\n",
    "# maximum true peptide length \n",
    "L_P_true = pep_mask.sum(dim=1)\n",
    "L_P_max = L_P_true.max()\n",
    "# maximum true HLA length \n",
    "L_H_true = hla_mask.sum(dim=1)\n",
    "L_H_max = L_H_true.max()\n",
    "\n",
    "\n",
    "tcr_encoder_new = ESMFactorisedEncoder(D, rL, rD, d, L_max=L_T_max).to(device)\n",
    "pep_encoder_new = ESMFactorisedEncoder(D, rL, rD, d, L_max=L_P_max).to(device)\n",
    "hla_encoder_new = ESMFactorisedEncoder(D, rL, rD, d, L_max=L_H_max).to(device)\n",
    "\n",
    "# when you call module(), you are calling the forward method (PyTorch convention)\n",
    "zT = tcr_encoder_new(emb_T, tcr_mask)\n",
    "zP = pep_encoder_new(emb_P, pep_mask)\n",
    "zH = hla_encoder_new(emb_H, hla_mask)\n",
    "\n",
    "# normalise T as we are not gating this\n",
    "zT = zT / (zT.norm(dim=-1, keepdim=True) + eps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e58c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to what Barbara suggested in meeting - scale within the projection learning?\n",
    "\n",
    "R_PH = 0.7  # peptide gets more weight\n",
    "\n",
    "gP = (R_PH ** 0.5)          # scalar\n",
    "gH = ((1.0 - R_PH) ** 0.5)  # scalar\n",
    "\n",
    "gP_t = torch.tensor(gP, device=device)\n",
    "gH_t = torch.tensor(gH, device=device)\n",
    "\n",
    "# zP, zH: (B, d)\n",
    "zPH = gP_t * zP + gH_t * zH      # (B, d)\n",
    "# Optionally normalise:\n",
    "zPH = zPH / (zPH.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "# concatenate into e_hat\n",
    "e_hat = torch.cat([zT, zPH], dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "584682df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next step to apply non linear layers (potentially ReLU as middle layers, as long as the end is linear transformation and can learn negative values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9e7eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf91b7",
   "metadata": {},
   "source": [
    "##### Get Boltz Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca6b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get Boltz embeddings\n",
    "# file_path = '/home/natasha/multimodal_model/outputs/boltz_runs/positives/pair_000/boltz_results_pair_000/predictions/pair_000/embeddings_pair_000.npz'\n",
    "# manifest_path = '/home/natasha/multimodal_model/data/manifests/boltz_100_manifest.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c488fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/natasha/multimodal_model/scripts/train\n",
      "/home/natasha/multimodal_model/data/manifests/boltz_100_manifest.csv\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "\n",
    "home = '/home/natasha/multimodal_model'\n",
    "manifest_path = os.path.join(home, 'data', 'manifests', 'boltz_100_manifest.csv')\n",
    "# positive_manifest_path = os.path.join(home, 'data', 'manifests', 'boltz_100_manifest.csv')\n",
    "# negative_manifest_path = os.path.join(home, 'data', 'negative_manifests', 'boltz_100_manifest.csv')\n",
    "#print(manifest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a64ea0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch z shapes: [torch.Size([604, 604, 128]), torch.Size([604, 604, 128]), torch.Size([604, 604, 128]), torch.Size([604, 604, 128]), torch.Size([604, 604, 128]), torch.Size([604, 604, 128]), torch.Size([604, 604, 128]), torch.Size([604, 604, 128])]\n",
      "Pep lengths: [ 9  9 10  9  9 10  9 10]\n"
     ]
    }
   ],
   "source": [
    "class BoltzDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for loading Boltz z-embeddings one by one,\n",
    "    with chain lengths from the manifest.\n",
    "    Each pair has its own .npz file.\n",
    "    ORIGINAL VERSION - returns numpy arrays\n",
    "    \"\"\"\n",
    "    def __init__(self, manifest_path, base_path):\n",
    "        self.manifest = pd.read_csv(manifest_path)\n",
    "        self.base_path = base_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.manifest)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        yaml_rel_path = self.manifest.iloc[idx]['yaml_path']\n",
    "        pair_id = os.path.splitext(os.path.basename(yaml_rel_path))[0]\n",
    "        emb_path = os.path.join(\n",
    "            self.base_path,\n",
    "            'outputs',\n",
    "            'boltz_runs',\n",
    "            'positives',\n",
    "            pair_id,\n",
    "            f'boltz_results_{pair_id}',\n",
    "            'predictions',\n",
    "            pair_id,\n",
    "            f'embeddings_{pair_id}.npz'\n",
    "        )\n",
    "        with np.load(emb_path) as arr:\n",
    "            z = arr['z']  # Returns numpy array as-is\n",
    "        pep_len = self.manifest.iloc[idx]['pep_len']\n",
    "        tcra_len = self.manifest.iloc[idx]['tcra_len']\n",
    "        tcrb_len = self.manifest.iloc[idx]['tcrb_len']\n",
    "        hla_len = self.manifest.iloc[idx]['hla_len']\n",
    "        return z, pep_len, tcra_len, tcrb_len, hla_len\n",
    "\n",
    "\n",
    "def boltz_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Uses numpy for padding, then converts to torch at the end\n",
    "    \"\"\"\n",
    "    zs, pep_lens, tcra_lens, tcrb_lens, hla_lens = zip(*batch)\n",
    "    # Each z has shape [sum_of_lengths, sum_of_lengths, dim] or [1, sum_of_lengths, sum_of_lengths, dim]\n",
    "    # Pad zs to max shape with zeros and stack into tensor\n",
    "    zs = [np.squeeze(z, axis=0) for z in zs]\n",
    "    max_len = max(z.shape[0] for z in zs)\n",
    "    # get channel dimension number from first z\n",
    "    dim = zs[0].shape[-1]\n",
    "    padded_zs = np.zeros((len(zs), max_len, max_len, dim), dtype=zs[0].dtype)\n",
    "    for i, z in enumerate(zs):\n",
    "        l = z.shape[0]\n",
    "        padded_zs[i, :l, :l, :] = z\n",
    "    zs = torch.from_numpy(padded_zs).float()  # or .to(device)\n",
    "    \n",
    "    return {\n",
    "        \"z\": zs,  # batch of z arrays, each possibly of different shape\n",
    "        \"pep_len\": np.array(pep_lens),\n",
    "        \"tcra_len\": np.array(tcra_lens),\n",
    "        \"tcrb_len\": np.array(tcrb_lens),\n",
    "        \"hla_len\": np.array(hla_lens),\n",
    "    }\n",
    "\n",
    "# Example usage with original version:\n",
    "dataset_original = BoltzDataset(manifest_path, home)\n",
    "dataloader_original = DataLoader(\n",
    "    dataset_original, batch_size=8, shuffle=True, collate_fn=boltz_collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "batch = next(iter(dataloader_original))\n",
    "print(\"Batch z shapes:\", [z.shape for z in batch[\"z\"]])\n",
    "print(\"Pep lengths:\", batch[\"pep_len\"])\n",
    "\n",
    "# 5.6s to load 100 pairs\n",
    "# 56 s to load 1000 pairs\n",
    "# 560 s to load 10000 pairs (9 mins)\n",
    "# 5600 s to load 100000 pairs (16 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9717bdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230 10 365 [112 114 112 110 113 112 113 112] [116 116 113 114 115 114 115 117] [ 9  9 10  9  9 10  9 10] [362 365 365 365 365 365 365 365]\n"
     ]
    }
   ],
   "source": [
    "L_T = batch[\"tcra_len\"] + batch[\"tcrb_len\"]\n",
    "L_P = batch[\"pep_len\"]\n",
    "L_H = batch[\"hla_len\"]\n",
    "\n",
    "L_T_max = max(L_T)\n",
    "L_P_max = max(L_P)\n",
    "L_H_max = max(L_H)\n",
    "\n",
    "print(L_T_max, L_P_max, L_H_max, batch['tcra_len'], batch['tcrb_len'], batch['pep_len'], batch['hla_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f9e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorisation Z\n",
    "\n",
    "for i in range(len(batch[\"z\"])):\n",
    "    L_T_max = batch[\"tcra_len\"][i]\n",
    "    L_P_max = batch[\"pep_len\"][i]\n",
    "    L_H_max = batch[\"hla_len\"][i]\n",
    "\n",
    "\n",
    "\n",
    "dB  = 128 # dimension of Boltz embeddings\n",
    "rB  = 16 # Boltz channel rank\n",
    "rT  = 8 # TCR positional rank\n",
    "rPH = 8 # pMHC positional rank\n",
    "\n",
    "B_Z = torch.nn.Parameter(torch.empty(dB, rB))\n",
    "nn.init.xavier_uniform_(B_Z)\n",
    "\n",
    "\n",
    "L_T_max = int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ccef78",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mBoltzFactorised\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Factorised Boltz embeddings for projection into latent shared space before NC loss\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    - Zstar_batch: (B, 2d, 2d) operator acting on e_hat_t, e_hat_pmc in R^2d\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dB, rB, rT, rPH, d, L_max, L_PH_max):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class BoltzFactorised(nn.Module):\n",
    "    \"\"\"\n",
    "    Factorised Boltz embeddings for projection into latent shared space before NC loss\n",
    "\n",
    "    Inputs:\n",
    "    - z_boltz: (B, L_pad, L_pad, d_boltz) full Boltz z for the batch\n",
    "    - L_alpha, L_beta, L_p, L_h: (B,) lengths of the TCR alpha, TCR beta, peptide, HLA\n",
    "    - gP, gH: (B,) scalar or (B,) peptide/HLA gates in [0,1], norm-preserving in quadrature???? As in, gP**2 + gH**2 = 1\n",
    "\n",
    "    Outputs:\n",
    "    - Zstar_batch: (B, 2d, 2d) operator acting on e_hat_t, e_hat_pmc in R^2d\n",
    "    \"\"\"\n",
    "    def __init__(self, dB, rB, rT, rPH, d, L_max, L_PH_max):\n",
    "        \"\"\"\n",
    "        dB      : channel dimension of Boltz embeddings\n",
    "        rB      : rank of Boltz channel factorisation\n",
    "        rT      : rank of TCR positional encoding\n",
    "        rPH     : rank of pMHC positional encoding\n",
    "        d       : latent dimension of shared space\n",
    "        L_T_max   : maximum length of any sequence in the batch\n",
    "        L_PH_max: maximum length of any pMHC sequence in the batch\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dB     = dB\n",
    "        self.rB     = rB\n",
    "        self.rT     = rT\n",
    "        self.rPH    = rPH\n",
    "        self.d      = d\n",
    "        self.L_T_max  = L_max\n",
    "        self.L_PH_max = L_PH_max\n",
    "\n",
    "        # ---- 1) Channel mixing: dB -> rB ---- \n",
    "        self.B_Z = torch.nn.Parameter(torch.empty(dB, rB))\n",
    "        nn.init.xavier_uniform_(self.B_Z)\n",
    "\n",
    "        # ---- 2)a) TCR positional encoding: rT -> rT ---- \n",
    "        self.A_T = torch.nn.Parameter(torch.empty(L_T_max, rT))\n",
    "        nn.init.xavier_uniform_(self.A_T)\n",
    "\n",
    "        # ---- 2)b) pMHC positional encoding: rPH -> rPH ---- \n",
    "        self.A_PH = torch.nn.Parameter(torch.empty(L_PH_max, rPH))\n",
    "        nn.init.xavier_uniform_(self.A_PH)\n",
    "\n",
    "        # ---- 3) Learnable maps from factorised z (r* x r* x rB) -> d x d ---- \n",
    "        # flatten sizes for each block\n",
    "        n_TT   = rT  * rT  * rB\n",
    "        n_TPH  = rT  * rPH * rB\n",
    "        n_PHT  = rPH * rT  * rB\n",
    "        n_PHPH = rPH * rPH * rB\n",
    "        dd     = d * d\n",
    "\n",
    "        self.H_TT   = nn.Parameter(torch.empty(n_TT,   dd))\n",
    "        self.H_TPH  = nn.Parameter(torch.empty(n_TPH,  dd))\n",
    "        self.H_PHT  = nn.Parameter(torch.empty(n_PHT,  dd))\n",
    "        self.H_PHPH = nn.Parameter(torch.empty(n_PHPH, dd))\n",
    "\n",
    "        nn.init.xavier_uniform_(self.H_TT)\n",
    "        nn.init.xavier_uniform_(self.H_TPH)\n",
    "        nn.init.xavier_uniform_(self.H_PHT)\n",
    "        nn.init.xavier_uniform_(self.H_PHPH)\n",
    "\n",
    "        # ---- 4) Final linear layer: d -> d ---- \n",
    "        self.W_out = nn.Parameter(torch.empty(d, d))\n",
    "        nn.init.xavier_uniform_(self.W_out)\n",
    "    \n",
    "    def _get_gate_scalar(self, g, b):\n",
    "        \"\"\"\n",
    "        Helper: allow g to be a scalar tensor () or per-sample tensor (B,).\n",
    "        Returns a Python float for sample b.\n",
    "        \"\"\"\n",
    "        if g.dim() == 0:\n",
    "            return float(g.item())\n",
    "        else:\n",
    "            return float(g[b].item())\n",
    "\n",
    "    def forward(self, z_boltz, L_alpha, L_beta, L_p, L_h, gP, gH):\n",
    "        \"\"\"\n",
    "        Z_boltz : (B, L_pad, L_pad, dB)\n",
    "        L_alpha : (B,) true alpha lengths\n",
    "        L_beta  : (B,) true beta lengths\n",
    "        L_p     : (B,) true peptide lengths\n",
    "        L_h     : (B,) true HLA lengths\n",
    "        gP      : scalar () or (B,) peptide gate (already sqrt(R_PH))\n",
    "        gH      : scalar () or (B,) HLA gate (already sqrt(1-R_PH))\n",
    "\n",
    "        Returns:\n",
    "          Zstar_batch: (B, 2d, 2d)\n",
    "        \"\"\"\n",
    "\n",
    "        device = z_boltz.device\n",
    "        B, L_pad, _, dB = z_boltz.shape\n",
    "        assert dB == self.dB\n",
    "\n",
    "        Zstar_list = []\n",
    "\n",
    "        for b in range(B):\n",
    "            La  = int(L_alpha[b].item())\n",
    "            Lb  = int(L_beta[b].item())\n",
    "            Lp_ = int(L_p[b].item())\n",
    "            Lh_ = int(L_h[b].item())\n",
    "\n",
    "            L_T     = La + Lb\n",
    "            L_PH    = Lp_ + Lh_\n",
    "            L       = L_T + L_PH\n",
    "\n",
    "            # if we have missing z it just returns identity\n",
    "            if L == 0:\n",
    "                I_2d = torch.eye(2* self.d, device=device)\n",
    "                Zstar_list.append(I_2d)\n",
    "                continue\n",
    "            \n",
    "\n",
    "            # restrict to true tokens for the sample\n",
    "            Z = z_boltz[b, :L, :L, :] # (L, L, dB)\n",
    "\n",
    "            # ---- 2) Per channel normalisation (right now omit this step) ----\n",
    "            # another potential here is to use Adam optimiser to learn the normalisation\n",
    "            # mu = Z.mean(dim=(0,1), keepdim=True) # (1, 1, dB)\n",
    "            # std = Z.std(dim=(0,1), keepdim=True) # (1, 1, dB)\n",
    "            # Zc = (Z - mu) / std            # (L, L, dB)\n",
    "            # Zc = Zc / (math.sqrt(L) + eps) # scale by sqrt(L)\n",
    "            Zc = Z.clone()\n",
    "\n",
    "            # ----- 3) Gating ----\n",
    "            # TCR gates\n",
    "            if La > 0 and Lb > 0:\n",
    "                gA_b = 2**-0.5\n",
    "                gB_b = 2**-0.5\n",
    "            elif La > 0 and Lb == 0:\n",
    "                gA_b = 1\n",
    "                gB_b = 0\n",
    "            elif La == 0 and Lb > 0:\n",
    "                gA_b = 0\n",
    "                gB_b = 1\n",
    "            else:\n",
    "                gA_b = 0\n",
    "                gB_b = 0\n",
    "\n",
    "            # Peptide/HLA gates (from R set in encoder part)\n",
    "            gP_b = self._get_gate_scalar(gP, b)\n",
    "            gH_b = self._get_gate_scalar(gH, b)\n",
    "\n",
    "            # ---- 4) Build token-level gate vector over [alpha | beta | p | h] ----\n",
    "            gate = torch.zeros(L, device=device) # (L,)\n",
    "\n",
    "            idx0 = 0\n",
    "            idx1 = idx0 + La\n",
    "            idx2 = idx1 + Lb\n",
    "            idx3 = idx2 + Lp_\n",
    "            idx4 = idx3 + Lh_\n",
    "\n",
    "            if La  > 0: gate[idx0:idx1] = gA_b\n",
    "            if Lb  > 0: gate[idx1:idx2] = gB_b\n",
    "            if Lp_ > 0: gate[idx2:idx3] = gP_b\n",
    "            if Lh_ > 0: gate[idx3:idx4] = gH_b\n",
    "\n",
    "            gate_row = gate.view(L, 1, 1) # (L, 1, 1)\n",
    "            gate_col = gate.view(1, L, 1) # (1, L, 1)\n",
    "\n",
    "            Zg = Zc * gate_row * gate_col # (L, L, dB)\n",
    "\n",
    "            # ---- 5) Get TCR/pMHC blocks ----\n",
    "            sT = slice(0, L_T)           # [0, L_T) -> TCR (alpha + beta)\n",
    "            sPH = slice(L_T, L_T + L_PH) # [L_T, L] -> pMHC (P+H)\n",
    "\n",
    "            Z_TT  = Zg[sT, sT, :]    # (L_T, L_T, dB)\n",
    "            Z_TPH = Zg[sT, sPH, :]   # (L_T, L_PH, dB)\n",
    "            Z_PHT = Zg[sPH, sT, :]   # (L_PH, L_T, dB)\n",
    "            Z_PHPH = Zg[sPH, sPH, :] # (L_PH, L_PH, dB)\n",
    "\n",
    "            # ---- 6) channel/dimension compression ----\n",
    "            B_Z = self.B_Z # (dB, rB) operator across channels\n",
    "            Y_TT   = torch.einsum('ijc,cr->ijr', Z_TT,   B_Z)   # (L_T,  L_T,  rB)\n",
    "            Y_TPH  = torch.einsum('ijc,cr->ijr', Z_TPH,  B_Z)   # (L_T,  L_PH, rB)\n",
    "            Y_PHT  = torch.einsum('ijc,cr->ijr', Z_PHT,  B_Z)   # (L_PH, L_T,  rB)\n",
    "            Y_PHPH = torch.einsum('ijc,cr->ijr', Z_PHPH, B_Z)   # (L_PH, L_PH, rB)\n",
    "\n",
    "            # ---- 7) TCR positional compression with A_T / A_PH ----\n",
    "            # Sample-specific rows for the correct lengths for the per-sample positional tensors\n",
    "            if L_T > 0:\n",
    "                A_T_b = self.A_T[:L_T, :] # (L_T, rT)\n",
    "            else: \n",
    "                # no TCRs, so treat as zero contribution (or could be noise, decide later)\n",
    "                A_T_b = self.A_T[:1, :] * 0.0 # (1, rT) dummy\n",
    "\n",
    "            if L_PH > 0:\n",
    "                A_PH_b = self.A_PH[:L_PH, :] # (L_PH, rPH)\n",
    "            else:\n",
    "                # no pMHCs, so treat as zero contribution (or could be noise, decide later)\n",
    "                A_PH_b = self.A_PH[:1, :] * 0.0 # (1, rPH) dummy\n",
    "            \n",
    "            rT  = self.rT\n",
    "            rPH = self.rPH\n",
    "            rB  = self.rB\n",
    "            d   = self.d\n",
    "\n",
    "            # N.B. U tensors are not learned, they are discarded as intermediary steps for compression\n",
    "            # TCR-TCR (L_T, L_T, rB) -> (rT, rT, rB)\n",
    "            if L_T > 0:\n",
    "                U_TT = torch.einsum('ip,ijr->pjr', A_T_b, Y_TT) # (rT, L_T, rB)\n",
    "                V_TT = torch.einsum('pjr,jq->pqr', U_TT, A_T_b) # (rT, rT, rB)\n",
    "            else:\n",
    "                V_TT = torch.zeros(rT, rT, rB, device=device) # (rT, rT, rB)\n",
    "\n",
    "            # TCR–pMHC: (L_T, L_PH, rB) -> (rT, rPH, rB)\n",
    "            if L_T > 0 and L_PH > 0:\n",
    "                U_TPH = torch.einsum('ip,ijr->pjr', A_T_b,  Y_TPH)   # (rT,  L_PH, rB)\n",
    "                V_TPH = torch.einsum('pjr,jq->pqr', U_TPH, A_PH_b)   # (rT,  rPH, rB)\n",
    "            else:\n",
    "                V_TPH = torch.zeros(rT, rPH, rB, device=device)\n",
    "\n",
    "            # pMHC–TCR: (L_PH, L_T, rB) -> (rPH, rT, rB)\n",
    "            if L_T > 0 and L_PH > 0:\n",
    "                U_PHT = torch.einsum('ip,ijr->pjr', A_PH_b, Y_PHT)   # (rPH, L_T,  rB)\n",
    "                V_PHT = torch.einsum('pjr,jq->pqr', U_PHT, A_T_b)    # (rPH, rT,  rB)\n",
    "            else:\n",
    "                V_PHT = torch.zeros(rPH, rT, rB, device=device)\n",
    "\n",
    "            # pMHC–pMHC: (L_PH, L_PH, rB) -> (rPH, rPH, rB)\n",
    "            if L_PH > 0:\n",
    "                U_PHPH = torch.einsum('ip,ijr->pjr', A_PH_b, Y_PHPH) # (rPH, L_PH, rB)\n",
    "                V_PHPH = torch.einsum('pjr,jq->pqr', U_PHPH, A_PH_b) # (rPH, rPH, rB)\n",
    "            else:\n",
    "                V_PHPH = torch.zeros(rPH, rPH, rB, device=device)\n",
    "\n",
    "            # ---- 8) Flatten factorised blocks and map to d×d via H_* ----\n",
    "            v_TT_flat   = V_TT.reshape(-1)    # (rT*rT*rB,)\n",
    "            v_TPH_flat  = V_TPH.reshape(-1)   # (rT*rPH*rB,)\n",
    "            v_PHT_flat  = V_PHT.reshape(-1)   # (rPH*rT*rB,)\n",
    "            v_PHPH_flat = V_PHPH.reshape(-1)  # (rPH*rPH*rB,)\n",
    "\n",
    "            k_TT_flat   = v_TT_flat   @ self.H_TT   # (d*d,)\n",
    "            k_TPH_flat  = v_TPH_flat  @ self.H_TPH  # (d*d,)\n",
    "            k_PHT_flat  = v_PHT_flat  @ self.H_PHT  # (d*d,)\n",
    "            k_PHPH_flat = v_PHPH_flat @ self.H_PHPH # (d*d,)\n",
    "\n",
    "            K_TT   = k_TT_flat.view(d, d)\n",
    "            K_TPH  = k_TPH_flat.view(d, d)\n",
    "            K_PHT  = k_PHT_flat.view(d, d)\n",
    "            K_PHPH = k_PHPH_flat.view(d, d)\n",
    "\n",
    "            # Optional: enforce symmetry on diagonal blocks\n",
    "            # Enforce symmetry on all blocks?\n",
    "            K_TT   = 0.5 * (K_TT   + K_TT.t())\n",
    "            K_PHPH = 0.5 * (K_PHPH + K_PHPH.t())\n",
    "\n",
    "            # ---- 9) Assemble 2d x 2d operator for this sample ----\n",
    "            I_d = torch.eye(d, device=device)\n",
    "            Zstar_b = torch.zeros(2*d, 2*d, device=device)\n",
    "\n",
    "            Zstar_b[:d,  :d]  = I_d + K_TT\n",
    "            Zstar_b[:d,  d:]  = I_d + K_TPH\n",
    "            Zstar_b[d:,  :d]  = I_d + K_PHT\n",
    "            Zstar_b[d:,  d:]  = I_d + K_PHPH\n",
    "\n",
    "            Zstar_list.append(Zstar_b)\n",
    "\n",
    "        Zstar_batch = torch.stack(Zstar_list, dim=0)  # (B, 2d, 2d)\n",
    "        return Zstar_batch \n",
    "\n",
    "\n",
    "# index convention for the einsum\n",
    "# i = row token position, “i” is historically “index” or “first axis”\n",
    "# j\t= column token position, second positional axis (matrix-like)\n",
    "# c\t= channel\n",
    "# p, q\t= latent positional modes, P = projection / latent position\n",
    "# r = latent channel modes, R = rank / channel rank\n",
    "# b\t= batch index, B = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d7b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting it all together\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcr-multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
